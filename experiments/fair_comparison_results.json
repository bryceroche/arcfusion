{
  "config": {
    "d_model": 256,
    "vocab_size": 50257,
    "n_layers": 4,
    "n_heads": 8,
    "seq_len": 128,
    "batch_size": 32,
    "learning_rate": 0.0003,
    "max_steps": 2000,
    "eval_interval": 200,
    "mixed_precision": true,
    "gpu": "T4",
    "dataset": "wikitext-2",
    "seed": 42
  },
  "baseline_model": "Transformer_MHA",
  "baseline_stats": {
    "mean_ppl": 282.2647381813213,
    "std_ppl": 9.043026888915591,
    "n_runs": 3
  },
  "results": {
    "Transformer_MHA": {
      "success": true,
      "model_name": "Transformer_MHA",
      "parameters": 30221393,
      "eval_loss": 5.642845418429398,
      "perplexity": 282.2647381813213,
      "perplexity_std": 9.043026888915591,
      "n_runs": 3,
      "is_baseline": true,
      "cached": true
    },
    "Transformer_GQA": {
      "success": true,
      "model_name": "Transformer_GQA",
      "final_train_loss": 5.367480278015137,
      "eval_loss": 5.683687634468079,
      "perplexity": 294.03171464649597,
      "steps": 2000,
      "time_seconds": 63.772971630096436,
      "parameters": 29826641,
      "error": null,
      "gpu": "A100",
      "mixed_precision": true,
      "dataset": "wikitext-2",
      "seed": 42,
      "is_baseline": false
    },
    "Transformer_MQA": {
      "success": true,
      "model_name": "Transformer_MQA",
      "final_train_loss": 5.348880767822266,
      "eval_loss": 5.68375412940979,
      "perplexity": 294.051266918279,
      "steps": 2000,
      "time_seconds": 54.44174790382385,
      "parameters": 29760849,
      "error": null,
      "gpu": "A100",
      "mixed_precision": true,
      "dataset": "wikitext-2",
      "seed": 42,
      "is_baseline": false
    }
  }
}