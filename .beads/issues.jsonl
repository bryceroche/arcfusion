{"id":"arcfusion-003","title":"Interface-aware dream composition","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:21.711003-08:00","updated_at":"2025-12-11T12:15:36.11899-08:00","closed_at":"2025-12-11T12:15:36.11899-08:00","close_reason":"Implemented: Shape normalization, category ordering, 4 strategies (greedy, random, crossover, mutate)"}
{"id":"arcfusion-00i","title":"Add training completion notifications","description":"Minimize GPU downtime by getting notified when training completes. Options: macOS notifications, Slack webhook, email, or terminal bell. Need to decide on best approach for the dream_and_train.py pipeline.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T14:55:57.675799-08:00","updated_at":"2025-12-16T15:00:24.029351-08:00","closed_at":"2025-12-16T15:00:24.029351-08:00","close_reason":"Added notify() function using macOS native notifications (osascript) + terminal bell. Notification fires at end of main() with model count and best PPL."}
{"id":"arcfusion-040","title":"Auto-validation pipeline: build, train, and benchmark dreamed architectures","description":"## Vision\n\nCreate an automated pipeline that takes dreamed architectures and actually validates them:\n\n```\nDream â†’ CodeGen â†’ Build Model â†’ Train (small) â†’ Benchmark â†’ Score â†’ Feedback\n```\n\n## Why This Matters\n\nCurrently we estimate scores with formulas. To truly validate the dream engine, we need to:\n1. Build real models from dreamed component lists\n2. Train them (on small datasets to minimize compute)\n3. Run actual benchmarks\n4. Feed scores back to improve future dreams\n\n## Pipeline Components\n\n### 1. Model Builder\n- Take CodeGenerator output and make it runnable\n- Handle edge cases (missing dims, incompatible interfaces)\n- Parameterize model size (tiny/small/medium for testing)\n\n### 2. Training Harness\n- Small dataset options (WikiText-2, TinyStories, MNIST for sanity)\n- Quick training runs (few epochs, small batch)\n- Configurable compute budget (max time, max steps)\n\n### 3. Benchmark Suite\n- Lightweight benchmarks first (perplexity, simple classification)\n- Standardized evaluation protocol\n- Results stored in benchmark_results table\n\n### 4. Feedback Loop\n- Update component usefulness_score based on real performance\n- Update configuration scores\n- Track which component combinations actually work\n\n## Compute Considerations\n\n- Start TINY: 1M param models, 1000 training steps\n- Use CPU-friendly sizes initially\n- Could integrate with cloud compute later (Lambda, Modal, etc.)\n\n## Dependencies\n\n- Solid CodeGenerator (we have this)\n- Benchmark integration (arcfusion-v3i) for storing results\n\n## Success Criteria\n\n- [ ] Can take a dream output and produce a trainable model\n- [ ] Can train on small dataset in \u003c5 minutes\n- [ ] Can run benchmark and store results\n- [ ] Feedback improves future dream scores\n\n## Notes\n\nThis is the \"close the loop\" feature that makes ArcFusion genuinely useful for architecture search.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T14:58:41.413332-08:00","updated_at":"2025-12-11T15:47:04.338619-08:00","closed_at":"2025-12-11T15:47:04.338619-08:00","close_reason":"Implemented: ModelBuilder, TrainingHarness, BenchmarkRunner, ValidationPipeline. CLI validate command added. Requires PyTorch (optional).","dependencies":[{"issue_id":"arcfusion-040","depends_on_id":"arcfusion-v3i","type":"blocks","created_at":"2025-12-11T14:58:47.289242-08:00","created_by":"daemon"}]}
{"id":"arcfusion-0sq","title":"Add beads issues to web UI dashboard","description":"Display open/in-progress beads issues in the web UI so users can see current work items and project status alongside the ML metrics.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T14:55:55.628965-08:00","updated_at":"2025-12-16T15:15:37.214268-08:00","closed_at":"2025-12-16T15:15:37.214268-08:00","close_reason":"Issues page implemented in web_ui.py with metrics, tabs, and issue cards"}
{"id":"arcfusion-13q","title":"Auto-update surrogate model after training runs","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-16T08:22:10.452847-08:00","updated_at":"2025-12-16T08:25:57.437832-08:00","closed_at":"2025-12-16T08:25:57.437832-08:00","close_reason":"Added retrain_if_needed() to surrogate_model.py and wired into dream_and_train.py Phase 4. Model now tracks n_training_samples and auto-retrains when 2+ new samples available."}
{"id":"arcfusion-1bu","title":"Add findings/insights logging to DB for dream sessions","description":"## Context\nDuring architecture experimentation, we discover valuable insights that should inform future dreaming sessions. Currently these insights are only in issue descriptions or lost between sessions.\n\n## Goal\nCreate Python code to log key findings including text thoughts from training to the DB for future dreaming sessions.\n\n## Key Findings to Capture (Examples)\n\n### Attention Position Insights\n- **Interleaved attention (Jamba-style) = better quality**\n- **Attention-at-end = 3x faster but slightly worse quality**\n- Position of attention layers matters more than ratio\n\n### Architecture Learnings\n- Pure Mamba achieves best quality (PPL 226.1, 21.5% better than baseline)\n- GQA/MQA sacrifice quality for 3.6x speed improvement\n- Hybrid architectures trade off between pure approaches\n\n### General Insights\n- SSM (Mamba) captures sequential patterns better than attention for language modeling\n- Fewer KV heads (GQA/MQA) hurt perplexity but dramatically improve training speed\n- 3:1 ratio hybrids don't outperform 2:1 ratio - interleaving is key\n\n## Proposed Implementation\n\n### 1. New DB Table: `findings`\n```sql\nCREATE TABLE findings (\n    id TEXT PRIMARY KEY,\n    category TEXT,           -- 'attention', 'architecture', 'training', 'efficiency'\n    finding TEXT,            -- The insight text\n    evidence TEXT,           -- Supporting data (JSON: model results, comparisons)\n    confidence REAL,         -- 0.0-1.0 how confident we are\n    source_models TEXT,      -- Comma-separated model names that led to this\n    created_at TEXT,\n    tags TEXT                -- Comma-separated tags for retrieval\n);\n```\n\n### 2. New Methods in db.py\n```python\ndef add_finding(self, category: str, finding: str, evidence: dict, \n                confidence: float, source_models: list[str], tags: list[str]) -\u003e str\n\ndef get_findings(self, category: str = None, tags: list[str] = None, \n                 min_confidence: float = 0.0) -\u003e list[Finding]\n\ndef get_findings_for_dreaming(self) -\u003e dict:\n    '''Return structured insights to inform dream strategies'''\n```\n\n### 3. Integration with Dream Strategies\n- `results_aware_compose()` should query findings\n- New dream strategy: `insights_aware` that uses both results AND findings\n- Auto-log findings when significant results are detected\n\n## Acceptance Criteria\n- [ ] Add findings table to DB schema\n- [ ] Implement CRUD for findings\n- [ ] Add method to retrieve findings for dreaming\n- [ ] Log initial findings from current experiments\n- [ ] Update results_aware strategy to use findings","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T06:46:53.463276-08:00","updated_at":"2025-12-15T08:17:27.500159-08:00","closed_at":"2025-12-15T08:17:27.500159-08:00","close_reason":"Implemented as training_insights table in commit 65eee00. Includes: training_insights DB table, CRUD methods, get_insights_for_dreaming(), CLI command (arcfusion insights), and integration with composer.py dream strategies via _insight_guided_compose()"}
{"id":"arcfusion-1d0","title":"Research: Efficient exploration of ML engine design space","description":"What is the most efficient way to explore the space of ML engines we can create with our DB of components to dream up the optimal ML engine?\n\nCurrent state:\n- 60 components across 8 categories in DB\n- 40 trained models with PPL/time benchmarks\n- model_templates.py for code generation\n- cloud_train_fair.py for Modal GPU training\n\nResearch questions:\n1. Search strategy: Grid search vs Bayesian optimization vs evolutionary algorithms vs RL?\n2. Efficiency metrics: PPL alone vs PPL/time tradeoff vs Pareto frontier?\n3. Component compatibility: How to encode valid component combinations?\n4. Transfer learning: Can we predict PPL from architecture without full training?\n5. Pruning: Early stopping for unpromising architectures?\n\nPotential approaches:\n- Neural Architecture Search (NAS) with weight sharing\n- Surrogate models (predict PPL from architecture features)\n- Evolutionary search with component crossover/mutation\n- Multi-fidelity optimization (train less, predict more)\n- Bayesian optimization over component space\n\nSuccess criteria: Find better architectures with fewer training runs than exhaustive grid search.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T20:39:38.837665-08:00","updated_at":"2025-12-16T06:36:54.980872-08:00","closed_at":"2025-12-16T06:36:54.980872-08:00","close_reason":"Implemented surrogate model in scripts/surrogate_model.py. Linear regression with L2 regularization achieves Test MAE: 11.6 PPL, correlation 0.869. Key finding: n_layers is dominant predictor (coef 0.849), Mamba helps (0.547), KV heads barely matter (0.050).","comments":[{"id":1,"issue_id":"arcfusion-1d0","author":"bryceroche","text":"Proposed approach: Use the existing dreamer (composer.py) to generate hybrid engine candidates from our 60 components, then train each candidate and log results to training_runs table. The accumulated results become our signal:\n\n1. Dreamer generates candidate architectures (greedy, random, crossover, mutate strategies)\n2. model_templates.py converts candidates to trainable code\n3. cloud_train_fair.py trains on Modal GPU\n4. Results logged to training_runs + insights auto-generated\n5. Pattern recognition: Which component combinations work? Which fail?\n\nThe DB becomes our experiment journal - we can query for patterns like:\n- 'Which attention mechanisms pair well with which FFN variants?'\n- 'Does adding normalization X improve PPL for architecture family Y?'\n- 'What's the efficiency frontier for hybrid Mamba+Attention?'\n\nThis is essentially empirical architecture search with full provenance tracking.","created_at":"2025-12-16T04:44:58Z"},{"id":2,"issue_id":"arcfusion-1d0","author":"bryceroche","text":"## Concrete Next Step: Surrogate Model\n\nWith 48 training runs in DB, we have enough data to train a simple surrogate model that predicts PPL from architecture features:\n\n**Features to extract:**\n- n_layers, n_kv_heads, attention_type (MHA/GQA/MQA/Mamba)\n- param_count, d_model\n- Component categories present\n\n**Model:** Simple regression (sklearn) or small MLP\n\n**Benefit:** Screen 100s of candidate architectures in seconds, only train the top-10 predictions. Could 10x our exploration efficiency.\n\n**Implementation:**\n1. Export training_runs to features CSV\n2. Train regressor on (features â†’ PPL)\n3. Use to score dream_and_train candidates before GPU training","created_at":"2025-12-16T14:28:37Z"}]}
{"id":"arcfusion-1jd","title":"Focus architecture search on efficiency-first designs","description":"## Philosophy\nWe're doing 30M parameter training runs now, but designing for scale. Architectures must be efficient at 30M to be practical at 1B+.\n\n## Current Constraints\n- **Max slowdown:** 1.5x vs baseline\n- **Min PPL gain for slow models:** 30%\n- **Result:** Most Mamba/hybrid variants are now 'historical' - they don't meet the bar\n\n## Active Patterns\nOnly `place_attention_at_end` survives the efficiency filter.\n\n## Implications\n1. Stop chasing raw PPL - chase PPL/training-time ratio\n2. The dreamer should explore attention variants (GQA, MQA, sparse) that are faster\n3. Look for architectural changes that REDUCE training time while maintaining quality\n4. Mamba/SSM only viable if we find ways to make them faster\n\n## Key Insight\nWhat seems 'acceptable' at 30M (1.9x slower for 10% gain) becomes a scaling disaster at 1B+ parameters. Better to be strict now.","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-16T15:53:55.192287-08:00","updated_at":"2025-12-16T15:53:55.192287-08:00"}
{"id":"arcfusion-1kl","title":"Save model recipes for reproducibility","description":"## Problem\nWe can train models but can't easily recreate them later. The model code is embedded as strings in MODELS dict but not linked to training runs.\n\n## Current State\n- Model code stored as strings in cloud_train_fair.py MODELS dict\n- Training runs reference model_name but not the actual code\n- No way to recover exact model definition from a training run\n\n## Requirements\n1. Save model code/recipe with each training run\n2. Store hyperparameters used (d_model, n_layers, etc.)\n3. Store random seed for reproducibility\n4. Optionally store model weights/checkpoints\n\n## Schema Changes Needed\n- Add model_code TEXT column to training_runs?\n- Or create separate recipes table with versioning?\n- Link training_runs to recipes via recipe_id\n\n## Acceptance Criteria\n- [ ] Can recreate any previously trained model\n- [ ] Recipe includes full model code\n- [ ] Hyperparameters are queryable\n- [ ] Seeds are tracked for reproducibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T07:37:14.459225-08:00","updated_at":"2025-12-14T16:43:05.924363-08:00","closed_at":"2025-12-14T16:43:05.924363-08:00","close_reason":"Implemented model_code and code_hash columns in training_runs table for full reproducibility"}
{"id":"arcfusion-2bf","title":"Store full leaderboard results in structured format","description":"Save the 21-architecture benchmark leaderboard to the DB in a queryable format. Include PPL, training time, architecture pattern, and tier classification.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-15T14:58:32.349653-08:00","updated_at":"2025-12-15T15:49:12.06544-08:00","closed_at":"2025-12-15T15:49:12.06544-08:00","close_reason":"Added 'arcfusion leaderboard' CLI command showing ranked training runs with tier classifications (S/A/B/C/D). Includes PPL, time, vs baseline %, and sorting by ppl/time/efficiency."}
{"id":"arcfusion-2q3","title":"Wire surrogate model to review trained results for prediction accuracy tracking","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-16T08:27:39.636759-08:00","updated_at":"2025-12-16T11:11:27.673561-08:00","closed_at":"2025-12-16T11:11:27.673561-08:00","close_reason":"Implemented prediction accuracy tracking: get_surrogate_accuracy_stats() in db.py, CLI stats display, and logging in surrogate retrain. Added 2 tests."}
{"id":"arcfusion-2q6","title":"Cache WikiText-2 data on Modal to avoid timeouts","description":"## Problem\nMamba training failed with \"ReadTimeout\" loading WikiText-2 from HuggingFace. Network issues on Modal cause wasted GPU time.\n\n## Options\n\n### Option 1: Modal Volume (Persistent)\n- Store data in Modal Volume once\n- Load from volume on each run\n- Cost: ~$0.20/GB/month = ~$0.01/month for WikiText-2\n- Pro: Never re-download\n- Con: Volume setup complexity\n\n### Option 2: Pre-bake into Image\n- Download data during image build\n- Data cached in image layers\n- Cost: Included in image storage\n- Pro: Always available, no runtime download\n- Con: Rebuilds image when data changes\n\n### Option 3: Retry with backoff\n- Add retry logic to load_dataset()\n- Exponential backoff on timeout\n- Pro: Simple, no infrastructure changes\n- Con: Still wastes time on retries\n\n## Recommendation\n**Option 2 (Pre-bake into Image)** because:\n- WikiText-2 is small (~13MB)\n- Data won't change\n- Zero runtime download time\n- Most reliable\n\n## Implementation\n```python\nimage = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .pip_install(\"torch\u003e=2.0\", \"datasets\", \"tiktoken\")\n    .run_commands(\n        \"python -c \\\"from datasets import load_dataset; load_dataset('wikitext', 'wikitext-2-raw-v1')\\\"\"\n    )\n)\n```\n\n## Cost Analysis\n- Current: ~60s download per run Ã— N runs = wasted GPU time\n- With cache: 0s download\n- Image storage: Negligible (data is \u003c20MB)\n- **ROI**: Pays for itself after 1 prevented timeout\n\n## Acceptance Criteria\n- [ ] Data pre-loaded in Modal image\n- [ ] Zero download time on training runs\n- [ ] No more timeout failures","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-14T08:02:29.118136-08:00","updated_at":"2025-12-14T08:12:21.559211-08:00","closed_at":"2025-12-14T08:12:21.559211-08:00","close_reason":"WikiText-2 now pre-cached in Modal image during build. Data load time reduced from timeout (60s+) to ~13s. Added run_commands step to download dataset during image build."}
{"id":"arcfusion-2si","title":"Improve paper ingestion workflow - API key handling","description":"## Context\nWhen ingesting papers via `arcfusion analyze`, the ANTHROPIC_API_KEY must be set in the shell. During session, the key wasn't available, forcing manual workaround (Claude Code manually extracted components).\n\n## Current State\n- `arcfusion analyze --ids \u003carxiv_ids\u003e` requires ANTHROPIC_API_KEY env var\n- If missing, fails with: `[ERROR] ANTHROPIC_API_KEY environment variable required`\n- No fallback or alternative ingestion path\n\n## Possible Improvements\n1. **Support .env file** - Load from `.env` if env var not set\n2. **Interactive prompt** - Ask for key if missing (with secure input)\n3. **Document the workflow** - Add to README/CLAUDE.md how to set up for paper ingestion\n\n## Acceptance Criteria\n- [ ] Paper ingestion works without manual env var export (via .env)\n- [ ] Clear error message with instructions if key unavailable\n- [ ] Documented in project README","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-15T08:25:28.94891-08:00","updated_at":"2025-12-15T19:54:15.573951-08:00","closed_at":"2025-12-15T19:54:15.573951-08:00","close_reason":"Added python-dotenv support with .env.example template"}
{"id":"arcfusion-30o","title":"Auto-validate generated code before saving","description":"GeneratedCode.save() writes code without calling validate_syntax(). Add automatic validation and sanitization of component names in docstrings to prevent invalid Python from being saved.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-11T13:02:23.288818-08:00","updated_at":"2025-12-11T13:11:37.162829-08:00","closed_at":"2025-12-11T13:11:37.162829-08:00","close_reason":"save() now validates syntax by default, raises ValueError on invalid code"}
{"id":"arcfusion-38g","title":"Add training_runs table to DB schema for detailed benchmark tracking","description":"Add a training_runs table to track: model architecture, training config (steps, lr, batch_size), hardware (GPU type, mixed precision), results (loss, perplexity, time), and baseline comparison data. Links to existing benchmark_results for post-training evaluation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:25:29.635508-08:00","updated_at":"2025-12-13T16:28:40.45964-08:00","closed_at":"2025-12-13T16:28:40.45964-08:00","close_reason":"Added training_runs table to DB schema with full CRUD methods. Tracks model config, hardware, results, and baseline comparison. 228 tests pass."}
{"id":"arcfusion-3dp","title":"DeepGQA6 breakthrough: Fast training + decent PPL","description":"DeepGQA6 (6 GQA layers) beats MHA baseline (270.3 vs 274.8 PPL) while training 3.2x faster (62s vs 197s). Explore: deeper GQA variants (8, 10 layers?), wider models, different n_kv_heads ratios.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T14:58:33.369169-08:00","updated_at":"2025-12-15T15:23:51.46183-08:00","closed_at":"2025-12-15T15:23:51.46183-08:00","close_reason":"BREAKTHROUGH: DeepGQA depth sweep confirms MORE layers = BETTER quality. DeepGQA10 achieves 245.6 PPL @ 81s (-12.2% vs MHA), beating DeepGQA6 (270.3) and approaching Mamba (226). The quality scales well with depth while time stays sublinear. Saved findings to DB as insight."}
{"id":"arcfusion-4it","title":"Refactor dream_candidates DB schema and rewire Python","description":"Clean up dream_candidates table schema:\n- Remove redundant boolean flags (has_mamba, has_linear_attn, is_hybrid) that can be inferred from components_json\n- Simplify arch_type since it's also derivable from components\n- Keep components_json as the source of truth\n- Update all Python code that reads/writes these fields (db.py, composer.py, dream_and_train.py, web_ui.py, tests)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T14:43:44.248248-08:00","updated_at":"2025-12-16T14:49:33.374629-08:00","closed_at":"2025-12-16T14:49:33.374629-08:00","close_reason":"Removed has_mamba, has_linear_attn, is_hybrid, arch_type columns. These are now @property methods computed from components_json. Updated db.py, dream_and_train.py, tests. All 20 tests pass."}
{"id":"arcfusion-4lc","title":"Research: Minimum training scale to differentiate architecture quality","description":"## Research Question\n\nHow much training is needed before it becomes apparent whether an architecture is good or bad?\n\n## Why This Matters\n\nIf we can detect architecture quality with minimal training:\n- Faster iteration on dream experiments\n- Lower compute costs\n- More architectures explored per dollar\n\nIf we need substantial training:\n- Need cloud compute integration\n- Batch validation runs\n- Different validation strategy\n\n## Experiment Design\n\n1. **Baseline**: Train known-good architecture (Transformer) at various scales\n   - 100 steps, 1K steps, 10K steps, 100K steps\n   - Track loss curves and final perplexity\n\n2. **Bad architecture**: Train known-bad architecture (random component soup)\n   - Same scale progression\n   - Compare curves to baseline\n\n3. **Analysis**: At what point do curves diverge?\n   - Early divergence = can validate cheaply\n   - Late divergence = need more compute\n\n## Metrics to Track\n\n- Loss at each checkpoint\n- Perplexity trajectory\n- Parameter efficiency (loss per param)\n- Training stability (gradient variance)\n\n## Literature Review\n\n- Scaling laws papers (Chinchilla, etc.)\n- Neural architecture search papers\n- Early stopping criteria research\n\n## Dependencies\n\n- Need PyTorch installed\n- Need cloud compute for larger runs (arcfusion-zzp)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T15:52:04.874825-08:00","updated_at":"2025-12-12T13:57:06.25733-08:00","closed_at":"2025-12-12T13:57:06.25733-08:00","close_reason":"Research complete: Need 2000+ steps to differentiate architecture quality. Simple architectures train faster initially on random data. Updated cloud defaults accordingly.","dependencies":[{"issue_id":"arcfusion-4lc","depends_on_id":"arcfusion-zzp","type":"blocks","created_at":"2025-12-11T15:52:10.640381-08:00","created_by":"daemon"}]}
{"id":"arcfusion-4to","title":"Ensure training_runs table has both PPL and time recorded for all runs","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T08:27:38.041815-08:00","updated_at":"2025-12-16T11:07:39.17291-08:00","closed_at":"2025-12-16T11:07:39.17291-08:00","close_reason":"Duplicate of arcfusion-upx (already closed). Training_runs table already has both perplexity and time_seconds columns, all 53 runs have both fields."}
{"id":"arcfusion-522","title":"Prioritize GQA/MQA variants - 3.5x training speedup","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-15T13:42:36.487123-08:00","updated_at":"2025-12-15T19:16:44.592981-08:00","closed_at":"2025-12-15T19:16:44.592981-08:00","close_reason":"Grid completed: MQA/GQA Ã— 10/12/14/18L Ã— kv_heads 1/2/4. Key finding: MQA18 (229.3) quality leader, GQA14 (235.4@108s) best efficiency. Template-based generation enables rapid exploration."}
{"id":"arcfusion-52n","title":"Integrate summarization into dreamed model evaluation","description":"Summarization is powerful for saving context. Ideas: 1) Auto-summarize training run findings after each benchmark, 2) Summarize architecture patterns that work/fail, 3) Generate 'architecture recipe cards' with key insights, 4) Use LLM to analyze loss curves and generate recommendations. Could integrate with training_insights table.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T15:01:40.941735-08:00","updated_at":"2025-12-15T19:24:18.504608-08:00","closed_at":"2025-12-15T19:24:18.504608-08:00","close_reason":"Added generate_auto_insight() function that automatically logs notable results (best in type, overall best, Pareto-optimal) to training_insights table. Integrated with run_gqa_mqa_grid.py."}
{"id":"arcfusion-5l7","title":"Integrate ThunderKittens into training pipeline","description":"## Goal\nAdd ThunderKittens CUDA kernels to our Modal A100 training pipeline.\n\n## Discovery: TK Requires H100 ğŸš«\n\n**ThunderKittens does NOT support A100** - all attention kernels only have H100/4090 source files.\n\nTK's core innovation uses H100's Tensor Memory Accelerator (TMA) for async data loading.\nA100 (sm_80) doesn't have TMA - it's an H100 (sm_90a) feature.\n\n## Baseline Results (A100 40GB, PyTorch SDPA)\n- Raw attention kernel: 0.066 ms/iter\n- Full MHA forward: 0.14 ms/iter\n- Full MHA fwd+bwd: 4.38 ms/iter\n\nThis is already using FlashAttention-2 (via SDPA) - very optimized baseline.\n\n## Options Going Forward\n\n1. **Stay on A100 with SDPA** (current)\n   - Already efficient, battle-tested\n   - Cost-effective (~$2/hr)\n\n2. **Upgrade to H100 for TK** (future)\n   - ~$4/hr on Modal\n   - Would enable TK's 10-40% speedups\n   - Better for linear attention (Based/Hedgehog)\n\n3. **Watch for A100 support**\n   - TK is actively developed\n   - May add A100 kernels in future\n\n## Recommendation\nClose this issue - A100 + SDPA is our best path for now.\nCreate new issue if/when we scale to H100.\n\n## Files Created\n- scripts/tk_attention.py (drop-in TK classes with SDPA fallback)\n- scripts/test_tk_modal.py (benchmark script)\n\n## Findings Added\n- 4b676f41364a: Baseline SDPA performance\n- 8d104ee983e6: TK requires H100","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-16T15:59:06.040522-08:00","updated_at":"2025-12-16T16:26:36.227192-08:00","closed_at":"2025-12-16T16:26:36.227192-08:00","close_reason":"TK requires H100 (sm_90a) - no A100 support. Our A100+SDPA baseline is already efficient. Created findings 4b676f41364a and 8d104ee983e6."}
{"id":"arcfusion-5ux","title":"Validate relationships in analyzer before adding","description":"In analyzer.py lines 334-348, component relationships are added without verifying both components exist. If LLM returns relationship for skipped component (low confidence), it fails silently or creates orphaned refs.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-11T13:02:16.95983-08:00","updated_at":"2025-12-11T13:11:37.107019-08:00","closed_at":"2025-12-11T13:11:37.107019-08:00","close_reason":"Added validation with warnings for skipped relationships"}
{"id":"arcfusion-5wj","title":"Web UI for architecture exploration","description":"Build a web interface to visualize components, relationships, and dream new architectures interactively","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-11T12:15:52.710451-08:00","updated_at":"2025-12-12T14:26:48.162589-08:00","closed_at":"2025-12-12T14:26:48.162589-08:00","close_reason":"Implemented FastAPI web UI with REST API, interactive frontend, vis.js graph, and dream interface"}
{"id":"arcfusion-5wr","title":"Clarification: We save architecture code, NOT weights (by design)","description":"RESOLVED: We intentionally save only the model architecture code, not trained weights.\n\nWhat we save (by design):\n- model_code: Full Python class definition for reproducibility\n- code_hash: SHA256 hash for deduplication\n- Training config and results\n\nWhat we DON'T save (intentionally):\n- Trained weights (~120MB per model)\n- Optimizer state\n\nRationale:\n- Weights are expensive to store (2.5GB for 21 runs)\n- We can retrain any architecture in ~1-10 minutes\n- Architecture is the valuable IP, not specific weights\n- Fast iteration \u003e checkpoint preservation\n\nThis is working as intended. Closing.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T15:05:36.431104-08:00","updated_at":"2025-12-15T15:07:25.936417-08:00","closed_at":"2025-12-15T15:07:25.936417-08:00","close_reason":"Working as intended: We save architecture code (model_code + code_hash), not weights. This is by design for fast iteration."}
{"id":"arcfusion-5xt","title":"Add findings table to web UI dashboard","description":"Display the findings DB table in the Streamlit web UI so users can see accumulated knowledge/patterns.\n\n## Requirements\n- New page or section showing findings from db.list_findings()\n- Display: title, confidence, delta_vs_baseline, tags\n- Filter by tags (architecture-pattern, efficiency, mamba, etc.)\n- Sort by confidence, delta, or date\n- Show recommendations derived from get_architecture_patterns()\n\n## Related\n- Findings already stored via add_finding()\n- Helper methods: get_findings_by_tags(), get_architecture_patterns(), get_efficiency_constraints()\n- Currently have 6 findings in DB including efficiency constraints and architecture patterns","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T15:34:06.562609-08:00","updated_at":"2025-12-16T15:35:41.067525-08:00","closed_at":"2025-12-16T15:35:41.067525-08:00","close_reason":"Added ğŸ“š Findings page to web UI with: metrics row, tag filtering, sorting, expandable findings with confidence/delta/tags, architecture patterns section, efficiency constraints display"}
{"id":"arcfusion-68k","title":"Add efficiency constraint: reject \u003e1.5x slower unless big PPL gains","description":"Encode the finding that architectures more than 1.5x slower than baseline are not worth it unless they deliver significant PPL improvements.\n\n## Rationale\nFrom benchmark results:\n- GQAMamba: 1.9x slower, +10% quality â†’ borderline\n- Pure Mamba: 3.5x slower, +25% quality â†’ only worth it for research\n- Sandwich: 1.8x slower, +9% quality â†’ acceptable\n\n## Implementation ideas\n- Add efficiency screening to dream candidate generation\n- Filter out candidates that exceed time threshold without PPL benefit\n- Could be a configurable parameter (default 1.5x slowdown max)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T15:21:10.130841-08:00","updated_at":"2025-12-16T15:23:46.322627-08:00","closed_at":"2025-12-16T15:23:46.322627-08:00","close_reason":"Added efficiency filter to screen_candidates_with_surrogate() - rejects \u003e1.5x slower unless 15%+ PPL gain"}
{"id":"arcfusion-6s2","title":"Surrogate model should screen by efficiency, not just PPL","description":"Currently dream_and_train.py screens candidates by predicted PPL only. This biases toward Mamba (212 PPL, 450s) over Linear Attention (254 PPL, 161s).\n\nShould screen by efficiency = PPL Ã— âˆš(time/300s) to find the Pareto frontier of quality vs speed. The surrogate already predicts both PPL and time, we just need to use both in screening.\n\nOptions:\n1. Default to efficiency screening\n2. Add --screen-by=[ppl|efficiency|time] CLI flag\n3. Screen for both and show Pareto optimal candidates","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T15:05:42.447478-08:00","updated_at":"2025-12-16T15:08:35.87964-08:00","closed_at":"2025-12-16T15:08:35.87964-08:00","close_reason":"Added compute_efficiency() and changed default screening to efficiency-based. Pipeline now finds fast architectures like Linear Attention that beat Mamba on efficiency despite higher PPL."}
{"id":"arcfusion-753","title":"Ensure Modal GPU cleanup to avoid billing","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-14T07:13:39.313164-08:00","updated_at":"2025-12-14T07:14:33.860315-08:00","closed_at":"2025-12-14T07:14:33.860315-08:00","close_reason":"Modal scales to zero by default - we pay only for actual compute. Our use of 'with app.run():' ensures clean shutdown. No scaledown_window/min_containers used. To monitor: 'modal app list' shows running apps."}
{"id":"arcfusion-75x","title":"Document model_templates.py usage","description":"Add model_templates.py usage examples to CLAUDE.md or create a dedicated doc. Include: (1) GQA/MQA generation with variable kv_heads and layers, (2) Mamba generation with parallel scan option, (3) Convenience functions (mqa, gqa, mha, mamba), (4) Integration with cloud_train_fair.py for experiments.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-15T20:33:16.086165-08:00","updated_at":"2025-12-15T21:36:46.766244-08:00","closed_at":"2025-12-15T21:36:46.766244-08:00","close_reason":"Documented in AGENTS.md: Model Templates section shows generate_gqa_model, generate_mamba_model, and convenience functions"}
{"id":"arcfusion-7fq","title":"Explore summarization as key capability - compress and reference content","description":"","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T13:42:31.02555-08:00","updated_at":"2025-12-15T16:02:35.213109-08:00","closed_at":"2025-12-15T16:02:35.213109-08:00","close_reason":"Implemented summarization feature: Summary dataclass, summaries table with indexes, CRUD methods (add_summary, get_summary, list_summaries, delete_summary), CLI commands (summary add/list/show/delete/recipe-cards), auto-generation of recipe cards from training runs"}
{"id":"arcfusion-7kc","title":"Capture key architectural decisions/ideas for each trained model","description":"","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T11:33:59.738301-08:00","updated_at":"2025-12-16T11:37:18.542007-08:00","closed_at":"2025-12-16T11:37:18.542007-08:00","close_reason":"Already implemented: dream_candidates has components_json, arch_type, predicted vs actual PPL. training_runs has full model_code, vs_baseline_pct. training_insights captures each experiment. We can query any dimension needed."}
{"id":"arcfusion-7xk","title":"Training too slow - evaluate faster GPU options","description":"Current training on A10G takes ~97s per model for 2000 steps. Options to explore:\n\n1. **A100 GPU** (~3x faster, ~3x cost) - best for serious experiments\n2. **H100 GPU** (~5x faster, ~5x cost) - overkill for small models?\n3. **Reduce model size** - smaller d_model/n_layers for faster iteration\n4. **Reduce steps** - 1000 steps might be enough to differentiate architectures\n5. **Batch multiple models** - train in parallel on same GPU\n\nQuestions to answer:\n- What's the minimum training needed to reliably differentiate architectures?\n- Is the bottleneck GPU compute or data loading?\n- Would spot instances save money?","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T17:42:01.101448-08:00","updated_at":"2025-12-14T07:08:20.817428-08:00","closed_at":"2025-12-14T07:08:20.817428-08:00","close_reason":"Switched to A100 GPU. Training now 3-4x faster (55-65s vs 200s on T4)"}
{"id":"arcfusion-8d3","title":"Research ThunderKittens ML papers and efficiency techniques","description":"## Goal\nResearch ThunderKittens - a CUDA kernel library for efficient ML operations.\n\n## What to investigate\n- ThunderKittens paper and implementation details\n- Attention kernel optimizations (faster than FlashAttention?)\n- Memory efficiency techniques\n- How it could integrate with our architecture search\n- Any efficiency gains that could help meet our 1.5x slowdown constraint\n\n## Why this matters\nWe need architectures that are FASTER, not slower. ThunderKittens may offer:\n- Faster attention implementations\n- Better GPU utilization\n- Techniques we can incorporate into our training pipeline\n\n## Resources to find\n- Original paper/blog posts\n- GitHub repository\n- Benchmark comparisons vs FlashAttention\n- Integration examples with PyTorch","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T15:54:51.571124-08:00","updated_at":"2025-12-16T15:56:43.813057-08:00","closed_at":"2025-12-16T15:56:43.813057-08:00","close_reason":"Researched ThunderKittens: 14x linear attention speedup, faster Mamba-2, ICLR 2025 paper. Could make previously-rejected architectures viable."}
{"id":"arcfusion-8rs","title":"Track arcfusion.db in git","description":"The database file isn't being tracked by git. Add it to version control so component/engine data persists across clones.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T16:19:14.914455-08:00","updated_at":"2025-12-14T16:28:12.598526-08:00","closed_at":"2025-12-14T16:28:12.598526-08:00","close_reason":"Added arcfusion.db and pre2017_experiment.db to git"}
{"id":"arcfusion-8sc","title":"Document dream_and_train.py empirical architecture search","description":"Document the dream â†’ train â†’ log pipeline for empirical architecture search.\n\nCreated: scripts/dream_and_train.py\n\nPipeline flow:\n1. Dream component combinations from 117 DB components (greedy/random strategies)\n2. Categorize components: attention, ssm, position, normalization, ffn, embedding\n3. Map patterns to trainable PyTorch code (GQA/MQA/MHA, Mamba, LinearAttention)\n4. Train on Modal A100 GPU\n5. Log to training_runs table\n6. Auto-generate insights tagged 'dreamed_architecture'\n7. Patterns accumulate for learning\n\nDocumentation needed:\n- Add to CLAUDE.md or AGENTS.md\n- Example usage and output\n- How to extend with new component patterns\n- Query examples for analyzing dreamed architecture results\n\nRelated: arcfusion-1d0 (research issue for efficient exploration)","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-15T20:52:33.502712-08:00","updated_at":"2025-12-15T21:36:45.665433-08:00","closed_at":"2025-12-15T21:36:45.665433-08:00","close_reason":"Documented in AGENTS.md: Empirical Architecture Search section with pipeline flow, model templates, extension guide, and query examples"}
{"id":"arcfusion-8ys","title":"Implement Mamba speedups (parallel scan, CUDA kernels)","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-15T13:42:32.690358-08:00","updated_at":"2025-12-15T20:02:57.174315-08:00","closed_at":"2025-12-15T20:02:57.174315-08:00","close_reason":"Implemented parallel scan - 4.64x speedup (706s -\u003e 152s) with only 0.8% PPL loss"}
{"id":"arcfusion-92k","title":"Investigate Modal cold start mitigation strategies","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T11:33:57.036217-08:00","updated_at":"2025-12-16T11:36:13.329571-08:00","closed_at":"2025-12-16T11:36:13.329571-08:00","close_reason":"Modal cold start is ~1-2 min overhead on 8 min train. Options: (1) keep_warm=1 costs ~$0.10/hr idle, (2) persistent HF cache already reduces startup. Recommendation: not needed now - cold start is acceptable overhead."}
{"id":"arcfusion-9c9","title":"DB table: recipes with adjustments tracking","description":"New DB table to store dreamed recipes:\n- recipe_id (unique)\n- component_ids (ordered list)\n- assembly_instructions (JSON: connections, residuals, shapes)\n- source_strategy (greedy, crossover, mutate, etc.)\n- created_at timestamp\n\nPlus adjustments table:\n- adjustment_id\n- recipe_id (FK)\n- original_value\n- adjusted_value  \n- reason (why modification was needed)\n- adjusted_at timestamp\n\nThis enables: recipe recreation, composer learning from failures, training run reproducibility.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:15:07.58677-08:00","updated_at":"2025-12-12T12:24:43.228152-08:00","closed_at":"2025-12-12T12:24:43.228152-08:00","close_reason":"DB tables implemented: recipes (recipe_id, name, component_ids, assembly, strategy, estimated_score, parent_engine_ids, notes) and recipe_adjustments (adjustment_id, recipe_id, adjustment_type, original_value, adjusted_value, reason, component_id). Full CRUD methods and stats tracking."}
{"id":"arcfusion-9ob","title":"Save model recipes to rebuild any historical model","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-15T13:42:26.851577-08:00","updated_at":"2025-12-15T15:49:10.916904-08:00","closed_at":"2025-12-15T15:49:10.916904-08:00","close_reason":"All 23 training runs have model_code stored. Added 'arcfusion recipe' CLI command to retrieve model code by run_id or model name. Can rebuild any model from the leaderboard."}
{"id":"arcfusion-9yr","title":"Fair model comparison: account for GPU availability and cold start variance","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T11:33:58.316332-08:00","updated_at":"2025-12-16T11:36:14.44809-08:00","closed_at":"2025-12-16T11:36:14.44809-08:00","close_reason":"Fair comparison achieved via: (1) fixed seeds per run, (2) same config for all models, (3) baseline averaging over 3 runs, (4) dedicated A100 GPU. Time measured is pure training time. PPL is key metric - time variance is acceptable."}
{"id":"arcfusion-a1s","title":"ML Agent: faithful recipe execution with modification tracking","description":"ML Agent receives Recipe from Composer and:\n- Makes best effort to train the model\n- Stays faithful to the recipe provided\n- Records ANY modifications needed to enable training\n- Modifications inform Composer for future dreams\n- Enables recreation of training runs\n\nKey principle: ML Agent has leeway to make necessary adjustments, but ALL adjustments must be recorded.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:15:00.085093-08:00","updated_at":"2025-12-12T12:58:49.512701-08:00","closed_at":"2025-12-12T12:58:49.512701-08:00","close_reason":"ML Agent implemented with execute_recipe(), adjustment tracking, ExecutionResult dataclass. Records all modifications (component_skip, build_fix, train_fix). 16 new tests, 215 total passing.","dependencies":[{"issue_id":"arcfusion-a1s","depends_on_id":"arcfusion-p9w","type":"blocks","created_at":"2025-12-12T12:15:33.445742-08:00","created_by":"daemon"},{"issue_id":"arcfusion-a1s","depends_on_id":"arcfusion-9c9","type":"blocks","created_at":"2025-12-12T12:15:33.493527-08:00","created_by":"daemon"}]}
{"id":"arcfusion-a8o","title":"Multi-agent autonomous development system","description":"Brainstorm: 6-8 AI agents working together:\n- 1 Project Maintainer agent: approves/rejects PRs on GitHub\n- Other agents: claim beads issues, write code, submit PRs to the repo\n- Autonomous development workflow with human oversight via PR reviews\n\n## Architecture\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    HUMAN OVERSIGHT                          â”‚\nâ”‚              (GitHub PR Reviews, bd issues)                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â–²\n                            â”‚ PRs\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                 PROJECT MAINTAINER AGENT                    â”‚\nâ”‚  - Reviews PRs from worker agents                           â”‚\nâ”‚  - Enforces code standards                                  â”‚\nâ”‚  - Manages release/merge decisions                          â”‚\nâ”‚  - Creates/prioritizes bd issues                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â–²\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚             â”‚             â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  WORKER AGENT 1 â”‚ â”‚ WORKER AGENT 2â”‚ â”‚ WORKER AGENT N  â”‚\nâ”‚  (Feature Dev)  â”‚ â”‚ (Bug Fixes)   â”‚ â”‚ (Testing/Docs)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚                  â”‚                  â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    SHARED STATE                             â”‚\nâ”‚  - bd (beads) issue tracker                                 â”‚\nâ”‚  - Git repo (branches per agent)                            â”‚\nâ”‚  - Coordination file (.agents/locks.json?)                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Agent Roles\n\n| Agent | Responsibility | Tools |\n|-------|---------------|-------|\n| Maintainer | PR review, issue triage, release management | gh, bd, git |\n| Feature Dev | New features, arcfusion improvements | claude code, bd, gh |\n| Bug Fixer | Fix issues, handle errors | claude code, bd, gh |\n| Researcher | Ingest papers, analyze architectures | arcfusion CLI, bd |\n| Trainer | Run cloud training, benchmarks | Modal, bd |\n| Test/QA | Write tests, validate PRs | pytest, bd |\n\n## Coordination Protocol\n\nWorker claims issue:\n1. bd update \u003cid\u003e --status=in_progress --assignee=agent-1\n2. git checkout -b agent-1/\u003cissue-id\u003e\n3. Do work\n4. gh pr create --title \"...\" --body \"Closes \u003cissue\u003e\"\n5. bd update \u003cid\u003e --status=review\n\n## Safety Layers\n\n1. First layer: Maintainer agent reviews all PRs\n2. Second layer: Human reviews Maintainer's merge decisions\n3. Third layer: CI/tests must pass\n\n## Implementation Options\n\nA. Cron-based polling (simple)\nB. Event-driven via GitHub webhooks (responsive)\nC. Long-running daemon processes (complex)\n\n## MVP\n\nStart with:\n1. One worker agent that picks from bd ready, writes code, submits PR\n2. Human as maintainer (manual PR review)\n3. Iterate from there\n\n## Open Questions\n\n- Where do agents run? Local? Cloud VMs? Modal?\n- Cost model for API tokens per agent session\n- Specialization vs one generalist agent\n- Auto-merge threshold or always require human?","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-14T16:19:13.545493-08:00","updated_at":"2025-12-16T14:07:13.714376-08:00"}
{"id":"arcfusion-a9f","title":"Local training validation before cloud deploy","description":"Test training runs locally before deploying to cloud:\n- Catch errors early (syntax, shape mismatches, missing imports)\n- Run quick ~50 step validation\n- Minimize wasted cloud GPU time from training errors","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T16:19:16.918777-08:00","updated_at":"2025-12-14T16:27:47.008033-08:00","closed_at":"2025-12-14T16:27:47.008033-08:00","close_reason":"Implemented validate_local.py - runs forward/backward pass on CPU"}
{"id":"arcfusion-abz","title":"Speed up Modal training - larger GPUs or less data","description":"Current training on Modal T4 GPUs times out at 30 min. Options: 1) Use A10G/A100 GPUs 2) Reduce dataset size 3) Use step-based training vs epochs 4) Reduce model size. Goal: complete training runs in \u003c10 min for fast iteration.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:27.035282-08:00","updated_at":"2025-12-13T16:28:37.654998-08:00","closed_at":"2025-12-13T16:28:37.654998-08:00","close_reason":"Added mixed precision (FP16 autocast + GradScaler) and upgraded to A10G GPU in cloud_train_fair.py. Expected 2-3x speedup."}
{"id":"arcfusion-agl","title":"Leaderboard efficiency metric: combine PPL and training time","description":"Currently leaderboard sorts by PPL only. We want an efficiency metric that balances:\n- Lower PPL (better quality)\n- Lower training time (faster iteration)\n\nOptions to consider:\n- PPL / time (lower is better)\n- PPL * log(time) \n- Pareto frontier visualization\n- Configurable weighting\n\nThis helps identify models that are both good AND fast to train.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T13:46:03.972126-08:00","updated_at":"2025-12-16T13:48:45.93816-08:00","closed_at":"2025-12-16T13:48:45.93816-08:00","close_reason":"Added get_efficiency_leaderboard() to db.py. Formula: PPL * (time/300)^0.5. Fast models now rank higher - Transformer_MHA10 (72s) beats Mamba (481s) on efficiency despite higher PPL."}
{"id":"arcfusion-aon","title":"Experiment: Run architecture search on H100 with ThunderKittens","description":"## Goal\nRun architecture search experiments on H100 with ThunderKittens to unlock:\n1. Faster standard attention (10-40% speedup)\n2. **14x faster linear attention** (Based/Hedgehog)\n3. Optimized Mamba-2 kernels\n\n## Cost Analysis\n- A100: ~$2/hr, no TK\n- H100: ~$4/hr, full TK support\n\n## Experiments to Run\n\n### Priority 1: Linear Attention\n- Test Based attention with TK (14x speedup claimed)\n- Test Hedgehog attention\n- If 14x speedup holds: 7x more experiments per dollar than A100!\n\n### Priority 2: Mamba-2\n- Our Mamba layers were 3.5x slower on A100\n- TK-optimized Mamba-2 might bring this under 1.5x threshold\n\n### Priority 3: Standard Attention  \n- 10-40% speedup would make H100 cost-competitive with A100\n- Focus on longer sequences where gains are bigger\n\n## Success Criteria\n- Document actual TK speedups vs A100 SDPA baseline\n- Identify which architectures become viable with TK\n- Calculate cost-per-experiment for H100+TK vs A100+SDPA\n\n## Resources\n- scripts/tk_attention.py (ready for TK)\n- scripts/test_tk_modal.py (benchmark script)\n- Finding 8d104ee983e6 (TK requires H100)","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-16T16:32:09.830032-08:00","updated_at":"2025-12-16T16:32:23.57588-08:00"}
{"id":"arcfusion-avc","title":"Validation: Can we discover Transformer without the attention paper?","description":"## Research Question\n\nIf the \"Attention Is All You Need\" paper had never been published, could ArcFusion's dream engine independently discover the Transformer architecture by composing components from other papers?\n\n## Why This Matters\n\nThis is a key validation of the entire ArcFusion approach:\n- If YES â†’ The system can genuinely discover novel architectures\n- If NO â†’ We're just recombining known patterns, not innovating\n\n## Experiment Design\n\n### Setup\n1. Remove Transformer and all attention-related components from the database\n2. Seed only with pre-2017 components (RNNs, CNNs, LSTMs, embeddings, etc.)\n3. Add components from papers that influenced attention (memory networks, seq2seq, etc.)\n\n### Test\n1. Run dream engine with various strategies (greedy, random, crossover, mutate)\n2. Generate many candidate architectures\n3. Analyze: Do any resemble Transformer's key innovations?\n   - Multi-head attention pattern\n   - Encoder-decoder with cross-attention\n   - Positional encodings + self-attention (no recurrence)\n\n### Success Criteria\n- **Strong success**: Dream engine produces attention-like mechanism\n- **Partial success**: Produces components that could lead to attention with minor tweaks\n- **Failure**: Only produces RNN/CNN variants\n\n## Dependencies\n- Robust component extraction from pre-2017 papers\n- Good relationship scoring between components\n- Multiple dream strategies working well\n\n## Notes\n- This may require ingesting more historical papers (2014-2016 era)\n- May need to tune dream engine parameters\n- Could be a good benchmark for measuring system improvement over time","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T14:19:39.83596-08:00","updated_at":"2025-12-11T14:43:31.692448-08:00","closed_at":"2025-12-11T14:43:31.692448-08:00","close_reason":"Experiment complete. PARTIAL SUCCESS: Dream engine found architecture with attention+parallelism+position (5/5 score). Key finding: Can get 80% of the way but misses self-attention, multi-head, and dot-product innovations that required human insight."}
{"id":"arcfusion-ayd","title":"Track component configurations (sub-architectures)","description":"## Problem\n\nCurrently we track full engines (e.g., Transformer with 15 components) but not successful sub-configurations. For example, if 7 specific components from Transformer in a certain order consistently produce good results, we have no way to capture and reuse that pattern.\n\n## Proposed Solution\n\nTrack \"configurations\" - ordered subsets of components that work well together:\n\n### New DB Table: `component_configurations`\n```sql\nCREATE TABLE component_configurations (\n    config_id TEXT PRIMARY KEY,\n    name TEXT,                          -- e.g., \"Transformer Core Block\"\n    description TEXT,\n    component_ids TEXT,                 -- JSON array of component IDs in order\n    source_engine_id TEXT,              -- Engine this was derived from (optional)\n    configuration_score REAL,           -- How well this config performs\n    usage_count INTEGER DEFAULT 0,      -- How often used in dreams\n    validated BOOLEAN DEFAULT 0,        -- Has been tested/validated\n    created_at TIMESTAMP\n);\n```\n\n### Key Features\n1. **Configuration extraction**: Analyze engines to find common successful patterns\n2. **Configuration scoring**: Track which configs produce good dream results\n3. **Dream integration**: Use proven configs as building blocks for new architectures\n4. **Configuration discovery**: Find configs that appear across multiple engines\n\n### Use Cases\n- \"Attention + LayerNorm + FFN\" block appears in many architectures â†’ track as config\n- User validates a dreamed architecture â†’ extract and save its sub-configs\n- Dream engine prefers using proven configs when available\n\n## Acceptance Criteria\n- [ ] Add `component_configurations` table to schema\n- [ ] Add CRUD operations for configurations\n- [ ] Add method to extract configs from existing engines\n- [ ] Integrate with composer to prefer known-good configs\n- [ ] CLI commands: `arcfusion config list/show/extract`","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T14:17:56.139247-08:00","updated_at":"2025-12-11T14:27:49.145668-08:00","closed_at":"2025-12-11T14:27:49.145668-08:00","close_reason":"Feature complete: DB schema, CRUD ops, extraction, CLI commands, and dream integration all implemented. 167 tests passing."}
{"id":"arcfusion-b59","title":"Add results-aware dream strategy","description":"## Problem\nWhen dreaming up new architectures, the system doesn't reference past benchmark results from the DB. A human ML researcher would study what worked before (Mamba's quality wins, GQA/MQA's speed wins) and use that to inform new designs.\n\n## Solution\nCreate a results-aware dream strategy that:\n\n1. **Queries Past Results** - Load training_runs from DB to understand what worked\n2. **Identifies Winning Components** - Which attention types excel at quality vs speed?\n3. **Analyzes Trade-offs** - Use efficiency_score (quality * speed) to rank approaches  \n4. **Generates Informed Hypotheses** - Dream architectures that combine winning traits\n\n## Implementation Steps\n1. Add `get_component_performance_stats()` to db.py - aggregate results by component type\n2. Add `results_aware` strategy to composer.py - queries DB before dreaming\n3. Strategy should output reasoning: 'Mamba has best quality (1.25x baseline), GQA best speed (3.78x), trying hybrid with GQA projection on Mamba core'\n4. Save dreamed config with reference to which past results inspired it\n\n## Acceptance Criteria\n- [ ] Dream strategy can access and interpret past benchmark results\n- [ ] Strategy logs reasoning about why it chose specific components\n- [ ] New architectures include metadata about their design rationale\n\n## Related\n- arcfusion-zdr: Patch Transformer weaknesses (analyzes what doesn't work)\n- Current efficiency scores: Mamba (0.36), Hybrid (0.42), MHA (1.07), GQA (3.63)","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T06:18:25.18003-08:00","updated_at":"2025-12-15T06:24:22.092571-08:00","closed_at":"2025-12-15T06:24:22.092571-08:00","close_reason":"Implemented results_aware dream strategy with get_model_performance_stats() in db.py and results_aware_compose() in composer.py"}
{"id":"arcfusion-b9y","title":"Use cached baseline average - train transformer N times, store mean result","description":"The vanilla Transformer_MHA baseline is critical for fair comparisons. Current approach trains it once and caches. Better approach:\n\n1. **Train baseline N times** (e.g., 3-5 runs) with different seeds\n2. **Store all runs** in training_runs table  \n3. **Compute mean + stddev** for perplexity\n4. **Use mean as reference** for vs_baseline_pct calculations\n5. **Never retrain** - just pull cached average from DB\n\nBenefits:\n- More reliable baseline (reduces variance from random init)\n- Faster benchmarking (skip baseline training entirely)\n- Statistical significance (can compute confidence intervals)\n\nImplementation:\n- Add 'seed' field to TrainingRun and CONFIG\n- Add get_baseline_stats(config) -\u003e (mean_ppl, std_ppl, n_runs)\n- Run baseline 3-5x on first setup, then never again","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-13T17:42:04.211643-08:00","updated_at":"2025-12-13T19:07:43.273021-08:00","closed_at":"2025-12-13T19:07:43.273021-08:00","close_reason":"Implemented baseline averaging: added seed to TrainingRun, get_baseline_stats(), get_baseline_seeds_needed() in db.py. Updated cloud_train_fair.py to train N baselines with different seeds and use mean perplexity for comparisons. Testing blocked by Modal service RemoteError - retry later."}
{"id":"arcfusion-bpa","title":"Clarify surrogate model update flow: predictions should update from new training run data","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T11:03:57.014449-08:00","updated_at":"2025-12-16T11:06:10.168356-08:00","closed_at":"2025-12-16T11:06:10.168356-08:00","close_reason":"No issue - confirmed implementation is correct. Surrogate model already updates from training_runs table via retrain_if_needed() flow."}
{"id":"arcfusion-bzw","title":"Create web UI for database visualization","description":"Build a web-based dashboard to visualize the ArcFusion database. Features: 1) Component browser with search/filter by category 2) Engine/architecture viewer 3) Training leaderboards (PPL and efficiency) 4) Dream candidates pipeline view 5) Surrogate model accuracy stats. Tech options: Flask/FastAPI + simple HTML, Streamlit, or Gradio for quick prototyping.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T14:23:17.916052-08:00","updated_at":"2025-12-16T14:32:20.634074-08:00","closed_at":"2025-12-16T14:32:20.634074-08:00","close_reason":"Created Streamlit web UI at scripts/web_ui.py. Features: Overview dashboard, Component browser with search/filter, Engine viewer, PPL and Efficiency leaderboards, Dream candidates pipeline view, Surrogate model stats. Run with: streamlit run scripts/web_ui.py"}
{"id":"arcfusion-ctf","title":"PyTorch code generation from dreamed architectures","description":"Generate runnable PyTorch code from dreamed component combinations, using code_sketch fields and interface compatibility","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T12:15:49.586369-08:00","updated_at":"2025-12-11T12:37:51.398835-08:00","closed_at":"2025-12-11T12:37:51.398835-08:00","close_reason":"Implemented CodeGenerator with generate CLI command. Generates valid PyTorch nn.Module code from dreamed architectures using greedy, random, crossover, and mutate strategies."}
{"id":"arcfusion-d7u","title":"Core database schema with components, engines, relationships","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:17.526253-08:00","updated_at":"2025-12-11T12:15:31.395323-08:00","closed_at":"2025-12-11T12:15:31.395323-08:00","close_reason":"Implemented: 8 tables (components, engines, relationships, compatibility, papers, benchmarks, dreams, validations)"}
{"id":"arcfusion-dd3","title":"Set realistic expectations: moonshot project perspective","description":"This is a moonshot research project exploring novel ML architecture composition. Current results show MHA Transformer (72s, 247.6 PPL) beats Mamba variants on efficiency despite Mamba having better raw PPL (212.1). This is expected and valuable data - we're learning what works. The goal is exploration and discovery, not immediate SOTA. Celebrate the infrastructure we've built: surrogate model, dreaming strategies, cloud training pipeline, efficiency metrics.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-16T14:23:16.5609-08:00","updated_at":"2025-12-16T14:23:33.621391-08:00"}
{"id":"arcfusion-e2f","title":"Validation: Implement Transformer using only DB components via auto-pipeline","description":"## Goal\n\nTest the auto-validation pipeline by implementing the Transformer architecture using ONLY components extracted into our database - no external code.\n\n## Why This Test\n\n- We KNOW Transformer works (it's a winning recipe)\n- If our pipeline can reconstruct it from DB components and train successfully, the pipeline is validated\n- Fair comparison: only use what we've extracted, not code from the internet\n\n## Test Protocol\n\n1. **Component Selection**\n   - Query DB for Transformer-relevant components\n   - Use: MultiHeadAttention, FeedForward, LayerNorm, Embedding, etc.\n   - Only components with code sketches from our extraction\n\n2. **Model Assembly**\n   - Use CodeGenerator to assemble from DB components\n   - Standard Transformer config: 6 layers, 512 dim, 8 heads\n   - Keep it small for fast iteration\n\n3. **Training**\n   - Small dataset (WikiText-2 or similar)\n   - Short training run (sanity check, not SOTA)\n   - Track loss curves\n\n4. **Benchmark**\n   - Measure perplexity\n   - Compare to known Transformer baselines\n   - Store in benchmark_results\n\n## Success Criteria\n\n- [ ] Model assembles from DB components without manual code\n- [ ] Model trains without errors\n- [ ] Achieves reasonable perplexity (not random)\n- [ ] Results stored in DB\n- [ ] Pipeline proven end-to-end\n\n## Why \"Fair Comparison\" Matters\n\nIf we copy Transformer code from HuggingFace, we're not testing our system. The whole point is: can our extracted component representations actually work?\n\n## Dependencies\n\n- Auto-validation pipeline (arcfusion-040)\n- Sufficient Transformer components in DB (we have these from seeds)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T15:13:20.534039-08:00","updated_at":"2025-12-11T17:47:48.47835-08:00","closed_at":"2025-12-11T17:47:48.47835-08:00","close_reason":"Validation pipeline works end-to-end: Transformer builds from DB components (5 components, 1M params), trains successfully (50 steps, loss ~6.9), and benchmarks run correctly. Fixed vocab_size propagation in codegen.py and validator.py.","dependencies":[{"issue_id":"arcfusion-e2f","depends_on_id":"arcfusion-040","type":"blocks","created_at":"2025-12-11T15:13:26.60807-08:00","created_by":"daemon"}]}
{"id":"arcfusion-e6e","title":"Ensure dreamer checks dream_candidates table before dreaming","description":"The dream_and_train.py pipeline should check dream_candidates table before dreaming new architectures to:\n\n1. **Avoid re-dreaming identical architectures** - Check if component combo already exists\n2. **Learn from past results** - Use actual vs predicted PPL to guide future dreams\n3. **Track exploration coverage** - Know which regions of architecture space are under-explored\n\nImplementation ideas:\n- Query dream_candidates for existing component hashes before dreaming\n- Bias away from already-explored regions (unless results were promising)\n- Surface candidates that were predicted good but never trained\n- Use surrogate accuracy metrics to improve predictions\n\nRelated: arcfusion-sh6 (dream_candidates table)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T07:18:22.684481-08:00","updated_at":"2025-12-16T07:23:40.16168-08:00","closed_at":"2025-12-16T07:23:40.16168-08:00","close_reason":"Implemented dream dedup: get_existing_candidate_hashes(), get_untrained_promising_candidates(), modified Phase 1 loop to skip already-dreamed architectures using component hash matching"}
{"id":"arcfusion-ecq","title":"Brainstorm: Ingest more arXiv papers or focus on training?","description":"## Question\nShould we prioritize ingesting more arXiv papers into the component database, or focus on training/benchmarking with what we have?\n\n## Decision: Focus on Training with Selective Ingestion\n\nAfter analyzing our current state:\n- 113 components already in DB (Mamba, RetentionHead, FlashAttention, RotaryEmbedding, etc.)\n- 22 engines (BERT, GPT-2, LLaMA, Falcon)\n- 7 training runs with real benchmarks showing Mamba -20% vs MHA\n\n**Strategy:**\n1. **Primary focus: Training** - Validate more architectures with our existing components\n2. **Selective ingestion** - Only add papers for specific components we want to test:\n   - Linear Attention (to compare O(n) alternatives to Mamba)\n   - RWKV (another recurrent approach to compare)\n   - NOT: papers for components we already have well-covered\n\n## Rationale\n- Training validates which components actually matter in practice\n- Our Mamba result (-20% perplexity) proves the pipeline works\n- Efficiency scores show GQA/MQA are most efficient (quality Ã— speed)\n- Better to deeply validate fewer components than broadly catalog many\n\n## Next Actions\n1. Run more training experiments with existing components\n2. If we need a specific capability (e.g., linear attention), then ingest that paper\n3. Don't ingest papers \"just in case\" - it's wasted effort\n\n## Current Benchmark Results\n\n| Model | Perplexity | Quality | Speed | Efficiency |\n|-------|-----------|---------|-------|------------|\n| Mamba | 226.1 | 1.248 | 0.29x | 0.362 |\n| Hybrid | 242.9 | 1.162 | 0.36x | 0.421 |\n| MHA | 274.8 | 1.027 | 1.04x | 1.071 |\n| GQA | 294.0 | 0.960 | 3.78x | 3.632 |\n| MQA | 294.1 | 0.960 | 3.50x | 3.355 |","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-14T07:43:14.477875-08:00","updated_at":"2025-12-15T06:10:24.508501-08:00","closed_at":"2025-12-15T06:10:24.508501-08:00","close_reason":"Decision made: Focus on training with selective paper ingestion. We have 113 components already - only ingest papers for specific capabilities we want to test (Linear Attention, RWKV). Training validates what matters."}
{"id":"arcfusion-ekl","title":"Ensure Claude compaction happens before training, not during","description":"Claude compaction can disrupt Modal training runs. When context gets long, compaction can happen mid-training which may cause timeouts, retries, or connection issues with Modal containers. Solution: trigger compaction explicitly before starting long training runs, or use a separate terminal for training.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-15T14:37:58.439392-08:00","updated_at":"2025-12-15T20:17:05.534341-08:00","closed_at":"2025-12-15T20:17:05.534341-08:00","close_reason":"Added cloud training best practices section to AGENTS.md documenting background execution requirement"}
{"id":"arcfusion-f0p","title":"Re-benchmark Mamba-2 with ThunderKittens kernels","description":"## Goal\nTest if ThunderKittens fused Mamba-2 kernels bring Mamba within our 1.5x efficiency threshold.\n\n## Current State\n- Pure Mamba: 3.5x slower, 25% PPL gain â†’ REJECTED (exceeds 1.5x, below 30%)\n- TK claims 'several x faster' Mamba-2 kernels\n\n## Experiment\n1. Implement Mamba-2 with TK fused kernels\n2. Train on same WikiText-2 setup\n3. Measure: PPL, training time, memory\n\n## Success Criteria\nIf Mamba-2 + TK achieves:\n- â‰¤1.5x slowdown: viable for any PPL gain\n- OR \u003e1.5x but with 30%+ PPL gain: viable\n\n## Depends on\n- ThunderKittens integration (arcfusion-???)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T15:59:07.102603-08:00","updated_at":"2025-12-16T16:26:36.281809-08:00","closed_at":"2025-12-16T16:26:36.281809-08:00","close_reason":"Blocked: TK requires H100. Mamba-2 TK kernels only available for H100.","dependencies":[{"issue_id":"arcfusion-f0p","depends_on_id":"arcfusion-5l7","type":"blocks","created_at":"2025-12-16T15:59:15.361838-08:00","created_by":"daemon"}]}
{"id":"arcfusion-f73","title":"Add test coverage for untested modules","description":"composer.py, dedup.py, decomposer.py, and seeds.py have no dedicated tests. Add tests for:\n- All 4 dream strategies (greedy, random, mutate, crossover)\n- Duplicate detection and merging\n- Paper decomposition\n- Database seeding","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T13:02:07.568054-08:00","updated_at":"2025-12-11T13:07:14.417626-08:00","closed_at":"2025-12-11T13:07:14.417626-08:00","close_reason":"Added 102 new tests (157 total) covering composer, dedup, decomposer, and seeds modules"}
{"id":"arcfusion-fi7","title":"Analyze ML paper coverage: need more components?","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T11:33:56.147604-08:00","updated_at":"2025-12-16T11:35:28.763918-08:00","closed_at":"2025-12-16T11:35:28.763918-08:00","close_reason":"Analysis: 117 components provide good coverage. Have MHA/GQA/MQA/Mamba/MoE/xLSTM/Hyena. Future adds: ring attention, paged attention, quantization. Current coverage sufficient for architecture search."}
{"id":"arcfusion-fik","title":"Dream architectures targeting Transformer weaknesses + GQA/MQA wins","description":"","notes":"Completed GQA/MQA variant benchmarks:\n\nGQAMambaHeavy [GQA,M,M,M]: PPL=247.8, Time=613s - Slightly underperforms MambaHeavy\nMQAMamba [MQA,M,MQA,M]: PPL=246.0, Time=434s - EFFICIENCY WINNER: 2.2x vs MHA, good quality  \nGQAMamba5 [GQA,M,M,M,M,M]: PPL=234.6, Time=1058s - Best GQA quality, but slow\n\nKey Finding: MQAMamba is the sweet spot - better quality than GQAMambaHeavy and 30% faster training time. MQA's extreme KV-efficiency + Mamba's recurrence works well together.\n\nGenerated 30 training insights total.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-15T13:42:34.257082-08:00","updated_at":"2025-12-15T14:35:25.07846-08:00","closed_at":"2025-12-15T14:35:25.07846-08:00","close_reason":"Completed benchmarks for 3 GQA/MQA hybrid architectures. MQAMamba emerged as efficiency champion."}
{"id":"arcfusion-fks","title":"Make efficiency metric default for leaderboard","description":"Switch the default tab on the Leaderboard page from 'By Perplexity' to 'By Efficiency' since efficiency (PPL Ã— âˆš(time/300s)) better captures the tradeoff we care about.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-16T14:55:56.747603-08:00","updated_at":"2025-12-16T15:16:07.244566-08:00","closed_at":"2025-12-16T15:16:07.244566-08:00","close_reason":"Swapped leaderboard tabs - efficiency is now the default view"}
{"id":"arcfusion-frl","title":"Fuzzy deduplication with variant detection","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:20.748145-08:00","updated_at":"2025-12-11T12:15:34.510907-08:00","closed_at":"2025-12-11T12:15:34.510907-08:00","close_reason":"Implemented: ComponentDeduplicator with normalized names, semantic signatures, architecture variant detection"}
{"id":"arcfusion-gb6","title":"Evaluate Modal vs other GPU services (Lambda, RunPod, etc)","description":"","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-15T13:42:29.299997-08:00","updated_at":"2025-12-16T11:23:37.989639-08:00","closed_at":"2025-12-16T11:23:37.989639-08:00","close_reason":"Researched GPU cloud options (Dec 2025): Modal A100 ~$2.50/hr vs RunPod ~$1.19/hr (52% cheaper). Recommendation: Stay with Modal for DX, consider RunPod at 50+ runs/day. Thunder Compute cheapest at $0.66/hr but less mature."}
{"id":"arcfusion-gjd","title":"Ensure model reproducibility from leaderboard","description":"Critical: Can we recreate any model from the leaderboard? Need to verify: 1) Model code is stored in DB (model_code column exists), 2) Training config is captured (d_model, n_layers, etc), 3) Seeds are recorded for determinism, 4) Code hash allows dedup. Check cloud_train_fair.py save_result_to_db function.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-15T15:01:39.655986-08:00","updated_at":"2025-12-15T15:04:54.88578-08:00","closed_at":"2025-12-15T15:04:54.88578-08:00","close_reason":"Fixed: save_result_to_db now stores model_code and code_hash. Backfilled all 21 existing runs. All models are now reproducible from DB."}
{"id":"arcfusion-gzl","title":"Log Complete MHA vs GQA Scaling Results","description":"## Complete MHA vs GQA Layer Scaling Results\n\nComprehensive apples-to-apples comparison of Multi-Head Attention (MHA) vs Grouped Query Attention (GQA) at 10, 12, 14, 18, 24, 32, and 40 layers.\n\n### Results Table\n\n| Layers | MHA PPL | MHA Time | GQA PPL | GQA Time | Quality Winner | Speed Winner |\n|--------|---------|----------|---------|----------|----------------|--------------|\n| 10 | 247.6 | 72s | 245.6 | 79s | GQA (-0.8%) | MHA (-9%) |\n| 12 | 243.0 | 125s | 241.9 | 110s | GQA (-0.5%) | **GQA (-12%)** |\n| 14 | 239.0 | 87s | 236.3 | 110s | GQA (-1.1%) | MHA (-21%) |\n| 18 | 235.6 | 103s | 231.9 | 129s | GQA (-1.6%) | MHA (-20%) |\n| **24** | **228.2** | **136s** | 228.5 | 177s | **MHA (+0.2%)** | **MHA (-24%)** |\n| **32** | **217.1** | **179s** | 217.9 | 224s | **MHA (+0.4%)** | **MHA (-20%)** |\n| 40 | 221.8 | 235s | crashed | - | - | - |\n\n### Key Findings\n\n1. **Crossover Point**: Around 18-24 layers\n   - At 10-18L: GQA has slight quality advantage (0.5-1.6%)\n   - At 24-32L: MHA dominates both quality AND speed\n\n2. **12 Layer Sweet Spot**: Only depth where GQA wins BOTH quality and speed\n\n3. **GQA's Advantage Disappears**: At extreme depths (24+), GQA's quality edge reverses and MHA is both faster and better quality\n\n4. **40+ Layers**: GQA hit memory/timeout issues; MHA more stable at extreme depths\n\n### Implications\n\n- For shallow models (8-18L): Consider GQA for slight quality boost\n- For deep models (24+L): Use MHA - it's faster AND better\n- The \"GQA is always better\" conventional wisdom doesn't hold at scale","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-15T18:10:30.935099-08:00","updated_at":"2025-12-15T19:39:02.06884-08:00","closed_at":"2025-12-15T19:39:02.06884-08:00","close_reason":"Formalized in training_insights table with full evidence JSON. Key insight: crossover at 18-24L where MHA takes lead over GQA."}
{"id":"arcfusion-h9y","title":"Wire up dreamer and surrogate to review findings DB","description":"Make the architecture search pipeline smarter by having both the dreamer (composer) and surrogate model consult the findings table before generating/evaluating candidates.\n\n## Goal\nUse accumulated knowledge (findings) to guide architecture search:\n- Dreamer: Bias component selection toward patterns that worked (e.g., 'more Mamba = better', 'attention at end')\n- Surrogate: Weight predictions based on known patterns and enforce efficiency constraints dynamically\n\n## Key Constraint (from findings)\n**\u003e1.5x slower is not worth it unless we're getting big PPL gains**\n- The surrogate screening should read this constraint from findings DB rather than hardcoding\n- Allows the threshold to evolve as we learn more (e.g., maybe 1.3x becomes the limit)\n\n## Implementation ideas\n1. Add `get_relevant_findings(tags)` helper to fetch findings by tag\n2. In composer.py dream strategies, query findings for 'architecture-pattern' tag\n3. In surrogate screening, query findings for 'efficiency' and 'constraint' tags\n4. Parse efficiency constraints from findings to set max_slowdown/min_ppl_gain dynamically\n5. Consider a `KnowledgeBase` abstraction that aggregates findings + training_insights\n\n## Example usage\n```python\n# Before screening, load efficiency constraints from findings\nefficiency_findings = db.list_findings(tags=['efficiency', 'constraint'])\nfor f in efficiency_findings:\n    if 'slowdown' in f.title.lower():\n        # Parse constraint parameters from finding\n        max_slowdown = extract_slowdown_threshold(f)\n```\n\n## Related findings already in DB\n- Pure Mamba beats Transformer by ~25%\n- GQAMamba is the sweet spot: +10% quality, 1.9x slower\n- More Mamba layers = better quality\n- Attention position matters: end \u003e sandwich \u003e start\n- **Efficiency constraint: reject \u003e1.5x slower unless 15%+ PPL gains**","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T15:25:09.436875-08:00","updated_at":"2025-12-16T15:33:29.921108-08:00","closed_at":"2025-12-16T15:33:29.921108-08:00","close_reason":"Implemented: get_findings_by_tags(), get_efficiency_constraints(), get_architecture_patterns() in db.py. Wired up surrogate screening to read constraints dynamically. Added findings_guided dreaming strategy."}
{"id":"arcfusion-hjg","title":"Implement local testing process before cloud deployment","description":"## Problem\nDeploying untested code to Modal wastes GPU time and money when there are bugs. We should validate locally first.\n\n## Proposed Process\n1. **Syntax check**: Run model code through exec() locally\n2. **Shape test**: Forward pass with small batch (2, 16)\n3. **Parameter count**: Verify comparable to baseline (~30M params)\n4. **Gradient test**: One backward pass to check autograd works\n5. **Only then**: Deploy to Modal for full training\n\n## Implementation\nCreate a `test_model_locally()` function that:\n```python\ndef test_model_locally(model_code: str, model_name: str) -\u003e bool:\n    # 1. Parse and exec\n    ns = {}\n    exec(model_code, ns)\n    model_class = ns.get(model_name)\n    \n    # 2. Instantiate\n    model = model_class(d_model=256, vocab_size=50257, n_layers=4, n_heads=8)\n    \n    # 3. Forward pass\n    x = torch.randint(0, 50257, (2, 16))\n    out = model(x)\n    assert out.shape == (2, 16, 50257)\n    \n    # 4. Backward pass\n    loss = out.sum()\n    loss.backward()\n    \n    return True\n```\n\n## Benefits\n- Catch bugs before spending GPU credits\n- Faster iteration (no Modal cold start)\n- Validate shapes and gradients work\n- Parameter count sanity check\n\n## Acceptance Criteria\n- [ ] Local test function implemented\n- [ ] Integrated into cloud_train_fair.py workflow\n- [ ] All new models pass local test before cloud","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T08:02:28.935724-08:00","updated_at":"2025-12-14T16:43:04.709535-08:00","closed_at":"2025-12-14T16:43:04.709535-08:00","close_reason":"Duplicate of arcfusion-a9f (local validation). arcfusion-a9f was completed previously."}
{"id":"arcfusion-hns","title":"Focus on text modality only - no vision/audio/multimodal","description":"ArcFusion should focus exclusively on text/language model architectures. No vision, audio, or multimodal components. This keeps the scope manageable and allows fair comparisons between attention mechanisms on language modeling tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T16:46:57.824019-08:00","updated_at":"2025-12-13T16:53:55.36472-08:00","closed_at":"2025-12-13T16:53:55.36472-08:00","close_reason":"Established text-only focus. Training script now uses WikiText-2 with GPT-2 tokenizer. All benchmarks are language modeling (perplexity)."}
{"id":"arcfusion-iyg","title":"Always include vanilla transformer as baseline in benchmarks","description":"Every benchmark run should include a vanilla transformer (standard MHA) as the baseline. This provides a consistent reference point to measure improvements or regressions from architectural changes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:30.956414-08:00","updated_at":"2025-12-13T16:28:38.899894-08:00","closed_at":"2025-12-13T16:28:38.899894-08:00","close_reason":"Baseline transformer (Transformer_MHA) is now always trained first in cloud_train_fair.py. All other models show vs_baseline comparison."}
{"id":"arcfusion-k53","title":"Surrogate model should predict both PPL and training time","description":"","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T06:46:37.712226-08:00","updated_at":"2025-12-16T06:52:30.894846-08:00","closed_at":"2025-12-16T06:52:30.894846-08:00","close_reason":"Implemented dual PPL+time prediction. PPL correlation 0.825, Time correlation 0.700. Feature importance shows n_layers drives PPL, has_mamba drives time.","comments":[{"id":3,"issue_id":"arcfusion-k53","author":"bryceroche","text":"Currently the surrogate model only predicts PPL. It should also predict training time so we can optimize for efficiency (quality per compute). Changes needed:\n- Add time_seconds as a second target in SurrogateModel\n- Modify predict() to return (pred_ppl, pred_time)\n- Update screening to allow ranking by efficiency (ppl * time) or pareto-optimal","created_at":"2025-12-16T14:46:54Z"}]}
{"id":"arcfusion-kug","title":"Reconcile contradictory findings: 1.5x slowdown vs GQAMamba 1.9x","description":"We have contradictory findings in the DB:\n\n## Contradiction\n1. **Efficiency constraint**: '\u003e1.5x slower is not worth it unless big PPL gains'\n2. **GQAMamba recommendation**: 'sweet spot at +10% quality, 1.9x slower'\n\nBut 1.9x \u003e 1.5x, and +10% PPL gain may not qualify as 'big' (we set threshold at 15%).\n\n## Options to resolve\n1. **Relax the constraint**: Change 1.5x â†’ 2.0x threshold\n2. **Remove GQAMamba recommendation**: It violates our own constraint\n3. **Tiered thresholds**: Allow more slowdown for larger PPL gains (graduated scale)\n4. **Redefine 'big gains'**: Lower min_ppl_gain_for_slow from 15% â†’ 10%\n\n## Action needed\n- Decide which findings to keep/modify\n- Update findings DB accordingly\n- Ensure dreamer and surrogate use consistent constraints","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-16T15:40:28.594026-08:00","updated_at":"2025-12-16T15:44:53.159661-08:00","closed_at":"2025-12-16T15:44:53.159661-08:00","close_reason":"Resolved: Changed threshold to 30% PPL gain. Demoted GQAMamba (1.9x \u003e 1.5x). Updated defaults in db.py and dream_and_train.py. Pattern matcher now skips historical/exceeds findings."}
{"id":"arcfusion-l3a","title":"Expand paper knowledge base","description":"Analyze more seminal papers: GPT-3, PaLM, Gemini, Claude architecture (if published), Mixtral, Phi, etc.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-11T12:15:50.758608-08:00","updated_at":"2025-12-12T14:17:32.259717-08:00","closed_at":"2025-12-12T14:17:32.259717-08:00","close_reason":"Covered by arcfusion-xol - papers/architectures expanded together."}
{"id":"arcfusion-l3i","title":"Cache wiki training data for faster experiments","description":"","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-15T13:42:28.502642-08:00","updated_at":"2025-12-16T06:30:02.655828-08:00","closed_at":"2025-12-16T06:30:02.655828-08:00","close_reason":"Added Modal Volume cache for WikiText-2 dataset - persists between runs for faster startup"}
{"id":"arcfusion-mnx","title":"Add training insights table with auto-summary after runs","description":"## Goal\nAfter each training run, automatically generate and store a text summary of what worked and what didn't. These insights inform future dreaming sessions.\n\n## DB Schema Changes\n\n```sql\n-- New table for storing insights/findings\nCREATE TABLE IF NOT EXISTS training_insights (\n    id TEXT PRIMARY KEY,\n    created_at TEXT DEFAULT (datetime('now')),\n    \n    -- What triggered this insight\n    source_run_id TEXT,              -- Training run that led to this insight\n    source_comparison TEXT,          -- e.g., 'AttnFirst vs MambaHeavy'\n    \n    -- The insight itself\n    category TEXT,                   -- 'architecture', 'attention', 'efficiency', 'training'\n    title TEXT,                      -- Short summary: 'Attention at END \u003e START'\n    description TEXT,                -- Full explanation\n    \n    -- Supporting evidence\n    evidence_json TEXT,              -- JSON with PPL comparisons, times, etc.\n    confidence REAL DEFAULT 0.8,     -- 0.0-1.0\n    \n    -- For retrieval\n    tags TEXT,                       -- Comma-separated: 'mamba,hybrid,attention,position'\n    \n    FOREIGN KEY (source_run_id) REFERENCES training_runs(id)\n);\n\nCREATE INDEX IF NOT EXISTS idx_insights_category ON training_insights(category);\nCREATE INDEX IF NOT EXISTS idx_insights_tags ON training_insights(tags);\n```\n\n## Python Implementation\n\n### 1. Add to db.py - Dataclass and CRUD\n\n```python\n@dataclass\nclass TrainingInsight:\n    id: str = ''\n    created_at: str = ''\n    source_run_id: str = ''\n    source_comparison: str = ''\n    category: str = ''  # architecture, attention, efficiency, training\n    title: str = ''\n    description: str = ''\n    evidence_json: str = ''\n    confidence: float = 0.8\n    tags: str = ''\n\n\nclass ArcFusionDB:\n    # ... existing code ...\n    \n    def _init_insights_table(self):\n        '''Create training_insights table if not exists'''\n        self.conn.execute('''\n            CREATE TABLE IF NOT EXISTS training_insights (\n                id TEXT PRIMARY KEY,\n                created_at TEXT DEFAULT (datetime('now')),\n                source_run_id TEXT,\n                source_comparison TEXT,\n                category TEXT,\n                title TEXT,\n                description TEXT,\n                evidence_json TEXT,\n                confidence REAL DEFAULT 0.8,\n                tags TEXT\n            )\n        ''')\n        self.conn.execute('CREATE INDEX IF NOT EXISTS idx_insights_category ON training_insights(category)')\n        self.conn.commit()\n    \n    def add_insight(self, insight: TrainingInsight) -\u003e str:\n        '''Add a new training insight'''\n        if not insight.id:\n            insight.id = str(uuid.uuid4())[:12]\n        if not insight.created_at:\n            insight.created_at = datetime.now().isoformat()\n        \n        self.conn.execute('''\n            INSERT INTO training_insights \n            (id, created_at, source_run_id, source_comparison, category, title, description, evidence_json, confidence, tags)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        ''', (insight.id, insight.created_at, insight.source_run_id, insight.source_comparison,\n              insight.category, insight.title, insight.description, insight.evidence_json,\n              insight.confidence, insight.tags))\n        self.conn.commit()\n        return insight.id\n    \n    def get_insights(self, category: str = None, tags: list = None, min_confidence: float = 0.0) -\u003e list:\n        '''Get insights filtered by category and/or tags'''\n        query = 'SELECT * FROM training_insights WHERE confidence \u003e= ?'\n        params = [min_confidence]\n        \n        if category:\n            query += ' AND category = ?'\n            params.append(category)\n        \n        if tags:\n            for tag in tags:\n                query += ' AND tags LIKE ?'\n                params.append(f'%{tag}%')\n        \n        query += ' ORDER BY created_at DESC'\n        rows = self.conn.execute(query, params).fetchall()\n        return [TrainingInsight(**dict(r)) for r in rows]\n    \n    def get_insights_for_dreaming(self) -\u003e dict:\n        '''Get structured insights to inform dream strategies'''\n        insights = self.get_insights(min_confidence=0.7)\n        \n        result = {\n            'architecture': [],\n            'attention': [],\n            'efficiency': [],\n            'recommendations': []\n        }\n        \n        for i in insights:\n            if i.category in result:\n                result[i.category].append({\n                    'title': i.title,\n                    'description': i.description,\n                    'confidence': i.confidence\n                })\n        \n        # Generate recommendations from insights\n        for i in insights:\n            if 'best' in i.title.lower() or 'better' in i.title.lower():\n                result['recommendations'].append(i.description)\n        \n        return result\n```\n\n### 2. Auto-generate insights after training\n\n```python\ndef generate_training_insights(db: ArcFusionDB, new_run_id: str) -\u003e list:\n    '''Auto-generate insights by comparing new run to existing runs'''\n    import json\n    \n    new_run = db.get_training_run(new_run_id)\n    if not new_run or not new_run.success:\n        return []\n    \n    insights = []\n    all_runs = db.list_training_runs(success_only=True)\n    \n    # Group by attention type for comparison\n    def get_attn_type(name):\n        if 'Mamba' in name and 'Hybrid' not in name and 'Heavy' not in name and 'First' not in name:\n            return 'SSM'\n        elif 'MHA' in name:\n            return 'MHA'\n        elif 'GQA' in name:\n            return 'GQA'\n        elif 'MQA' in name:\n            return 'MQA'\n        elif 'Hybrid' in name:\n            return 'Hybrid'\n        elif 'Heavy' in name:\n            return 'MambaHeavy'\n        elif 'First' in name:\n            return 'AttnFirst'\n        return 'Unknown'\n    \n    # Compare to baseline (MHA)\n    baseline_runs = [r for r in all_runs if 'MHA' in r.model_name and 'Hybrid' not in r.model_name]\n    if baseline_runs:\n        baseline_ppl = sum(r.perplexity for r in baseline_runs) / len(baseline_runs)\n        \n        if new_run.perplexity \u003c baseline_ppl * 0.95:  # 5%+ improvement\n            quality_gain = (baseline_ppl - new_run.perplexity) / baseline_ppl * 100\n            insights.append(TrainingInsight(\n                source_run_id=new_run_id,\n                source_comparison=f'{new_run.model_name} vs MHA baseline',\n                category='architecture',\n                title=f'{get_attn_type(new_run.model_name)} beats baseline by {quality_gain:.1f}%',\n                description=f'{new_run.model_name} achieved PPL {new_run.perplexity:.1f} vs baseline {baseline_ppl:.1f}. '\n                           f'This {get_attn_type(new_run.model_name)} architecture shows {quality_gain:.1f}% quality improvement.',\n                evidence_json=json.dumps({\n                    'new_ppl': new_run.perplexity,\n                    'baseline_ppl': baseline_ppl,\n                    'improvement_pct': quality_gain\n                }),\n                confidence=0.9 if quality_gain \u003e 10 else 0.7,\n                tags=f'{get_attn_type(new_run.model_name).lower()},quality,improvement'\n            ))\n    \n    # Compare similar architectures (e.g., AttnFirst vs MambaHeavy)\n    similar_runs = [r for r in all_runs if r.id != new_run_id and \n                   r.parameters == new_run.parameters]  # Same size models\n    \n    for other in similar_runs:\n        if abs(new_run.perplexity - other.perplexity) \u003e 5:  # Significant difference\n            better = new_run if new_run.perplexity \u003c other.perplexity else other\n            worse = other if new_run.perplexity \u003c other.perplexity else new_run\n            \n            insights.append(TrainingInsight(\n                source_run_id=new_run_id,\n                source_comparison=f'{better.model_name} vs {worse.model_name}',\n                category='attention',\n                title=f'{get_attn_type(better.model_name)} \u003e {get_attn_type(worse.model_name)} for quality',\n                description=f'{better.model_name} (PPL {better.perplexity:.1f}) outperforms '\n                           f'{worse.model_name} (PPL {worse.perplexity:.1f}). '\n                           f'Difference: {worse.perplexity - better.perplexity:.1f} PPL.',\n                evidence_json=json.dumps({\n                    'better': {'model': better.model_name, 'ppl': better.perplexity},\n                    'worse': {'model': worse.model_name, 'ppl': worse.perplexity}\n                }),\n                confidence=0.85,\n                tags=f'{get_attn_type(better.model_name).lower()},{get_attn_type(worse.model_name).lower()},comparison'\n            ))\n    \n    # Save all insights\n    for insight in insights:\n        db.add_insight(insight)\n    \n    return insights\n```\n\n### 3. Wire into training script (cloud_train_fair.py)\n\n```python\n# At the end of save_result_to_db():\ndef save_result_to_db(db, result, config, baseline_run_id=''):\n    # ... existing code to save run ...\n    run_id = db.add_training_run(run)\n    \n    # Auto-generate insights from this run\n    insights = generate_training_insights(db, run_id)\n    if insights:\n        print(f'  Generated {len(insights)} new insights')\n        for i in insights:\n            print(f'    - {i.title}')\n    \n    return run_id\n```\n\n## Example Insights That Would Be Auto-Generated\n\nFrom today's experiments, these insights would be created:\n\n1. **'SSM beats baseline by 21.5%'**\n   - Mamba achieved PPL 226.1 vs baseline 274.8\n\n2. **'MambaHeavy \u003e AttnFirst for quality'** \n   - Attention at END (PPL 247) outperforms attention at START (PPL 254)\n   - Key learning: Mamba builds representations that attention refines\n\n3. **'Interleaved attention best for hybrids'**\n   - Jamba-style 2:1 (PPL 242.9) beats both 3:1 variants\n\n## Acceptance Criteria\n- [ ] Add training_insights table to DB\n- [ ] Implement TrainingInsight dataclass\n- [ ] Implement add_insight() and get_insights() \n- [ ] Implement generate_training_insights() auto-summarizer\n- [ ] Wire into save_result_to_db()\n- [ ] Test with existing training runs","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-15T07:04:59.648085-08:00","updated_at":"2025-12-15T07:10:13.035857-08:00","closed_at":"2025-12-15T07:10:13.035857-08:00","close_reason":"Implemented training_insights table with TrainingInsight dataclass, CRUD methods (add_insight, get_insight, get_insights, delete_insight), get_insights_for_dreaming(), and generate_training_insights() auto-summarizer"}
{"id":"arcfusion-p9w","title":"Recipe dataclass: ordered components + assembly instructions","description":"Composer outputs a Recipe dataclass containing:\n- Ordered list of component IDs\n- Assembly instructions (how components connect, residuals, etc.)\n- Metadata (strategy used, score, timestamp)\n\nThis is the handoff format between Composer and ML Agent.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:14:52.735707-08:00","updated_at":"2025-12-12T12:24:41.769431-08:00","closed_at":"2025-12-12T12:24:41.769431-08:00","close_reason":"Recipe dataclass implemented with: name, component_ids, assembly instructions (connections, residuals, shapes, categories, notes), strategy, estimated_score, parent_engine_ids. Composer.create_recipe() generates Recipes from any dream strategy."}
{"id":"arcfusion-pqn","title":"Ensure all benchmark results are saved","description":"## Problem\nTraining runs produce valuable benchmark data but not all results are being persisted properly.\n\n## Current State\n- Results saved to experiments/fair_comparison_results.json\n- Some results saved to DB via add_training_run()\n- No guarantee all runs are captured\n\n## Requirements\n1. Every training run should be saved to DB automatically\n2. Results should include all metrics (loss, perplexity, time, params)\n3. Comparison results should be queryable (vs baseline %)\n4. Should be able to regenerate any results JSON from DB\n\n## Acceptance Criteria\n- [ ] All training runs persisted to DB\n- [ ] Can query historical comparisons\n- [ ] Results JSON can be regenerated from DB","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T07:37:13.522653-08:00","updated_at":"2025-12-15T06:06:13.196868-08:00","closed_at":"2025-12-15T06:06:13.196868-08:00","close_reason":"Implemented export_results_json and update_vs_baseline_pct methods in db.py, added export-results CLI command. vs_baseline_pct now calculated and persisted for all runs."}
{"id":"arcfusion-qhj","title":"Cache baseline transformer - train once, reference forever","description":"Train vanilla Transformer_MHA baseline once and save to training_runs table. Future benchmark runs should load the cached baseline result instead of retraining. Only retrain baseline when config changes (d_model, n_layers, etc.).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-13T16:55:47.117325-08:00","updated_at":"2025-12-13T17:33:18.582724-08:00","closed_at":"2025-12-13T17:33:18.582724-08:00","close_reason":"Baseline caching implemented via config hash matching. Train vanilla transformer once, lookup by config_hash in notes field."}
{"id":"arcfusion-r3o","title":"Template-based model code generation for KV-head variants","description":"Insight from arcfusion-522: Instead of hardcoding dozens of GQA/MQA model variants with different kv_heads values, use a template-based approach that generates model code dynamically. The generate_model_code(n_layers, n_kv_heads) function creates complete model definitions on the fly, enabling systematic grid exploration. See scripts/run_gqa_mqa_grid.py for implementation.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T19:09:05.522162-08:00","updated_at":"2025-12-15T19:16:45.742913-08:00","closed_at":"2025-12-15T19:16:45.742913-08:00","close_reason":"Implemented in scripts/run_gqa_mqa_grid.py - generate_model_code() function dynamically creates models with variable kv_heads and layers."}
{"id":"arcfusion-sh6","title":"DB table for dreamed model surrogate scores (even if not trained)","description":"","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T06:46:38.699795-08:00","updated_at":"2025-12-16T06:57:56.743474-08:00","closed_at":"2025-12-16T06:57:56.743474-08:00","close_reason":"Added dream_candidates table to DB with DreamCandidate dataclass, CRUD operations, and wired dream_and_train.py to save all candidates with surrogate predictions and update with actual results after training.","comments":[{"id":4,"issue_id":"arcfusion-sh6","author":"bryceroche","text":"Create a 'dream_candidates' table to store all dreamed architectures with their surrogate scores, even if not trained. This enables:\n1. Avoid re-dreaming the same architecture\n2. Track which candidates were selected vs rejected\n3. Compare surrogate predictions to actual results after training\n4. Enable smarter exploration by learning from rejected candidates\n\nSchema:\n- candidate_id, arch_hash, model_name, components_json\n- pred_ppl, pred_time, strategy, temperature\n- was_trained, actual_ppl, actual_time (nullable)\n- created_at\n\nWire dreamer to check this table before generating new candidates.","created_at":"2025-12-16T14:46:56Z"}]}
{"id":"arcfusion-sjf","title":"Test linear attention (Based/Hedgehog) with ThunderKittens","description":"## Goal\nLinear attention is O(n) vs O(nÂ²) for standard attention. With TK's 14x speedup, it might be FASTER than baseline while maintaining quality.\n\n## What to test\n1. Based linear attention with TK kernels\n2. Hedgehog (LoLCATS) linear attention with TK kernels\n\n## Experiment Setup\n- Same 30M param model architecture\n- Replace MHA with linear attention\n- Train on WikiText-2\n- Measure: PPL, training time\n\n## Why This Could Be Big\n- If linear attention is faster AND quality is acceptable\n- We get O(n) scaling for free\n- Could be the efficiency breakthrough we need\n\n## TK Performance Claims\n- Based: 14x faster than Triton\n- Hedgehog: 6.5x faster than Triton","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T15:59:08.159694-08:00","updated_at":"2025-12-16T16:26:36.33472-08:00","closed_at":"2025-12-16T16:26:36.33472-08:00","close_reason":"Blocked: TK requires H100. Based/Hedgehog linear attention only available for H100.","dependencies":[{"issue_id":"arcfusion-sjf","depends_on_id":"arcfusion-5l7","type":"blocks","created_at":"2025-12-16T15:59:15.419262-08:00","created_by":"daemon"}]}
{"id":"arcfusion-sxn","title":"Save key findings to DB - schema changes needed","description":"## Problem\nKey findings from training (like Mamba beating Transformers by 20%) should be saved to DB for historical analysis, but current schema may not support this.\n\n## Key Findings to Track\n- Architecture comparisons (A vs B perplexity delta)\n- Breakthrough results (significant improvements)\n- Failed experiments (what didn't work)\n- Component combinations tested\n- Statistical significance (std dev, n runs)\n\n## Current Schema Gaps\n- No way to link related training runs (same experiment)\n- No experiment_id or batch_id concept\n- No findings/notes table for insights\n- Limited metadata on runs\n\n## Proposed Schema Changes\n1. Add experiment_runs table to group related runs\n2. Add findings table for insights/conclusions\n3. Add experiment_id FK to training_runs\n4. Add tags/labels for categorization\n\n## Acceptance Criteria\n- [ ] Can group runs into experiments\n- [ ] Can record findings/insights with evidence\n- [ ] Can query historical comparisons\n- [ ] Can tag/categorize experiments","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-14T07:37:15.761391-08:00","updated_at":"2025-12-14T16:43:06.984131-08:00","closed_at":"2025-12-14T16:43:06.984131-08:00","close_reason":"Implemented experiments and findings tables with full CRUD operations for scientific tracking"}
{"id":"arcfusion-upx","title":"Ensure DB train table has both PPL and time to train recorded for all training runs","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T08:29:35.721805-08:00","updated_at":"2025-12-16T08:33:16.235433-08:00","closed_at":"2025-12-16T08:33:16.235433-08:00","close_reason":"Already complete: training_runs table has both perplexity and time_seconds columns, all 53 runs have both fields populated (0 missing)"}
{"id":"arcfusion-uvq","title":"Save architecture benchmark findings to DB","description":"Store the key findings from arcfusion-zdr experiments in the database:\n\n## Findings to persist\n- Pure Mamba beats Transformer by ~25% quality but is 3.5x slower\n- GQAMamba is the sweet spot: +10% quality, only 1.9x slower\n- More Mamba layers = better quality (confirmed pattern)\n- Attention position matters (end \u003e sandwich \u003e start)\n\n## Implementation\nConsider a new table like `architecture_findings` or `benchmark_insights` to store:\n- Finding description\n- Supporting data (PPL, time comparisons)\n- Date discovered\n- Related architectures/components","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T15:19:41.103665-08:00","updated_at":"2025-12-16T15:22:52.087382-08:00","closed_at":"2025-12-16T15:22:52.087382-08:00","close_reason":"Added 4 key benchmark findings to findings table with tags and delta_vs_baseline"}
{"id":"arcfusion-v3i","title":"Benchmark integration for architecture scoring","description":"Add support for storing and querying benchmark results (perplexity, accuracy, speed) to score dreamed architectures against real performance data","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T12:15:47.542041-08:00","updated_at":"2025-12-11T15:18:29.598876-08:00","closed_at":"2025-12-11T15:18:29.598876-08:00","close_reason":"Complete: CLI commands for add/list/leaderboard/show/compare. DB layer was already done."}
{"id":"arcfusion-vho","title":"Apples-to-apples layer scaling: DeepGQA10 vs MHA Transformer","description":"Compare S-tier DeepGQA10 (245.6 PPL @ 81s) vs vanilla MHA Transformer at increasing layer counts.\n\nGoal: Determine if GQA's efficiency advantage holds or grows as we scale depth.\n\nExperiments:\n- MHA-6: 6 MHA layers (current baseline ~275 PPL @ 197s)\n- MHA-10: 10 MHA layers \n- MHA-14: 14 MHA layers\n- DeepGQA-10: 10 GQA layers (current S-tier @ 245.6 PPL)\n- DeepGQA-14: 14 GQA layers\n- DeepGQA-18: 18 GQA layers\n\nKey metrics: PPL, training time, PPL/time ratio\n\nHypothesis: GQA's efficiency advantage should grow with depth since KV-cache savings compound.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T16:12:03.377677-08:00","updated_at":"2025-12-15T16:43:33.014988-08:00","closed_at":"2025-12-15T16:43:33.014988-08:00","close_reason":"Layer scaling experiment complete. Key findings: (1) GQA gives ~1% better quality at same depth, (2) Speed varies by depth - GQA faster at 12 layers, MHA faster at 10/14, (3) DeepGQA18 achieves best quality (231.9 PPL), (4) MHA14 best S-tier (239 PPL @ 87s). 29 recipe cards saved for reproducibility."}
{"id":"arcfusion-w2h","title":"Improve search space coverage: increase dreaming diversity","description":"Current dreaming may not be exploring the full space of possible ML engines from our component DB. Consider:\n- Increasing temperature in dream strategies\n- More creative/exploratory dreaming approaches\n- Analyze which components are underutilized\n- Try different strategy mixes (more random, crossover, mutation)\n- Ensure we're not stuck in local optima around Mamba hybrids","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-16T13:29:57.070371-08:00","updated_at":"2025-12-16T13:35:37.337625-08:00","closed_at":"2025-12-16T13:35:37.337625-08:00","close_reason":"Implemented: (1) Added diverse and exploratory strategies, (2) Expanded categories from 7 to 11 (added moe, alternative, activation, regularization), (3) Increased temperature from 0.2-0.8 to 0.4-1.0, (4) Increased top-k from 3 to 5, (5) Dreaming now covers 82% of components vs 12% before."}
{"id":"arcfusion-wbi","title":"Component DB granularity for distinct, trainable recipes","description":"Ensure component database is fine-grained enough that:\n1. Composer can create recipes that are trainable by ML Agent\n2. Recipes stay faithful to the original idea\n3. Resulting models are DISTINCT - don't all blend together\n\nMay require:\n- More specific component variants (not just 'Attention' but 'MultiHeadAttention', 'GroupedQueryAttention', etc.)\n- Clearer interface specifications\n- Assembly instruction patterns that preserve architectural intent\n- Component compatibility metadata","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-12T12:15:27.388153-08:00","updated_at":"2025-12-12T13:21:23.836477-08:00","closed_at":"2025-12-12T13:21:23.836477-08:00","close_reason":"Added MQA/SlidingWindow/Linear attention variants, temperature sampling for greedy, validity constraints for random walk, fixed categorization"}
{"id":"arcfusion-wcm","title":"LLM-powered paper analysis with Claude API","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:19.280082-08:00","updated_at":"2025-12-11T12:15:33.171477-08:00","closed_at":"2025-12-11T12:15:33.171477-08:00","close_reason":"Implemented: PaperAnalyzer extracts components with interfaces, hyperparameters, complexity, code sketches"}
{"id":"arcfusion-xol","title":"Scale up: ingest more ML papers into pipeline","description":"## Goal\n\nExpand the knowledge base by ingesting more ML architecture papers into the pipeline.\n\n## Why Now\n\nAs we build out the auto-validation pipeline, we need:\n- More components to dream with\n- More proven architectures to learn from\n- Better relationship data between components\n\n## Papers to Prioritize\n\n### Foundational (pre-2020)\n- ResNet (residual connections)\n- ELMo (contextualized embeddings)\n- GPT-1 (decoder-only transformer)\n- XLNet (permutation language modeling)\n- T5 (encoder-decoder, text-to-text)\n\n### Modern (2020-2024)\n- GPT-3/4 architecture details\n- PaLM (pathways, scaling)\n- Chinchilla (compute-optimal scaling)\n- Mistral/Mixtral (MoE, sliding window)\n- Llama 2/3 (grouped query attention, improvements)\n- Gemma (efficient small models)\n- Phi-1/2/3 (data quality focus)\n- RWKV variants\n- Mamba 2\n\n### Efficiency/Training\n- LoRA (low-rank adaptation)\n- QLoRA (quantized fine-tuning)\n- Flash Attention 2/3\n- Ring Attention\n- Mixture of Experts papers\n\n## Approach\n\n1. Use ArxivFetcher to batch fetch papers\n2. Run through PaperAnalyzer for deep extraction\n3. Deduplicate components\n4. Build relationship graph\n\n## Success Criteria\n\n- [ ] 50+ papers ingested\n- [ ] 100+ unique components\n- [ ] Rich relationship graph for dreaming","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T15:13:19.150207-08:00","updated_at":"2025-12-12T14:17:31.363926-08:00","closed_at":"2025-12-12T14:17:31.363926-08:00","close_reason":"Expanded DB: 72 components (+9), 17 engines (+6). Added LoRA, MoE, ALiBi, CrossAttention, T5, Mixtral, PaLM, BLOOM, Falcon, GPT-NeoX."}
{"id":"arcfusion-xw9","title":"Include training time in results rankings","description":"## Problem\nCurrent rankings only show perplexity, but training time matters for practical use. Mamba takes 706s vs MHA's 57s - that's important context.\n\n## Current Results (missing time context)\n```\nModel             Perplexity    vs Baseline\nMamba             226.14        -19.9%\nHybrid            242.90        -13.9%\nMHA               282.26        baseline\n```\n\n## Proposed Results (with time)\n```\nModel             Perplexity    vs Baseline    Time      Speed\nMamba             226.14        -19.9%         706s      0.08x\nHybrid            242.90        -13.9%         565s      0.10x\nMHA               282.26        baseline       57s       1.0x\nGQA               294.03        +4.2%          61s       0.93x\nMQA               294.05        +4.2%          54s       1.06x\n```\n\n## Implementation\n1. Already tracking `time_seconds` in results\n2. Add to summary table formatting\n3. Calculate speed relative to baseline\n4. Store in fair_comparison_results.json\n\n## Quality/Speed Tradeoff Metric\nCould add a combined metric:\n- `efficiency = perplexity_improvement / time_cost`\n- Shows \"perplexity gain per second spent\"\n\n## Acceptance Criteria\n- [ ] Time shown in results table\n- [ ] Speed relative to baseline calculated\n- [ ] Results JSON includes time data\n- [ ] Optional: efficiency metric","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T08:02:30.631525-08:00","updated_at":"2025-12-14T08:16:36.76025-08:00","closed_at":"2025-12-14T08:16:36.76025-08:00","close_reason":"Added Time column to results summary table. Training time now displayed alongside perplexity and baseline comparison."}
{"id":"arcfusion-xwu","title":"Ensure all dreamed Mamba models use parallel scan (3-4x speedup)","description":"The parallel_scan implementation for Mamba SSM provides a 3-4x training speedup (MambaFast vs Mamba). Currently dreamed Mamba models in dream_and_train.py use parallel scan, but we should verify this is consistently applied and consider making it the default for all Mamba code generation. Check: generate_mamba_code() in dream_and_train.py uses parallel_scan. Ensure model_templates.py Mamba functions also use it by default.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-16T07:01:44.126881-08:00","updated_at":"2025-12-16T07:06:40.232528-08:00","closed_at":"2025-12-16T07:06:40.232528-08:00","close_reason":"Verified: all Mamba code paths use parallel_scan. model_templates.py defaults to use_parallel_scan=True, dream_and_train.py has it hardcoded, and COMPONENT_PATTERNS all specify True."}
{"id":"arcfusion-xxm","title":"Batch training for dreamed architectures","description":"Train dreamed-up architecture ideas in batches:\n- Queue up multiple dream compositions\n- Run overnight or when user is AFK\n- Collect results for review later\n- Maximize GPU utilization during off-hours","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-14T16:19:15.836125-08:00","updated_at":"2025-12-14T16:32:31.174725-08:00","closed_at":"2025-12-14T16:32:31.174725-08:00","close_reason":"Added scripts/batch_train.py for overnight batch training"}
{"id":"arcfusion-y8w","title":"Run trained models through benchmarks and record results in DB","description":"After training, run models through standard benchmarks. Ensure benchmark difficulty is calibrated so models don't saturate near 100% (cannot distinguish between models). Store benchmark results in arcfusion.db for tracking and comparison.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:28.919697-08:00","updated_at":"2025-12-13T17:33:17.771115-08:00","closed_at":"2025-12-13T17:33:17.771115-08:00","close_reason":"Training results now saved to training_runs table with full config, hardware, and baseline comparison data."}
{"id":"arcfusion-ye0","title":"Combined perplexity + training time score","description":"Training time is almost as important as perplexity. Create a composite score:\n- Combine perplexity improvement with training efficiency\n- Rank all results by this combined metric\n- Help identify architectures that are both good AND fast\n- Consider: score = perplexity_improvement / log(training_time) or similar","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-14T16:19:19.884443-08:00","updated_at":"2025-12-14T16:24:31.268554-08:00","closed_at":"2025-12-14T16:24:31.268554-08:00","close_reason":"Implemented combined score: 0.7*ppl + 0.3*time efficiency"}
{"id":"arcfusion-ys6","title":"Review GPT and Gemini feedback on project priorities","description":"External feedback collected in gpt1.md and gemini1.md. Key recommendations:\n\n**Both agree on:**\n- arcfusion-8ys (Mamba speedups) is high priority\n- arcfusion-ekl (compaction fix) needed for stability\n- arcfusion-2si (.env support) for DX\n\n**GPT recommends (stability-first):**\n1. Fix .env + compaction issues first\n2. Add daily loop command (leaderboard, insights, next experiments)\n3. Then Mamba speedups\n\n**Gemini recommends (performance-first):**\n1. Mamba speedups first (biggest bottleneck)\n2. Exploit MHA/GQA crossover at 18-24L depth\n3. Test Mamba at extreme depths (24L+)\n\n**Action:** Review and prioritize remaining open issues based on this feedback.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-15T19:48:24.26266-08:00","updated_at":"2025-12-15T19:49:56.593556-08:00","closed_at":"2025-12-15T19:49:56.593556-08:00","close_reason":"Reviewed and reprioritized. P1: arcfusion-2si (.env) + arcfusion-8ys (Mamba). Compaction (ekl) mitigated by background shells. Multi-agent (a8o) deferred."}
{"id":"arcfusion-z2j","title":"Wire up surrogate model to review trained results DB table to update its PPL and time predictions","description":"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-16T08:29:37.048193-08:00","updated_at":"2025-12-16T08:33:17.540761-08:00","closed_at":"2025-12-16T08:33:17.540761-08:00","close_reason":"Implemented update_untrained_candidate_predictions() in surrogate_model.py and update_dream_candidate_predictions() in db.py. Now when surrogate model retrains, it automatically updates predictions for all untrained dream candidates."}
{"id":"arcfusion-zdr","title":"Brainstorm: Patch Transformer weaknesses with dreamed architectures","description":"## Context\nTransformer is the dominant architecture for AI but has known weaknesses. Our pipeline can dream new architectures by combining components.\n\n## Complete Benchmark Results (Dec 2024)\n\n| Architecture | PPL | Quality | Time (s) | Notes |\n|-------------|-----|---------|----------|-------|\n| Mamba (pure SSM) | 226.1 | **1.248x** | 706 | Best quality |\n| **Mamba5to1 (5:1)** | **228.5** | **1.234x** | 931 | NEW - Near pure Mamba! |\n| Mamba4to1 (4:1) | 238.3 | 1.184x | 738 | 4 Mamba + 1 MHA |\n| Hybrid (2:1 Jamba) | 242.9 | 1.162x | 566 | Interleaved attention |\n| LinearAttn (2:1) | 245.3 | 1.151x | 803 | O(n) linear attention |\n| MambaHeavy (3:1) | 247.0 | 1.142x | 564 | Attn at end only |\n| **GQAMamba (1:1)** | **252.7** | **1.116x** | **377** | NEW - Speed winner! |\n| AttnFirst (3:1) | 253.7 | 1.113x | 540 | Attn at start only |\n| **Sandwich** | **255.9** | **1.102x** | **366** | NEW - Fastest hybrid! |\n| MHA (baseline) | ~282 | 1.000x | ~200 | Standard Transformer |\n| GQA | 294.0 | 0.959x | 54 | Fastest overall |\n| MQA | 294.1 | 0.959x | 59 | KV-efficient |\n\n## Key Discoveries\n\n### 1. Speed vs Quality Sweet Spots\n- **Best quality**: Pure Mamba (226.1) or Mamba5to1 (228.5)\n- **Best speed+quality**: GQAMamba (252.7 @ 377s) or Sandwich (255.9 @ 366s)\n- **Pure speed**: GQA/MQA (294 @ ~55s)\n\n### 2. GQA-Mamba Solves the Speed Problem!\n| Model | Speed vs MHA | Quality vs MHA |\n|-------|--------------|----------------|\n| Pure Mamba | 3.5x slower | +24.8% better |\n| **GQAMamba** | **1.9x slower** | **+10.4% better** |\n| Sandwich | 1.8x slower | +9.2% better |\n\n### 3. More Mamba = Better Quality (Confirmed)\n- Mamba (pure): 226.1\n- Mamba5to1: 228.5 (only 2.4 worse!)\n- Mamba4to1: 238.3\n- MambaHeavy 3:1: 247.0\n\n### 4. Attention Position Still Matters\n- Sandwich [MHA, M, M, MHA]: 255.9 - OK\n- MambaHeavy [M, M, M, MHA]: 247.0 - Better\n- AttnFirst [MHA, M, M, M]: 253.7 - Worse\n\n## Architecture Patterns Tested\n\n| Pattern | Architecture | PPL | Time |\n|---------|-------------|-----|------|\n| Pure SSM | [M, M, M, M] | 226.1 | 706s |\n| 5:1 Hybrid | [M, M, M, M, M, MHA] | 228.5 | 931s |\n| 4:1 Hybrid | [M, M, M, M, MHA] | 238.3 | 738s |\n| 3:1 Attn-End | [M, M, M, MHA] | 247.0 | 564s |\n| 2:1 Interleaved | [M, M, MHA, M] | 242.9 | 566s |\n| GQA-Mamba 1:1 | [GQA, M, GQA, M] | 252.7 | 377s |\n| Sandwich | [MHA, M, M, MHA] | 255.9 | 366s |\n\n## Completed\n- [x] Benchmark pure MHA, GQA, MQA, Mamba\n- [x] Implement and test Hybrid (2:1 Jamba-style)\n- [x] Implement and test MambaHeavy (3:1 attention-at-end)\n- [x] Implement and test AttnFirst (3:1 attention-at-start)\n- [x] Implement and test Mamba4to1 (4:1 ratio)\n- [x] Implement and test LinearAttn (O(n) attention)\n- [x] Implement and test Mamba5to1 (5:1 ratio)\n- [x] Implement and test GQAMamba (speed focus)\n- [x] Implement and test Sandwich (bookend pattern)\n\n## Recommendations\n1. **Production use**: GQAMamba - best speed/quality balance\n2. **Research/quality**: Pure Mamba or Mamba5to1\n3. **Resource constrained**: Sandwich - fastest with quality gains","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2025-12-14T07:37:12.60744-08:00","updated_at":"2025-12-15T13:32:55.353158-08:00"}
{"id":"arcfusion-zzp","title":"Cloud training integration: Groq/Modal/Lambda for auto-pipeline","description":"## Problem\n\nLocal compute is too limited to run meaningful training of auto-pipeline generated architectures. Need to offload training to cloud providers.\n\n## Potential Providers\n\n### Modal\n- Serverless GPU compute\n- Python-native, good DX\n- Pay-per-second billing\n\n### Lambda Labs\n- GPU cloud instances\n- Good for longer training runs\n\n### RunPod\n- Cheap GPU rentals\n- Spot instances available\n\n### Groq\n- Fast inference (maybe not for training?)\n\n## Implementation Ideas\n\n1. **Remote execution wrapper**: Serialize model code + config, send to cloud, run training, return metrics\n\n2. **Integration with ValidationPipeline**: `--device cloud:modal` flag\n\n3. **Cost tracking**: Store compute costs in benchmark_results","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T15:51:44.874322-08:00","updated_at":"2025-12-12T13:50:21.582784-08:00","closed_at":"2025-12-12T13:50:21.582784-08:00","close_reason":"Implemented Modal cloud training integration with CloudTrainer, CloudConfig, CloudResult. Added --cloud CLI flag and 13 new tests."}
