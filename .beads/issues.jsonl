{"id":"arcfusion-003","title":"Interface-aware dream composition","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:21.711003-08:00","updated_at":"2025-12-11T12:15:36.11899-08:00","closed_at":"2025-12-11T12:15:36.11899-08:00","close_reason":"Implemented: Shape normalization, category ordering, 4 strategies (greedy, random, crossover, mutate)"}
{"id":"arcfusion-040","title":"Auto-validation pipeline: build, train, and benchmark dreamed architectures","description":"## Vision\n\nCreate an automated pipeline that takes dreamed architectures and actually validates them:\n\n```\nDream → CodeGen → Build Model → Train (small) → Benchmark → Score → Feedback\n```\n\n## Why This Matters\n\nCurrently we estimate scores with formulas. To truly validate the dream engine, we need to:\n1. Build real models from dreamed component lists\n2. Train them (on small datasets to minimize compute)\n3. Run actual benchmarks\n4. Feed scores back to improve future dreams\n\n## Pipeline Components\n\n### 1. Model Builder\n- Take CodeGenerator output and make it runnable\n- Handle edge cases (missing dims, incompatible interfaces)\n- Parameterize model size (tiny/small/medium for testing)\n\n### 2. Training Harness\n- Small dataset options (WikiText-2, TinyStories, MNIST for sanity)\n- Quick training runs (few epochs, small batch)\n- Configurable compute budget (max time, max steps)\n\n### 3. Benchmark Suite\n- Lightweight benchmarks first (perplexity, simple classification)\n- Standardized evaluation protocol\n- Results stored in benchmark_results table\n\n### 4. Feedback Loop\n- Update component usefulness_score based on real performance\n- Update configuration scores\n- Track which component combinations actually work\n\n## Compute Considerations\n\n- Start TINY: 1M param models, 1000 training steps\n- Use CPU-friendly sizes initially\n- Could integrate with cloud compute later (Lambda, Modal, etc.)\n\n## Dependencies\n\n- Solid CodeGenerator (we have this)\n- Benchmark integration (arcfusion-v3i) for storing results\n\n## Success Criteria\n\n- [ ] Can take a dream output and produce a trainable model\n- [ ] Can train on small dataset in \u003c5 minutes\n- [ ] Can run benchmark and store results\n- [ ] Feedback improves future dream scores\n\n## Notes\n\nThis is the \"close the loop\" feature that makes ArcFusion genuinely useful for architecture search.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T14:58:41.413332-08:00","updated_at":"2025-12-11T15:47:04.338619-08:00","closed_at":"2025-12-11T15:47:04.338619-08:00","close_reason":"Implemented: ModelBuilder, TrainingHarness, BenchmarkRunner, ValidationPipeline. CLI validate command added. Requires PyTorch (optional).","dependencies":[{"issue_id":"arcfusion-040","depends_on_id":"arcfusion-v3i","type":"blocks","created_at":"2025-12-11T14:58:47.289242-08:00","created_by":"daemon"}]}
{"id":"arcfusion-30o","title":"Auto-validate generated code before saving","description":"GeneratedCode.save() writes code without calling validate_syntax(). Add automatic validation and sanitization of component names in docstrings to prevent invalid Python from being saved.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-11T13:02:23.288818-08:00","updated_at":"2025-12-11T13:11:37.162829-08:00","closed_at":"2025-12-11T13:11:37.162829-08:00","close_reason":"save() now validates syntax by default, raises ValueError on invalid code"}
{"id":"arcfusion-4lc","title":"Research: Minimum training scale to differentiate architecture quality","description":"## Research Question\n\nHow much training is needed before it becomes apparent whether an architecture is good or bad?\n\n## Why This Matters\n\nIf we can detect architecture quality with minimal training:\n- Faster iteration on dream experiments\n- Lower compute costs\n- More architectures explored per dollar\n\nIf we need substantial training:\n- Need cloud compute integration\n- Batch validation runs\n- Different validation strategy\n\n## Experiment Design\n\n1. **Baseline**: Train known-good architecture (Transformer) at various scales\n   - 100 steps, 1K steps, 10K steps, 100K steps\n   - Track loss curves and final perplexity\n\n2. **Bad architecture**: Train known-bad architecture (random component soup)\n   - Same scale progression\n   - Compare curves to baseline\n\n3. **Analysis**: At what point do curves diverge?\n   - Early divergence = can validate cheaply\n   - Late divergence = need more compute\n\n## Metrics to Track\n\n- Loss at each checkpoint\n- Perplexity trajectory\n- Parameter efficiency (loss per param)\n- Training stability (gradient variance)\n\n## Literature Review\n\n- Scaling laws papers (Chinchilla, etc.)\n- Neural architecture search papers\n- Early stopping criteria research\n\n## Dependencies\n\n- Need PyTorch installed\n- Need cloud compute for larger runs (arcfusion-zzp)","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-11T15:52:04.874825-08:00","updated_at":"2025-12-11T15:52:04.874825-08:00","dependencies":[{"issue_id":"arcfusion-4lc","depends_on_id":"arcfusion-zzp","type":"blocks","created_at":"2025-12-11T15:52:10.640381-08:00","created_by":"daemon"}]}
{"id":"arcfusion-5ux","title":"Validate relationships in analyzer before adding","description":"In analyzer.py lines 334-348, component relationships are added without verifying both components exist. If LLM returns relationship for skipped component (low confidence), it fails silently or creates orphaned refs.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-11T13:02:16.95983-08:00","updated_at":"2025-12-11T13:11:37.107019-08:00","closed_at":"2025-12-11T13:11:37.107019-08:00","close_reason":"Added validation with warnings for skipped relationships"}
{"id":"arcfusion-5wj","title":"Web UI for architecture exploration","description":"Build a web interface to visualize components, relationships, and dream new architectures interactively","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-11T12:15:52.710451-08:00","updated_at":"2025-12-11T12:15:52.710451-08:00"}
{"id":"arcfusion-9c9","title":"DB table: recipes with adjustments tracking","description":"New DB table to store dreamed recipes:\n- recipe_id (unique)\n- component_ids (ordered list)\n- assembly_instructions (JSON: connections, residuals, shapes)\n- source_strategy (greedy, crossover, mutate, etc.)\n- created_at timestamp\n\nPlus adjustments table:\n- adjustment_id\n- recipe_id (FK)\n- original_value\n- adjusted_value  \n- reason (why modification was needed)\n- adjusted_at timestamp\n\nThis enables: recipe recreation, composer learning from failures, training run reproducibility.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:15:07.58677-08:00","updated_at":"2025-12-12T12:24:43.228152-08:00","closed_at":"2025-12-12T12:24:43.228152-08:00","close_reason":"DB tables implemented: recipes (recipe_id, name, component_ids, assembly, strategy, estimated_score, parent_engine_ids, notes) and recipe_adjustments (adjustment_id, recipe_id, adjustment_type, original_value, adjusted_value, reason, component_id). Full CRUD methods and stats tracking."}
{"id":"arcfusion-a1s","title":"ML Agent: faithful recipe execution with modification tracking","description":"ML Agent receives Recipe from Composer and:\n- Makes best effort to train the model\n- Stays faithful to the recipe provided\n- Records ANY modifications needed to enable training\n- Modifications inform Composer for future dreams\n- Enables recreation of training runs\n\nKey principle: ML Agent has leeway to make necessary adjustments, but ALL adjustments must be recorded.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:15:00.085093-08:00","updated_at":"2025-12-12T12:58:49.512701-08:00","closed_at":"2025-12-12T12:58:49.512701-08:00","close_reason":"ML Agent implemented with execute_recipe(), adjustment tracking, ExecutionResult dataclass. Records all modifications (component_skip, build_fix, train_fix). 16 new tests, 215 total passing.","dependencies":[{"issue_id":"arcfusion-a1s","depends_on_id":"arcfusion-p9w","type":"blocks","created_at":"2025-12-12T12:15:33.445742-08:00","created_by":"daemon"},{"issue_id":"arcfusion-a1s","depends_on_id":"arcfusion-9c9","type":"blocks","created_at":"2025-12-12T12:15:33.493527-08:00","created_by":"daemon"}]}
{"id":"arcfusion-avc","title":"Validation: Can we discover Transformer without the attention paper?","description":"## Research Question\n\nIf the \"Attention Is All You Need\" paper had never been published, could ArcFusion's dream engine independently discover the Transformer architecture by composing components from other papers?\n\n## Why This Matters\n\nThis is a key validation of the entire ArcFusion approach:\n- If YES → The system can genuinely discover novel architectures\n- If NO → We're just recombining known patterns, not innovating\n\n## Experiment Design\n\n### Setup\n1. Remove Transformer and all attention-related components from the database\n2. Seed only with pre-2017 components (RNNs, CNNs, LSTMs, embeddings, etc.)\n3. Add components from papers that influenced attention (memory networks, seq2seq, etc.)\n\n### Test\n1. Run dream engine with various strategies (greedy, random, crossover, mutate)\n2. Generate many candidate architectures\n3. Analyze: Do any resemble Transformer's key innovations?\n   - Multi-head attention pattern\n   - Encoder-decoder with cross-attention\n   - Positional encodings + self-attention (no recurrence)\n\n### Success Criteria\n- **Strong success**: Dream engine produces attention-like mechanism\n- **Partial success**: Produces components that could lead to attention with minor tweaks\n- **Failure**: Only produces RNN/CNN variants\n\n## Dependencies\n- Robust component extraction from pre-2017 papers\n- Good relationship scoring between components\n- Multiple dream strategies working well\n\n## Notes\n- This may require ingesting more historical papers (2014-2016 era)\n- May need to tune dream engine parameters\n- Could be a good benchmark for measuring system improvement over time","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T14:19:39.83596-08:00","updated_at":"2025-12-11T14:43:31.692448-08:00","closed_at":"2025-12-11T14:43:31.692448-08:00","close_reason":"Experiment complete. PARTIAL SUCCESS: Dream engine found architecture with attention+parallelism+position (5/5 score). Key finding: Can get 80% of the way but misses self-attention, multi-head, and dot-product innovations that required human insight."}
{"id":"arcfusion-ayd","title":"Track component configurations (sub-architectures)","description":"## Problem\n\nCurrently we track full engines (e.g., Transformer with 15 components) but not successful sub-configurations. For example, if 7 specific components from Transformer in a certain order consistently produce good results, we have no way to capture and reuse that pattern.\n\n## Proposed Solution\n\nTrack \"configurations\" - ordered subsets of components that work well together:\n\n### New DB Table: `component_configurations`\n```sql\nCREATE TABLE component_configurations (\n    config_id TEXT PRIMARY KEY,\n    name TEXT,                          -- e.g., \"Transformer Core Block\"\n    description TEXT,\n    component_ids TEXT,                 -- JSON array of component IDs in order\n    source_engine_id TEXT,              -- Engine this was derived from (optional)\n    configuration_score REAL,           -- How well this config performs\n    usage_count INTEGER DEFAULT 0,      -- How often used in dreams\n    validated BOOLEAN DEFAULT 0,        -- Has been tested/validated\n    created_at TIMESTAMP\n);\n```\n\n### Key Features\n1. **Configuration extraction**: Analyze engines to find common successful patterns\n2. **Configuration scoring**: Track which configs produce good dream results\n3. **Dream integration**: Use proven configs as building blocks for new architectures\n4. **Configuration discovery**: Find configs that appear across multiple engines\n\n### Use Cases\n- \"Attention + LayerNorm + FFN\" block appears in many architectures → track as config\n- User validates a dreamed architecture → extract and save its sub-configs\n- Dream engine prefers using proven configs when available\n\n## Acceptance Criteria\n- [ ] Add `component_configurations` table to schema\n- [ ] Add CRUD operations for configurations\n- [ ] Add method to extract configs from existing engines\n- [ ] Integrate with composer to prefer known-good configs\n- [ ] CLI commands: `arcfusion config list/show/extract`","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T14:17:56.139247-08:00","updated_at":"2025-12-11T14:27:49.145668-08:00","closed_at":"2025-12-11T14:27:49.145668-08:00","close_reason":"Feature complete: DB schema, CRUD ops, extraction, CLI commands, and dream integration all implemented. 167 tests passing."}
{"id":"arcfusion-ctf","title":"PyTorch code generation from dreamed architectures","description":"Generate runnable PyTorch code from dreamed component combinations, using code_sketch fields and interface compatibility","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T12:15:49.586369-08:00","updated_at":"2025-12-11T12:37:51.398835-08:00","closed_at":"2025-12-11T12:37:51.398835-08:00","close_reason":"Implemented CodeGenerator with generate CLI command. Generates valid PyTorch nn.Module code from dreamed architectures using greedy, random, crossover, and mutate strategies."}
{"id":"arcfusion-d7u","title":"Core database schema with components, engines, relationships","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:17.526253-08:00","updated_at":"2025-12-11T12:15:31.395323-08:00","closed_at":"2025-12-11T12:15:31.395323-08:00","close_reason":"Implemented: 8 tables (components, engines, relationships, compatibility, papers, benchmarks, dreams, validations)"}
{"id":"arcfusion-e2f","title":"Validation: Implement Transformer using only DB components via auto-pipeline","description":"## Goal\n\nTest the auto-validation pipeline by implementing the Transformer architecture using ONLY components extracted into our database - no external code.\n\n## Why This Test\n\n- We KNOW Transformer works (it's a winning recipe)\n- If our pipeline can reconstruct it from DB components and train successfully, the pipeline is validated\n- Fair comparison: only use what we've extracted, not code from the internet\n\n## Test Protocol\n\n1. **Component Selection**\n   - Query DB for Transformer-relevant components\n   - Use: MultiHeadAttention, FeedForward, LayerNorm, Embedding, etc.\n   - Only components with code sketches from our extraction\n\n2. **Model Assembly**\n   - Use CodeGenerator to assemble from DB components\n   - Standard Transformer config: 6 layers, 512 dim, 8 heads\n   - Keep it small for fast iteration\n\n3. **Training**\n   - Small dataset (WikiText-2 or similar)\n   - Short training run (sanity check, not SOTA)\n   - Track loss curves\n\n4. **Benchmark**\n   - Measure perplexity\n   - Compare to known Transformer baselines\n   - Store in benchmark_results\n\n## Success Criteria\n\n- [ ] Model assembles from DB components without manual code\n- [ ] Model trains without errors\n- [ ] Achieves reasonable perplexity (not random)\n- [ ] Results stored in DB\n- [ ] Pipeline proven end-to-end\n\n## Why \"Fair Comparison\" Matters\n\nIf we copy Transformer code from HuggingFace, we're not testing our system. The whole point is: can our extracted component representations actually work?\n\n## Dependencies\n\n- Auto-validation pipeline (arcfusion-040)\n- Sufficient Transformer components in DB (we have these from seeds)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T15:13:20.534039-08:00","updated_at":"2025-12-11T17:47:48.47835-08:00","closed_at":"2025-12-11T17:47:48.47835-08:00","close_reason":"Validation pipeline works end-to-end: Transformer builds from DB components (5 components, 1M params), trains successfully (50 steps, loss ~6.9), and benchmarks run correctly. Fixed vocab_size propagation in codegen.py and validator.py.","dependencies":[{"issue_id":"arcfusion-e2f","depends_on_id":"arcfusion-040","type":"blocks","created_at":"2025-12-11T15:13:26.60807-08:00","created_by":"daemon"}]}
{"id":"arcfusion-f73","title":"Add test coverage for untested modules","description":"composer.py, dedup.py, decomposer.py, and seeds.py have no dedicated tests. Add tests for:\n- All 4 dream strategies (greedy, random, mutate, crossover)\n- Duplicate detection and merging\n- Paper decomposition\n- Database seeding","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T13:02:07.568054-08:00","updated_at":"2025-12-11T13:07:14.417626-08:00","closed_at":"2025-12-11T13:07:14.417626-08:00","close_reason":"Added 102 new tests (157 total) covering composer, dedup, decomposer, and seeds modules"}
{"id":"arcfusion-frl","title":"Fuzzy deduplication with variant detection","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:20.748145-08:00","updated_at":"2025-12-11T12:15:34.510907-08:00","closed_at":"2025-12-11T12:15:34.510907-08:00","close_reason":"Implemented: ComponentDeduplicator with normalized names, semantic signatures, architecture variant detection"}
{"id":"arcfusion-l3a","title":"Expand paper knowledge base","description":"Analyze more seminal papers: GPT-3, PaLM, Gemini, Claude architecture (if published), Mixtral, Phi, etc.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-11T12:15:50.758608-08:00","updated_at":"2025-12-11T12:15:50.758608-08:00"}
{"id":"arcfusion-p9w","title":"Recipe dataclass: ordered components + assembly instructions","description":"Composer outputs a Recipe dataclass containing:\n- Ordered list of component IDs\n- Assembly instructions (how components connect, residuals, etc.)\n- Metadata (strategy used, score, timestamp)\n\nThis is the handoff format between Composer and ML Agent.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:14:52.735707-08:00","updated_at":"2025-12-12T12:24:41.769431-08:00","closed_at":"2025-12-12T12:24:41.769431-08:00","close_reason":"Recipe dataclass implemented with: name, component_ids, assembly instructions (connections, residuals, shapes, categories, notes), strategy, estimated_score, parent_engine_ids. Composer.create_recipe() generates Recipes from any dream strategy."}
{"id":"arcfusion-v3i","title":"Benchmark integration for architecture scoring","description":"Add support for storing and querying benchmark results (perplexity, accuracy, speed) to score dreamed architectures against real performance data","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T12:15:47.542041-08:00","updated_at":"2025-12-11T15:18:29.598876-08:00","closed_at":"2025-12-11T15:18:29.598876-08:00","close_reason":"Complete: CLI commands for add/list/leaderboard/show/compare. DB layer was already done."}
{"id":"arcfusion-wbi","title":"Component DB granularity for distinct, trainable recipes","description":"Ensure component database is fine-grained enough that:\n1. Composer can create recipes that are trainable by ML Agent\n2. Recipes stay faithful to the original idea\n3. Resulting models are DISTINCT - don't all blend together\n\nMay require:\n- More specific component variants (not just 'Attention' but 'MultiHeadAttention', 'GroupedQueryAttention', etc.)\n- Clearer interface specifications\n- Assembly instruction patterns that preserve architectural intent\n- Component compatibility metadata","status":"in_progress","priority":2,"issue_type":"feature","created_at":"2025-12-12T12:15:27.388153-08:00","updated_at":"2025-12-12T13:02:30.511765-08:00"}
{"id":"arcfusion-wcm","title":"LLM-powered paper analysis with Claude API","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:19.280082-08:00","updated_at":"2025-12-11T12:15:33.171477-08:00","closed_at":"2025-12-11T12:15:33.171477-08:00","close_reason":"Implemented: PaperAnalyzer extracts components with interfaces, hyperparameters, complexity, code sketches"}
{"id":"arcfusion-xol","title":"Scale up: ingest more ML papers into pipeline","description":"## Goal\n\nExpand the knowledge base by ingesting more ML architecture papers into the pipeline.\n\n## Why Now\n\nAs we build out the auto-validation pipeline, we need:\n- More components to dream with\n- More proven architectures to learn from\n- Better relationship data between components\n\n## Papers to Prioritize\n\n### Foundational (pre-2020)\n- ResNet (residual connections)\n- ELMo (contextualized embeddings)\n- GPT-1 (decoder-only transformer)\n- XLNet (permutation language modeling)\n- T5 (encoder-decoder, text-to-text)\n\n### Modern (2020-2024)\n- GPT-3/4 architecture details\n- PaLM (pathways, scaling)\n- Chinchilla (compute-optimal scaling)\n- Mistral/Mixtral (MoE, sliding window)\n- Llama 2/3 (grouped query attention, improvements)\n- Gemma (efficient small models)\n- Phi-1/2/3 (data quality focus)\n- RWKV variants\n- Mamba 2\n\n### Efficiency/Training\n- LoRA (low-rank adaptation)\n- QLoRA (quantized fine-tuning)\n- Flash Attention 2/3\n- Ring Attention\n- Mixture of Experts papers\n\n## Approach\n\n1. Use ArxivFetcher to batch fetch papers\n2. Run through PaperAnalyzer for deep extraction\n3. Deduplicate components\n4. Build relationship graph\n\n## Success Criteria\n\n- [ ] 50+ papers ingested\n- [ ] 100+ unique components\n- [ ] Rich relationship graph for dreaming","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-11T15:13:19.150207-08:00","updated_at":"2025-12-11T15:13:19.150207-08:00"}
{"id":"arcfusion-zzp","title":"Cloud training integration: Groq/Modal/Lambda for auto-pipeline","description":"## Problem\n\nLocal compute is too limited to run meaningful training of auto-pipeline generated architectures. Need to offload training to cloud providers.\n\n## Potential Providers\n\n### Modal\n- Serverless GPU compute\n- Python-native, good DX\n- Pay-per-second billing\n\n### Lambda Labs\n- GPU cloud instances\n- Good for longer training runs\n\n### RunPod\n- Cheap GPU rentals\n- Spot instances available\n\n### Groq\n- Fast inference (maybe not for training?)\n\n## Implementation Ideas\n\n1. **Remote execution wrapper**: Serialize model code + config, send to cloud, run training, return metrics\n\n2. **Integration with ValidationPipeline**: `--device cloud:modal` flag\n\n3. **Cost tracking**: Store compute costs in benchmark_results","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-11T15:51:44.874322-08:00","updated_at":"2025-12-11T15:51:53.029527-08:00"}
