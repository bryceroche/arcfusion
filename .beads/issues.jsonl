{"id":"arcfusion-003","title":"Interface-aware dream composition","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:21.711003-08:00","updated_at":"2025-12-11T12:15:36.11899-08:00","closed_at":"2025-12-11T12:15:36.11899-08:00","close_reason":"Implemented: Shape normalization, category ordering, 4 strategies (greedy, random, crossover, mutate)"}
{"id":"arcfusion-040","title":"Auto-validation pipeline: build, train, and benchmark dreamed architectures","description":"## Vision\n\nCreate an automated pipeline that takes dreamed architectures and actually validates them:\n\n```\nDream → CodeGen → Build Model → Train (small) → Benchmark → Score → Feedback\n```\n\n## Why This Matters\n\nCurrently we estimate scores with formulas. To truly validate the dream engine, we need to:\n1. Build real models from dreamed component lists\n2. Train them (on small datasets to minimize compute)\n3. Run actual benchmarks\n4. Feed scores back to improve future dreams\n\n## Pipeline Components\n\n### 1. Model Builder\n- Take CodeGenerator output and make it runnable\n- Handle edge cases (missing dims, incompatible interfaces)\n- Parameterize model size (tiny/small/medium for testing)\n\n### 2. Training Harness\n- Small dataset options (WikiText-2, TinyStories, MNIST for sanity)\n- Quick training runs (few epochs, small batch)\n- Configurable compute budget (max time, max steps)\n\n### 3. Benchmark Suite\n- Lightweight benchmarks first (perplexity, simple classification)\n- Standardized evaluation protocol\n- Results stored in benchmark_results table\n\n### 4. Feedback Loop\n- Update component usefulness_score based on real performance\n- Update configuration scores\n- Track which component combinations actually work\n\n## Compute Considerations\n\n- Start TINY: 1M param models, 1000 training steps\n- Use CPU-friendly sizes initially\n- Could integrate with cloud compute later (Lambda, Modal, etc.)\n\n## Dependencies\n\n- Solid CodeGenerator (we have this)\n- Benchmark integration (arcfusion-v3i) for storing results\n\n## Success Criteria\n\n- [ ] Can take a dream output and produce a trainable model\n- [ ] Can train on small dataset in \u003c5 minutes\n- [ ] Can run benchmark and store results\n- [ ] Feedback improves future dream scores\n\n## Notes\n\nThis is the \"close the loop\" feature that makes ArcFusion genuinely useful for architecture search.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T14:58:41.413332-08:00","updated_at":"2025-12-11T15:47:04.338619-08:00","closed_at":"2025-12-11T15:47:04.338619-08:00","close_reason":"Implemented: ModelBuilder, TrainingHarness, BenchmarkRunner, ValidationPipeline. CLI validate command added. Requires PyTorch (optional).","dependencies":[{"issue_id":"arcfusion-040","depends_on_id":"arcfusion-v3i","type":"blocks","created_at":"2025-12-11T14:58:47.289242-08:00","created_by":"daemon"}]}
{"id":"arcfusion-1bu","title":"Add findings/insights logging to DB for dream sessions","description":"## Context\nDuring architecture experimentation, we discover valuable insights that should inform future dreaming sessions. Currently these insights are only in issue descriptions or lost between sessions.\n\n## Goal\nCreate Python code to log key findings including text thoughts from training to the DB for future dreaming sessions.\n\n## Key Findings to Capture (Examples)\n\n### Attention Position Insights\n- **Interleaved attention (Jamba-style) = better quality**\n- **Attention-at-end = 3x faster but slightly worse quality**\n- Position of attention layers matters more than ratio\n\n### Architecture Learnings\n- Pure Mamba achieves best quality (PPL 226.1, 21.5% better than baseline)\n- GQA/MQA sacrifice quality for 3.6x speed improvement\n- Hybrid architectures trade off between pure approaches\n\n### General Insights\n- SSM (Mamba) captures sequential patterns better than attention for language modeling\n- Fewer KV heads (GQA/MQA) hurt perplexity but dramatically improve training speed\n- 3:1 ratio hybrids don't outperform 2:1 ratio - interleaving is key\n\n## Proposed Implementation\n\n### 1. New DB Table: `findings`\n```sql\nCREATE TABLE findings (\n    id TEXT PRIMARY KEY,\n    category TEXT,           -- 'attention', 'architecture', 'training', 'efficiency'\n    finding TEXT,            -- The insight text\n    evidence TEXT,           -- Supporting data (JSON: model results, comparisons)\n    confidence REAL,         -- 0.0-1.0 how confident we are\n    source_models TEXT,      -- Comma-separated model names that led to this\n    created_at TEXT,\n    tags TEXT                -- Comma-separated tags for retrieval\n);\n```\n\n### 2. New Methods in db.py\n```python\ndef add_finding(self, category: str, finding: str, evidence: dict, \n                confidence: float, source_models: list[str], tags: list[str]) -\u003e str\n\ndef get_findings(self, category: str = None, tags: list[str] = None, \n                 min_confidence: float = 0.0) -\u003e list[Finding]\n\ndef get_findings_for_dreaming(self) -\u003e dict:\n    '''Return structured insights to inform dream strategies'''\n```\n\n### 3. Integration with Dream Strategies\n- `results_aware_compose()` should query findings\n- New dream strategy: `insights_aware` that uses both results AND findings\n- Auto-log findings when significant results are detected\n\n## Acceptance Criteria\n- [ ] Add findings table to DB schema\n- [ ] Implement CRUD for findings\n- [ ] Add method to retrieve findings for dreaming\n- [ ] Log initial findings from current experiments\n- [ ] Update results_aware strategy to use findings","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T06:46:53.463276-08:00","updated_at":"2025-12-15T08:17:27.500159-08:00","closed_at":"2025-12-15T08:17:27.500159-08:00","close_reason":"Implemented as training_insights table in commit 65eee00. Includes: training_insights DB table, CRUD methods, get_insights_for_dreaming(), CLI command (arcfusion insights), and integration with composer.py dream strategies via _insight_guided_compose()"}
{"id":"arcfusion-1kl","title":"Save model recipes for reproducibility","description":"## Problem\nWe can train models but can't easily recreate them later. The model code is embedded as strings in MODELS dict but not linked to training runs.\n\n## Current State\n- Model code stored as strings in cloud_train_fair.py MODELS dict\n- Training runs reference model_name but not the actual code\n- No way to recover exact model definition from a training run\n\n## Requirements\n1. Save model code/recipe with each training run\n2. Store hyperparameters used (d_model, n_layers, etc.)\n3. Store random seed for reproducibility\n4. Optionally store model weights/checkpoints\n\n## Schema Changes Needed\n- Add model_code TEXT column to training_runs?\n- Or create separate recipes table with versioning?\n- Link training_runs to recipes via recipe_id\n\n## Acceptance Criteria\n- [ ] Can recreate any previously trained model\n- [ ] Recipe includes full model code\n- [ ] Hyperparameters are queryable\n- [ ] Seeds are tracked for reproducibility","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T07:37:14.459225-08:00","updated_at":"2025-12-14T16:43:05.924363-08:00","closed_at":"2025-12-14T16:43:05.924363-08:00","close_reason":"Implemented model_code and code_hash columns in training_runs table for full reproducibility"}
{"id":"arcfusion-2q6","title":"Cache WikiText-2 data on Modal to avoid timeouts","description":"## Problem\nMamba training failed with \"ReadTimeout\" loading WikiText-2 from HuggingFace. Network issues on Modal cause wasted GPU time.\n\n## Options\n\n### Option 1: Modal Volume (Persistent)\n- Store data in Modal Volume once\n- Load from volume on each run\n- Cost: ~$0.20/GB/month = ~$0.01/month for WikiText-2\n- Pro: Never re-download\n- Con: Volume setup complexity\n\n### Option 2: Pre-bake into Image\n- Download data during image build\n- Data cached in image layers\n- Cost: Included in image storage\n- Pro: Always available, no runtime download\n- Con: Rebuilds image when data changes\n\n### Option 3: Retry with backoff\n- Add retry logic to load_dataset()\n- Exponential backoff on timeout\n- Pro: Simple, no infrastructure changes\n- Con: Still wastes time on retries\n\n## Recommendation\n**Option 2 (Pre-bake into Image)** because:\n- WikiText-2 is small (~13MB)\n- Data won't change\n- Zero runtime download time\n- Most reliable\n\n## Implementation\n```python\nimage = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .pip_install(\"torch\u003e=2.0\", \"datasets\", \"tiktoken\")\n    .run_commands(\n        \"python -c \\\"from datasets import load_dataset; load_dataset('wikitext', 'wikitext-2-raw-v1')\\\"\"\n    )\n)\n```\n\n## Cost Analysis\n- Current: ~60s download per run × N runs = wasted GPU time\n- With cache: 0s download\n- Image storage: Negligible (data is \u003c20MB)\n- **ROI**: Pays for itself after 1 prevented timeout\n\n## Acceptance Criteria\n- [ ] Data pre-loaded in Modal image\n- [ ] Zero download time on training runs\n- [ ] No more timeout failures","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-14T08:02:29.118136-08:00","updated_at":"2025-12-14T08:12:21.559211-08:00","closed_at":"2025-12-14T08:12:21.559211-08:00","close_reason":"WikiText-2 now pre-cached in Modal image during build. Data load time reduced from timeout (60s+) to ~13s. Added run_commands step to download dataset during image build."}
{"id":"arcfusion-2si","title":"Improve paper ingestion workflow - API key handling","description":"## Context\nWhen ingesting papers via `arcfusion analyze`, the ANTHROPIC_API_KEY must be set in the shell. During session, the key wasn't available, forcing manual workaround (Claude Code manually extracted components).\n\n## Current State\n- `arcfusion analyze --ids \u003carxiv_ids\u003e` requires ANTHROPIC_API_KEY env var\n- If missing, fails with: `[ERROR] ANTHROPIC_API_KEY environment variable required`\n- No fallback or alternative ingestion path\n\n## Possible Improvements\n1. **Support .env file** - Load from `.env` if env var not set\n2. **Interactive prompt** - Ask for key if missing (with secure input)\n3. **Document the workflow** - Add to README/CLAUDE.md how to set up for paper ingestion\n\n## Acceptance Criteria\n- [ ] Paper ingestion works without manual env var export (via .env)\n- [ ] Clear error message with instructions if key unavailable\n- [ ] Documented in project README","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-15T08:25:28.94891-08:00","updated_at":"2025-12-15T08:25:38.360112-08:00"}
{"id":"arcfusion-30o","title":"Auto-validate generated code before saving","description":"GeneratedCode.save() writes code without calling validate_syntax(). Add automatic validation and sanitization of component names in docstrings to prevent invalid Python from being saved.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-11T13:02:23.288818-08:00","updated_at":"2025-12-11T13:11:37.162829-08:00","closed_at":"2025-12-11T13:11:37.162829-08:00","close_reason":"save() now validates syntax by default, raises ValueError on invalid code"}
{"id":"arcfusion-38g","title":"Add training_runs table to DB schema for detailed benchmark tracking","description":"Add a training_runs table to track: model architecture, training config (steps, lr, batch_size), hardware (GPU type, mixed precision), results (loss, perplexity, time), and baseline comparison data. Links to existing benchmark_results for post-training evaluation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:25:29.635508-08:00","updated_at":"2025-12-13T16:28:40.45964-08:00","closed_at":"2025-12-13T16:28:40.45964-08:00","close_reason":"Added training_runs table to DB schema with full CRUD methods. Tracks model config, hardware, results, and baseline comparison. 228 tests pass."}
{"id":"arcfusion-4lc","title":"Research: Minimum training scale to differentiate architecture quality","description":"## Research Question\n\nHow much training is needed before it becomes apparent whether an architecture is good or bad?\n\n## Why This Matters\n\nIf we can detect architecture quality with minimal training:\n- Faster iteration on dream experiments\n- Lower compute costs\n- More architectures explored per dollar\n\nIf we need substantial training:\n- Need cloud compute integration\n- Batch validation runs\n- Different validation strategy\n\n## Experiment Design\n\n1. **Baseline**: Train known-good architecture (Transformer) at various scales\n   - 100 steps, 1K steps, 10K steps, 100K steps\n   - Track loss curves and final perplexity\n\n2. **Bad architecture**: Train known-bad architecture (random component soup)\n   - Same scale progression\n   - Compare curves to baseline\n\n3. **Analysis**: At what point do curves diverge?\n   - Early divergence = can validate cheaply\n   - Late divergence = need more compute\n\n## Metrics to Track\n\n- Loss at each checkpoint\n- Perplexity trajectory\n- Parameter efficiency (loss per param)\n- Training stability (gradient variance)\n\n## Literature Review\n\n- Scaling laws papers (Chinchilla, etc.)\n- Neural architecture search papers\n- Early stopping criteria research\n\n## Dependencies\n\n- Need PyTorch installed\n- Need cloud compute for larger runs (arcfusion-zzp)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T15:52:04.874825-08:00","updated_at":"2025-12-12T13:57:06.25733-08:00","closed_at":"2025-12-12T13:57:06.25733-08:00","close_reason":"Research complete: Need 2000+ steps to differentiate architecture quality. Simple architectures train faster initially on random data. Updated cloud defaults accordingly.","dependencies":[{"issue_id":"arcfusion-4lc","depends_on_id":"arcfusion-zzp","type":"blocks","created_at":"2025-12-11T15:52:10.640381-08:00","created_by":"daemon"}]}
{"id":"arcfusion-5ux","title":"Validate relationships in analyzer before adding","description":"In analyzer.py lines 334-348, component relationships are added without verifying both components exist. If LLM returns relationship for skipped component (low confidence), it fails silently or creates orphaned refs.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-11T13:02:16.95983-08:00","updated_at":"2025-12-11T13:11:37.107019-08:00","closed_at":"2025-12-11T13:11:37.107019-08:00","close_reason":"Added validation with warnings for skipped relationships"}
{"id":"arcfusion-5wj","title":"Web UI for architecture exploration","description":"Build a web interface to visualize components, relationships, and dream new architectures interactively","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-11T12:15:52.710451-08:00","updated_at":"2025-12-12T14:26:48.162589-08:00","closed_at":"2025-12-12T14:26:48.162589-08:00","close_reason":"Implemented FastAPI web UI with REST API, interactive frontend, vis.js graph, and dream interface"}
{"id":"arcfusion-753","title":"Ensure Modal GPU cleanup to avoid billing","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-14T07:13:39.313164-08:00","updated_at":"2025-12-14T07:14:33.860315-08:00","closed_at":"2025-12-14T07:14:33.860315-08:00","close_reason":"Modal scales to zero by default - we pay only for actual compute. Our use of 'with app.run():' ensures clean shutdown. No scaledown_window/min_containers used. To monitor: 'modal app list' shows running apps."}
{"id":"arcfusion-7xk","title":"Training too slow - evaluate faster GPU options","description":"Current training on A10G takes ~97s per model for 2000 steps. Options to explore:\n\n1. **A100 GPU** (~3x faster, ~3x cost) - best for serious experiments\n2. **H100 GPU** (~5x faster, ~5x cost) - overkill for small models?\n3. **Reduce model size** - smaller d_model/n_layers for faster iteration\n4. **Reduce steps** - 1000 steps might be enough to differentiate architectures\n5. **Batch multiple models** - train in parallel on same GPU\n\nQuestions to answer:\n- What's the minimum training needed to reliably differentiate architectures?\n- Is the bottleneck GPU compute or data loading?\n- Would spot instances save money?","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T17:42:01.101448-08:00","updated_at":"2025-12-14T07:08:20.817428-08:00","closed_at":"2025-12-14T07:08:20.817428-08:00","close_reason":"Switched to A100 GPU. Training now 3-4x faster (55-65s vs 200s on T4)"}
{"id":"arcfusion-8rs","title":"Track arcfusion.db in git","description":"The database file isn't being tracked by git. Add it to version control so component/engine data persists across clones.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T16:19:14.914455-08:00","updated_at":"2025-12-14T16:28:12.598526-08:00","closed_at":"2025-12-14T16:28:12.598526-08:00","close_reason":"Added arcfusion.db and pre2017_experiment.db to git"}
{"id":"arcfusion-9c9","title":"DB table: recipes with adjustments tracking","description":"New DB table to store dreamed recipes:\n- recipe_id (unique)\n- component_ids (ordered list)\n- assembly_instructions (JSON: connections, residuals, shapes)\n- source_strategy (greedy, crossover, mutate, etc.)\n- created_at timestamp\n\nPlus adjustments table:\n- adjustment_id\n- recipe_id (FK)\n- original_value\n- adjusted_value  \n- reason (why modification was needed)\n- adjusted_at timestamp\n\nThis enables: recipe recreation, composer learning from failures, training run reproducibility.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:15:07.58677-08:00","updated_at":"2025-12-12T12:24:43.228152-08:00","closed_at":"2025-12-12T12:24:43.228152-08:00","close_reason":"DB tables implemented: recipes (recipe_id, name, component_ids, assembly, strategy, estimated_score, parent_engine_ids, notes) and recipe_adjustments (adjustment_id, recipe_id, adjustment_type, original_value, adjusted_value, reason, component_id). Full CRUD methods and stats tracking."}
{"id":"arcfusion-a1s","title":"ML Agent: faithful recipe execution with modification tracking","description":"ML Agent receives Recipe from Composer and:\n- Makes best effort to train the model\n- Stays faithful to the recipe provided\n- Records ANY modifications needed to enable training\n- Modifications inform Composer for future dreams\n- Enables recreation of training runs\n\nKey principle: ML Agent has leeway to make necessary adjustments, but ALL adjustments must be recorded.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:15:00.085093-08:00","updated_at":"2025-12-12T12:58:49.512701-08:00","closed_at":"2025-12-12T12:58:49.512701-08:00","close_reason":"ML Agent implemented with execute_recipe(), adjustment tracking, ExecutionResult dataclass. Records all modifications (component_skip, build_fix, train_fix). 16 new tests, 215 total passing.","dependencies":[{"issue_id":"arcfusion-a1s","depends_on_id":"arcfusion-p9w","type":"blocks","created_at":"2025-12-12T12:15:33.445742-08:00","created_by":"daemon"},{"issue_id":"arcfusion-a1s","depends_on_id":"arcfusion-9c9","type":"blocks","created_at":"2025-12-12T12:15:33.493527-08:00","created_by":"daemon"}]}
{"id":"arcfusion-a8o","title":"Multi-agent autonomous development system","description":"Brainstorm: 6-8 AI agents working together:\n- 1 Project Maintainer agent: approves/rejects PRs on GitHub\n- Other agents: claim beads issues, write code, submit PRs to the repo\n- Autonomous development workflow with human oversight via PR reviews\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    HUMAN OVERSIGHT                          │\n│              (GitHub PR Reviews, bd issues)                 │\n└─────────────────────────────────────────────────────────────┘\n                            ▲\n                            │ PRs\n┌─────────────────────────────────────────────────────────────┐\n│                 PROJECT MAINTAINER AGENT                    │\n│  - Reviews PRs from worker agents                           │\n│  - Enforces code standards                                  │\n│  - Manages release/merge decisions                          │\n│  - Creates/prioritizes bd issues                            │\n└─────────────────────────────────────────────────────────────┘\n                            ▲\n              ┌─────────────┼─────────────┐\n              │             │             │\n┌─────────────▼───┐ ┌───────▼───────┐ ┌───▼─────────────┐\n│  WORKER AGENT 1 │ │ WORKER AGENT 2│ │ WORKER AGENT N  │\n│  (Feature Dev)  │ │ (Bug Fixes)   │ │ (Testing/Docs)  │\n└─────────────────┘ └───────────────┘ └─────────────────┘\n         │                  │                  │\n         └──────────────────┼──────────────────┘\n                            ▼\n┌─────────────────────────────────────────────────────────────┐\n│                    SHARED STATE                             │\n│  - bd (beads) issue tracker                                 │\n│  - Git repo (branches per agent)                            │\n│  - Coordination file (.agents/locks.json?)                  │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Agent Roles\n\n| Agent | Responsibility | Tools |\n|-------|---------------|-------|\n| Maintainer | PR review, issue triage, release management | gh, bd, git |\n| Feature Dev | New features, arcfusion improvements | claude code, bd, gh |\n| Bug Fixer | Fix issues, handle errors | claude code, bd, gh |\n| Researcher | Ingest papers, analyze architectures | arcfusion CLI, bd |\n| Trainer | Run cloud training, benchmarks | Modal, bd |\n| Test/QA | Write tests, validate PRs | pytest, bd |\n\n## Coordination Protocol\n\nWorker claims issue:\n1. bd update \u003cid\u003e --status=in_progress --assignee=agent-1\n2. git checkout -b agent-1/\u003cissue-id\u003e\n3. Do work\n4. gh pr create --title \"...\" --body \"Closes \u003cissue\u003e\"\n5. bd update \u003cid\u003e --status=review\n\n## Safety Layers\n\n1. First layer: Maintainer agent reviews all PRs\n2. Second layer: Human reviews Maintainer's merge decisions\n3. Third layer: CI/tests must pass\n\n## Implementation Options\n\nA. Cron-based polling (simple)\nB. Event-driven via GitHub webhooks (responsive)\nC. Long-running daemon processes (complex)\n\n## MVP\n\nStart with:\n1. One worker agent that picks from bd ready, writes code, submits PR\n2. Human as maintainer (manual PR review)\n3. Iterate from there\n\n## Open Questions\n\n- Where do agents run? Local? Cloud VMs? Modal?\n- Cost model for API tokens per agent session\n- Specialization vs one generalist agent\n- Auto-merge threshold or always require human?","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-14T16:19:13.545493-08:00","updated_at":"2025-12-14T20:50:21.349498-08:00"}
{"id":"arcfusion-a9f","title":"Local training validation before cloud deploy","description":"Test training runs locally before deploying to cloud:\n- Catch errors early (syntax, shape mismatches, missing imports)\n- Run quick ~50 step validation\n- Minimize wasted cloud GPU time from training errors","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T16:19:16.918777-08:00","updated_at":"2025-12-14T16:27:47.008033-08:00","closed_at":"2025-12-14T16:27:47.008033-08:00","close_reason":"Implemented validate_local.py - runs forward/backward pass on CPU"}
{"id":"arcfusion-abz","title":"Speed up Modal training - larger GPUs or less data","description":"Current training on Modal T4 GPUs times out at 30 min. Options: 1) Use A10G/A100 GPUs 2) Reduce dataset size 3) Use step-based training vs epochs 4) Reduce model size. Goal: complete training runs in \u003c10 min for fast iteration.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:27.035282-08:00","updated_at":"2025-12-13T16:28:37.654998-08:00","closed_at":"2025-12-13T16:28:37.654998-08:00","close_reason":"Added mixed precision (FP16 autocast + GradScaler) and upgraded to A10G GPU in cloud_train_fair.py. Expected 2-3x speedup."}
{"id":"arcfusion-avc","title":"Validation: Can we discover Transformer without the attention paper?","description":"## Research Question\n\nIf the \"Attention Is All You Need\" paper had never been published, could ArcFusion's dream engine independently discover the Transformer architecture by composing components from other papers?\n\n## Why This Matters\n\nThis is a key validation of the entire ArcFusion approach:\n- If YES → The system can genuinely discover novel architectures\n- If NO → We're just recombining known patterns, not innovating\n\n## Experiment Design\n\n### Setup\n1. Remove Transformer and all attention-related components from the database\n2. Seed only with pre-2017 components (RNNs, CNNs, LSTMs, embeddings, etc.)\n3. Add components from papers that influenced attention (memory networks, seq2seq, etc.)\n\n### Test\n1. Run dream engine with various strategies (greedy, random, crossover, mutate)\n2. Generate many candidate architectures\n3. Analyze: Do any resemble Transformer's key innovations?\n   - Multi-head attention pattern\n   - Encoder-decoder with cross-attention\n   - Positional encodings + self-attention (no recurrence)\n\n### Success Criteria\n- **Strong success**: Dream engine produces attention-like mechanism\n- **Partial success**: Produces components that could lead to attention with minor tweaks\n- **Failure**: Only produces RNN/CNN variants\n\n## Dependencies\n- Robust component extraction from pre-2017 papers\n- Good relationship scoring between components\n- Multiple dream strategies working well\n\n## Notes\n- This may require ingesting more historical papers (2014-2016 era)\n- May need to tune dream engine parameters\n- Could be a good benchmark for measuring system improvement over time","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T14:19:39.83596-08:00","updated_at":"2025-12-11T14:43:31.692448-08:00","closed_at":"2025-12-11T14:43:31.692448-08:00","close_reason":"Experiment complete. PARTIAL SUCCESS: Dream engine found architecture with attention+parallelism+position (5/5 score). Key finding: Can get 80% of the way but misses self-attention, multi-head, and dot-product innovations that required human insight."}
{"id":"arcfusion-ayd","title":"Track component configurations (sub-architectures)","description":"## Problem\n\nCurrently we track full engines (e.g., Transformer with 15 components) but not successful sub-configurations. For example, if 7 specific components from Transformer in a certain order consistently produce good results, we have no way to capture and reuse that pattern.\n\n## Proposed Solution\n\nTrack \"configurations\" - ordered subsets of components that work well together:\n\n### New DB Table: `component_configurations`\n```sql\nCREATE TABLE component_configurations (\n    config_id TEXT PRIMARY KEY,\n    name TEXT,                          -- e.g., \"Transformer Core Block\"\n    description TEXT,\n    component_ids TEXT,                 -- JSON array of component IDs in order\n    source_engine_id TEXT,              -- Engine this was derived from (optional)\n    configuration_score REAL,           -- How well this config performs\n    usage_count INTEGER DEFAULT 0,      -- How often used in dreams\n    validated BOOLEAN DEFAULT 0,        -- Has been tested/validated\n    created_at TIMESTAMP\n);\n```\n\n### Key Features\n1. **Configuration extraction**: Analyze engines to find common successful patterns\n2. **Configuration scoring**: Track which configs produce good dream results\n3. **Dream integration**: Use proven configs as building blocks for new architectures\n4. **Configuration discovery**: Find configs that appear across multiple engines\n\n### Use Cases\n- \"Attention + LayerNorm + FFN\" block appears in many architectures → track as config\n- User validates a dreamed architecture → extract and save its sub-configs\n- Dream engine prefers using proven configs when available\n\n## Acceptance Criteria\n- [ ] Add `component_configurations` table to schema\n- [ ] Add CRUD operations for configurations\n- [ ] Add method to extract configs from existing engines\n- [ ] Integrate with composer to prefer known-good configs\n- [ ] CLI commands: `arcfusion config list/show/extract`","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T14:17:56.139247-08:00","updated_at":"2025-12-11T14:27:49.145668-08:00","closed_at":"2025-12-11T14:27:49.145668-08:00","close_reason":"Feature complete: DB schema, CRUD ops, extraction, CLI commands, and dream integration all implemented. 167 tests passing."}
{"id":"arcfusion-b59","title":"Add results-aware dream strategy","description":"## Problem\nWhen dreaming up new architectures, the system doesn't reference past benchmark results from the DB. A human ML researcher would study what worked before (Mamba's quality wins, GQA/MQA's speed wins) and use that to inform new designs.\n\n## Solution\nCreate a results-aware dream strategy that:\n\n1. **Queries Past Results** - Load training_runs from DB to understand what worked\n2. **Identifies Winning Components** - Which attention types excel at quality vs speed?\n3. **Analyzes Trade-offs** - Use efficiency_score (quality * speed) to rank approaches  \n4. **Generates Informed Hypotheses** - Dream architectures that combine winning traits\n\n## Implementation Steps\n1. Add `get_component_performance_stats()` to db.py - aggregate results by component type\n2. Add `results_aware` strategy to composer.py - queries DB before dreaming\n3. Strategy should output reasoning: 'Mamba has best quality (1.25x baseline), GQA best speed (3.78x), trying hybrid with GQA projection on Mamba core'\n4. Save dreamed config with reference to which past results inspired it\n\n## Acceptance Criteria\n- [ ] Dream strategy can access and interpret past benchmark results\n- [ ] Strategy logs reasoning about why it chose specific components\n- [ ] New architectures include metadata about their design rationale\n\n## Related\n- arcfusion-zdr: Patch Transformer weaknesses (analyzes what doesn't work)\n- Current efficiency scores: Mamba (0.36), Hybrid (0.42), MHA (1.07), GQA (3.63)","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-15T06:18:25.18003-08:00","updated_at":"2025-12-15T06:24:22.092571-08:00","closed_at":"2025-12-15T06:24:22.092571-08:00","close_reason":"Implemented results_aware dream strategy with get_model_performance_stats() in db.py and results_aware_compose() in composer.py"}
{"id":"arcfusion-b9y","title":"Use cached baseline average - train transformer N times, store mean result","description":"The vanilla Transformer_MHA baseline is critical for fair comparisons. Current approach trains it once and caches. Better approach:\n\n1. **Train baseline N times** (e.g., 3-5 runs) with different seeds\n2. **Store all runs** in training_runs table  \n3. **Compute mean + stddev** for perplexity\n4. **Use mean as reference** for vs_baseline_pct calculations\n5. **Never retrain** - just pull cached average from DB\n\nBenefits:\n- More reliable baseline (reduces variance from random init)\n- Faster benchmarking (skip baseline training entirely)\n- Statistical significance (can compute confidence intervals)\n\nImplementation:\n- Add 'seed' field to TrainingRun and CONFIG\n- Add get_baseline_stats(config) -\u003e (mean_ppl, std_ppl, n_runs)\n- Run baseline 3-5x on first setup, then never again","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-13T17:42:04.211643-08:00","updated_at":"2025-12-13T19:07:43.273021-08:00","closed_at":"2025-12-13T19:07:43.273021-08:00","close_reason":"Implemented baseline averaging: added seed to TrainingRun, get_baseline_stats(), get_baseline_seeds_needed() in db.py. Updated cloud_train_fair.py to train N baselines with different seeds and use mean perplexity for comparisons. Testing blocked by Modal service RemoteError - retry later."}
{"id":"arcfusion-ctf","title":"PyTorch code generation from dreamed architectures","description":"Generate runnable PyTorch code from dreamed component combinations, using code_sketch fields and interface compatibility","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T12:15:49.586369-08:00","updated_at":"2025-12-11T12:37:51.398835-08:00","closed_at":"2025-12-11T12:37:51.398835-08:00","close_reason":"Implemented CodeGenerator with generate CLI command. Generates valid PyTorch nn.Module code from dreamed architectures using greedy, random, crossover, and mutate strategies."}
{"id":"arcfusion-d7u","title":"Core database schema with components, engines, relationships","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:17.526253-08:00","updated_at":"2025-12-11T12:15:31.395323-08:00","closed_at":"2025-12-11T12:15:31.395323-08:00","close_reason":"Implemented: 8 tables (components, engines, relationships, compatibility, papers, benchmarks, dreams, validations)"}
{"id":"arcfusion-e2f","title":"Validation: Implement Transformer using only DB components via auto-pipeline","description":"## Goal\n\nTest the auto-validation pipeline by implementing the Transformer architecture using ONLY components extracted into our database - no external code.\n\n## Why This Test\n\n- We KNOW Transformer works (it's a winning recipe)\n- If our pipeline can reconstruct it from DB components and train successfully, the pipeline is validated\n- Fair comparison: only use what we've extracted, not code from the internet\n\n## Test Protocol\n\n1. **Component Selection**\n   - Query DB for Transformer-relevant components\n   - Use: MultiHeadAttention, FeedForward, LayerNorm, Embedding, etc.\n   - Only components with code sketches from our extraction\n\n2. **Model Assembly**\n   - Use CodeGenerator to assemble from DB components\n   - Standard Transformer config: 6 layers, 512 dim, 8 heads\n   - Keep it small for fast iteration\n\n3. **Training**\n   - Small dataset (WikiText-2 or similar)\n   - Short training run (sanity check, not SOTA)\n   - Track loss curves\n\n4. **Benchmark**\n   - Measure perplexity\n   - Compare to known Transformer baselines\n   - Store in benchmark_results\n\n## Success Criteria\n\n- [ ] Model assembles from DB components without manual code\n- [ ] Model trains without errors\n- [ ] Achieves reasonable perplexity (not random)\n- [ ] Results stored in DB\n- [ ] Pipeline proven end-to-end\n\n## Why \"Fair Comparison\" Matters\n\nIf we copy Transformer code from HuggingFace, we're not testing our system. The whole point is: can our extracted component representations actually work?\n\n## Dependencies\n\n- Auto-validation pipeline (arcfusion-040)\n- Sufficient Transformer components in DB (we have these from seeds)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T15:13:20.534039-08:00","updated_at":"2025-12-11T17:47:48.47835-08:00","closed_at":"2025-12-11T17:47:48.47835-08:00","close_reason":"Validation pipeline works end-to-end: Transformer builds from DB components (5 components, 1M params), trains successfully (50 steps, loss ~6.9), and benchmarks run correctly. Fixed vocab_size propagation in codegen.py and validator.py.","dependencies":[{"issue_id":"arcfusion-e2f","depends_on_id":"arcfusion-040","type":"blocks","created_at":"2025-12-11T15:13:26.60807-08:00","created_by":"daemon"}]}
{"id":"arcfusion-ecq","title":"Brainstorm: Ingest more arXiv papers or focus on training?","description":"## Question\nShould we prioritize ingesting more arXiv papers into the component database, or focus on training/benchmarking with what we have?\n\n## Decision: Focus on Training with Selective Ingestion\n\nAfter analyzing our current state:\n- 113 components already in DB (Mamba, RetentionHead, FlashAttention, RotaryEmbedding, etc.)\n- 22 engines (BERT, GPT-2, LLaMA, Falcon)\n- 7 training runs with real benchmarks showing Mamba -20% vs MHA\n\n**Strategy:**\n1. **Primary focus: Training** - Validate more architectures with our existing components\n2. **Selective ingestion** - Only add papers for specific components we want to test:\n   - Linear Attention (to compare O(n) alternatives to Mamba)\n   - RWKV (another recurrent approach to compare)\n   - NOT: papers for components we already have well-covered\n\n## Rationale\n- Training validates which components actually matter in practice\n- Our Mamba result (-20% perplexity) proves the pipeline works\n- Efficiency scores show GQA/MQA are most efficient (quality × speed)\n- Better to deeply validate fewer components than broadly catalog many\n\n## Next Actions\n1. Run more training experiments with existing components\n2. If we need a specific capability (e.g., linear attention), then ingest that paper\n3. Don't ingest papers \"just in case\" - it's wasted effort\n\n## Current Benchmark Results\n\n| Model | Perplexity | Quality | Speed | Efficiency |\n|-------|-----------|---------|-------|------------|\n| Mamba | 226.1 | 1.248 | 0.29x | 0.362 |\n| Hybrid | 242.9 | 1.162 | 0.36x | 0.421 |\n| MHA | 274.8 | 1.027 | 1.04x | 1.071 |\n| GQA | 294.0 | 0.960 | 3.78x | 3.632 |\n| MQA | 294.1 | 0.960 | 3.50x | 3.355 |","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-14T07:43:14.477875-08:00","updated_at":"2025-12-15T06:10:24.508501-08:00","closed_at":"2025-12-15T06:10:24.508501-08:00","close_reason":"Decision made: Focus on training with selective paper ingestion. We have 113 components already - only ingest papers for specific capabilities we want to test (Linear Attention, RWKV). Training validates what matters."}
{"id":"arcfusion-f73","title":"Add test coverage for untested modules","description":"composer.py, dedup.py, decomposer.py, and seeds.py have no dedicated tests. Add tests for:\n- All 4 dream strategies (greedy, random, mutate, crossover)\n- Duplicate detection and merging\n- Paper decomposition\n- Database seeding","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T13:02:07.568054-08:00","updated_at":"2025-12-11T13:07:14.417626-08:00","closed_at":"2025-12-11T13:07:14.417626-08:00","close_reason":"Added 102 new tests (157 total) covering composer, dedup, decomposer, and seeds modules"}
{"id":"arcfusion-frl","title":"Fuzzy deduplication with variant detection","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:20.748145-08:00","updated_at":"2025-12-11T12:15:34.510907-08:00","closed_at":"2025-12-11T12:15:34.510907-08:00","close_reason":"Implemented: ComponentDeduplicator with normalized names, semantic signatures, architecture variant detection"}
{"id":"arcfusion-hjg","title":"Implement local testing process before cloud deployment","description":"## Problem\nDeploying untested code to Modal wastes GPU time and money when there are bugs. We should validate locally first.\n\n## Proposed Process\n1. **Syntax check**: Run model code through exec() locally\n2. **Shape test**: Forward pass with small batch (2, 16)\n3. **Parameter count**: Verify comparable to baseline (~30M params)\n4. **Gradient test**: One backward pass to check autograd works\n5. **Only then**: Deploy to Modal for full training\n\n## Implementation\nCreate a `test_model_locally()` function that:\n```python\ndef test_model_locally(model_code: str, model_name: str) -\u003e bool:\n    # 1. Parse and exec\n    ns = {}\n    exec(model_code, ns)\n    model_class = ns.get(model_name)\n    \n    # 2. Instantiate\n    model = model_class(d_model=256, vocab_size=50257, n_layers=4, n_heads=8)\n    \n    # 3. Forward pass\n    x = torch.randint(0, 50257, (2, 16))\n    out = model(x)\n    assert out.shape == (2, 16, 50257)\n    \n    # 4. Backward pass\n    loss = out.sum()\n    loss.backward()\n    \n    return True\n```\n\n## Benefits\n- Catch bugs before spending GPU credits\n- Faster iteration (no Modal cold start)\n- Validate shapes and gradients work\n- Parameter count sanity check\n\n## Acceptance Criteria\n- [ ] Local test function implemented\n- [ ] Integrated into cloud_train_fair.py workflow\n- [ ] All new models pass local test before cloud","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T08:02:28.935724-08:00","updated_at":"2025-12-14T16:43:04.709535-08:00","closed_at":"2025-12-14T16:43:04.709535-08:00","close_reason":"Duplicate of arcfusion-a9f (local validation). arcfusion-a9f was completed previously."}
{"id":"arcfusion-hns","title":"Focus on text modality only - no vision/audio/multimodal","description":"ArcFusion should focus exclusively on text/language model architectures. No vision, audio, or multimodal components. This keeps the scope manageable and allows fair comparisons between attention mechanisms on language modeling tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T16:46:57.824019-08:00","updated_at":"2025-12-13T16:53:55.36472-08:00","closed_at":"2025-12-13T16:53:55.36472-08:00","close_reason":"Established text-only focus. Training script now uses WikiText-2 with GPT-2 tokenizer. All benchmarks are language modeling (perplexity)."}
{"id":"arcfusion-iyg","title":"Always include vanilla transformer as baseline in benchmarks","description":"Every benchmark run should include a vanilla transformer (standard MHA) as the baseline. This provides a consistent reference point to measure improvements or regressions from architectural changes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:30.956414-08:00","updated_at":"2025-12-13T16:28:38.899894-08:00","closed_at":"2025-12-13T16:28:38.899894-08:00","close_reason":"Baseline transformer (Transformer_MHA) is now always trained first in cloud_train_fair.py. All other models show vs_baseline comparison."}
{"id":"arcfusion-l3a","title":"Expand paper knowledge base","description":"Analyze more seminal papers: GPT-3, PaLM, Gemini, Claude architecture (if published), Mixtral, Phi, etc.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-11T12:15:50.758608-08:00","updated_at":"2025-12-12T14:17:32.259717-08:00","closed_at":"2025-12-12T14:17:32.259717-08:00","close_reason":"Covered by arcfusion-xol - papers/architectures expanded together."}
{"id":"arcfusion-mnx","title":"Add training insights table with auto-summary after runs","description":"## Goal\nAfter each training run, automatically generate and store a text summary of what worked and what didn't. These insights inform future dreaming sessions.\n\n## DB Schema Changes\n\n```sql\n-- New table for storing insights/findings\nCREATE TABLE IF NOT EXISTS training_insights (\n    id TEXT PRIMARY KEY,\n    created_at TEXT DEFAULT (datetime('now')),\n    \n    -- What triggered this insight\n    source_run_id TEXT,              -- Training run that led to this insight\n    source_comparison TEXT,          -- e.g., 'AttnFirst vs MambaHeavy'\n    \n    -- The insight itself\n    category TEXT,                   -- 'architecture', 'attention', 'efficiency', 'training'\n    title TEXT,                      -- Short summary: 'Attention at END \u003e START'\n    description TEXT,                -- Full explanation\n    \n    -- Supporting evidence\n    evidence_json TEXT,              -- JSON with PPL comparisons, times, etc.\n    confidence REAL DEFAULT 0.8,     -- 0.0-1.0\n    \n    -- For retrieval\n    tags TEXT,                       -- Comma-separated: 'mamba,hybrid,attention,position'\n    \n    FOREIGN KEY (source_run_id) REFERENCES training_runs(id)\n);\n\nCREATE INDEX IF NOT EXISTS idx_insights_category ON training_insights(category);\nCREATE INDEX IF NOT EXISTS idx_insights_tags ON training_insights(tags);\n```\n\n## Python Implementation\n\n### 1. Add to db.py - Dataclass and CRUD\n\n```python\n@dataclass\nclass TrainingInsight:\n    id: str = ''\n    created_at: str = ''\n    source_run_id: str = ''\n    source_comparison: str = ''\n    category: str = ''  # architecture, attention, efficiency, training\n    title: str = ''\n    description: str = ''\n    evidence_json: str = ''\n    confidence: float = 0.8\n    tags: str = ''\n\n\nclass ArcFusionDB:\n    # ... existing code ...\n    \n    def _init_insights_table(self):\n        '''Create training_insights table if not exists'''\n        self.conn.execute('''\n            CREATE TABLE IF NOT EXISTS training_insights (\n                id TEXT PRIMARY KEY,\n                created_at TEXT DEFAULT (datetime('now')),\n                source_run_id TEXT,\n                source_comparison TEXT,\n                category TEXT,\n                title TEXT,\n                description TEXT,\n                evidence_json TEXT,\n                confidence REAL DEFAULT 0.8,\n                tags TEXT\n            )\n        ''')\n        self.conn.execute('CREATE INDEX IF NOT EXISTS idx_insights_category ON training_insights(category)')\n        self.conn.commit()\n    \n    def add_insight(self, insight: TrainingInsight) -\u003e str:\n        '''Add a new training insight'''\n        if not insight.id:\n            insight.id = str(uuid.uuid4())[:12]\n        if not insight.created_at:\n            insight.created_at = datetime.now().isoformat()\n        \n        self.conn.execute('''\n            INSERT INTO training_insights \n            (id, created_at, source_run_id, source_comparison, category, title, description, evidence_json, confidence, tags)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n        ''', (insight.id, insight.created_at, insight.source_run_id, insight.source_comparison,\n              insight.category, insight.title, insight.description, insight.evidence_json,\n              insight.confidence, insight.tags))\n        self.conn.commit()\n        return insight.id\n    \n    def get_insights(self, category: str = None, tags: list = None, min_confidence: float = 0.0) -\u003e list:\n        '''Get insights filtered by category and/or tags'''\n        query = 'SELECT * FROM training_insights WHERE confidence \u003e= ?'\n        params = [min_confidence]\n        \n        if category:\n            query += ' AND category = ?'\n            params.append(category)\n        \n        if tags:\n            for tag in tags:\n                query += ' AND tags LIKE ?'\n                params.append(f'%{tag}%')\n        \n        query += ' ORDER BY created_at DESC'\n        rows = self.conn.execute(query, params).fetchall()\n        return [TrainingInsight(**dict(r)) for r in rows]\n    \n    def get_insights_for_dreaming(self) -\u003e dict:\n        '''Get structured insights to inform dream strategies'''\n        insights = self.get_insights(min_confidence=0.7)\n        \n        result = {\n            'architecture': [],\n            'attention': [],\n            'efficiency': [],\n            'recommendations': []\n        }\n        \n        for i in insights:\n            if i.category in result:\n                result[i.category].append({\n                    'title': i.title,\n                    'description': i.description,\n                    'confidence': i.confidence\n                })\n        \n        # Generate recommendations from insights\n        for i in insights:\n            if 'best' in i.title.lower() or 'better' in i.title.lower():\n                result['recommendations'].append(i.description)\n        \n        return result\n```\n\n### 2. Auto-generate insights after training\n\n```python\ndef generate_training_insights(db: ArcFusionDB, new_run_id: str) -\u003e list:\n    '''Auto-generate insights by comparing new run to existing runs'''\n    import json\n    \n    new_run = db.get_training_run(new_run_id)\n    if not new_run or not new_run.success:\n        return []\n    \n    insights = []\n    all_runs = db.list_training_runs(success_only=True)\n    \n    # Group by attention type for comparison\n    def get_attn_type(name):\n        if 'Mamba' in name and 'Hybrid' not in name and 'Heavy' not in name and 'First' not in name:\n            return 'SSM'\n        elif 'MHA' in name:\n            return 'MHA'\n        elif 'GQA' in name:\n            return 'GQA'\n        elif 'MQA' in name:\n            return 'MQA'\n        elif 'Hybrid' in name:\n            return 'Hybrid'\n        elif 'Heavy' in name:\n            return 'MambaHeavy'\n        elif 'First' in name:\n            return 'AttnFirst'\n        return 'Unknown'\n    \n    # Compare to baseline (MHA)\n    baseline_runs = [r for r in all_runs if 'MHA' in r.model_name and 'Hybrid' not in r.model_name]\n    if baseline_runs:\n        baseline_ppl = sum(r.perplexity for r in baseline_runs) / len(baseline_runs)\n        \n        if new_run.perplexity \u003c baseline_ppl * 0.95:  # 5%+ improvement\n            quality_gain = (baseline_ppl - new_run.perplexity) / baseline_ppl * 100\n            insights.append(TrainingInsight(\n                source_run_id=new_run_id,\n                source_comparison=f'{new_run.model_name} vs MHA baseline',\n                category='architecture',\n                title=f'{get_attn_type(new_run.model_name)} beats baseline by {quality_gain:.1f}%',\n                description=f'{new_run.model_name} achieved PPL {new_run.perplexity:.1f} vs baseline {baseline_ppl:.1f}. '\n                           f'This {get_attn_type(new_run.model_name)} architecture shows {quality_gain:.1f}% quality improvement.',\n                evidence_json=json.dumps({\n                    'new_ppl': new_run.perplexity,\n                    'baseline_ppl': baseline_ppl,\n                    'improvement_pct': quality_gain\n                }),\n                confidence=0.9 if quality_gain \u003e 10 else 0.7,\n                tags=f'{get_attn_type(new_run.model_name).lower()},quality,improvement'\n            ))\n    \n    # Compare similar architectures (e.g., AttnFirst vs MambaHeavy)\n    similar_runs = [r for r in all_runs if r.id != new_run_id and \n                   r.parameters == new_run.parameters]  # Same size models\n    \n    for other in similar_runs:\n        if abs(new_run.perplexity - other.perplexity) \u003e 5:  # Significant difference\n            better = new_run if new_run.perplexity \u003c other.perplexity else other\n            worse = other if new_run.perplexity \u003c other.perplexity else new_run\n            \n            insights.append(TrainingInsight(\n                source_run_id=new_run_id,\n                source_comparison=f'{better.model_name} vs {worse.model_name}',\n                category='attention',\n                title=f'{get_attn_type(better.model_name)} \u003e {get_attn_type(worse.model_name)} for quality',\n                description=f'{better.model_name} (PPL {better.perplexity:.1f}) outperforms '\n                           f'{worse.model_name} (PPL {worse.perplexity:.1f}). '\n                           f'Difference: {worse.perplexity - better.perplexity:.1f} PPL.',\n                evidence_json=json.dumps({\n                    'better': {'model': better.model_name, 'ppl': better.perplexity},\n                    'worse': {'model': worse.model_name, 'ppl': worse.perplexity}\n                }),\n                confidence=0.85,\n                tags=f'{get_attn_type(better.model_name).lower()},{get_attn_type(worse.model_name).lower()},comparison'\n            ))\n    \n    # Save all insights\n    for insight in insights:\n        db.add_insight(insight)\n    \n    return insights\n```\n\n### 3. Wire into training script (cloud_train_fair.py)\n\n```python\n# At the end of save_result_to_db():\ndef save_result_to_db(db, result, config, baseline_run_id=''):\n    # ... existing code to save run ...\n    run_id = db.add_training_run(run)\n    \n    # Auto-generate insights from this run\n    insights = generate_training_insights(db, run_id)\n    if insights:\n        print(f'  Generated {len(insights)} new insights')\n        for i in insights:\n            print(f'    - {i.title}')\n    \n    return run_id\n```\n\n## Example Insights That Would Be Auto-Generated\n\nFrom today's experiments, these insights would be created:\n\n1. **'SSM beats baseline by 21.5%'**\n   - Mamba achieved PPL 226.1 vs baseline 274.8\n\n2. **'MambaHeavy \u003e AttnFirst for quality'** \n   - Attention at END (PPL 247) outperforms attention at START (PPL 254)\n   - Key learning: Mamba builds representations that attention refines\n\n3. **'Interleaved attention best for hybrids'**\n   - Jamba-style 2:1 (PPL 242.9) beats both 3:1 variants\n\n## Acceptance Criteria\n- [ ] Add training_insights table to DB\n- [ ] Implement TrainingInsight dataclass\n- [ ] Implement add_insight() and get_insights() \n- [ ] Implement generate_training_insights() auto-summarizer\n- [ ] Wire into save_result_to_db()\n- [ ] Test with existing training runs","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-15T07:04:59.648085-08:00","updated_at":"2025-12-15T07:10:13.035857-08:00","closed_at":"2025-12-15T07:10:13.035857-08:00","close_reason":"Implemented training_insights table with TrainingInsight dataclass, CRUD methods (add_insight, get_insight, get_insights, delete_insight), get_insights_for_dreaming(), and generate_training_insights() auto-summarizer"}
{"id":"arcfusion-p9w","title":"Recipe dataclass: ordered components + assembly instructions","description":"Composer outputs a Recipe dataclass containing:\n- Ordered list of component IDs\n- Assembly instructions (how components connect, residuals, etc.)\n- Metadata (strategy used, score, timestamp)\n\nThis is the handoff format between Composer and ML Agent.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:14:52.735707-08:00","updated_at":"2025-12-12T12:24:41.769431-08:00","closed_at":"2025-12-12T12:24:41.769431-08:00","close_reason":"Recipe dataclass implemented with: name, component_ids, assembly instructions (connections, residuals, shapes, categories, notes), strategy, estimated_score, parent_engine_ids. Composer.create_recipe() generates Recipes from any dream strategy."}
{"id":"arcfusion-pqn","title":"Ensure all benchmark results are saved","description":"## Problem\nTraining runs produce valuable benchmark data but not all results are being persisted properly.\n\n## Current State\n- Results saved to experiments/fair_comparison_results.json\n- Some results saved to DB via add_training_run()\n- No guarantee all runs are captured\n\n## Requirements\n1. Every training run should be saved to DB automatically\n2. Results should include all metrics (loss, perplexity, time, params)\n3. Comparison results should be queryable (vs baseline %)\n4. Should be able to regenerate any results JSON from DB\n\n## Acceptance Criteria\n- [ ] All training runs persisted to DB\n- [ ] Can query historical comparisons\n- [ ] Results JSON can be regenerated from DB","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T07:37:13.522653-08:00","updated_at":"2025-12-15T06:06:13.196868-08:00","closed_at":"2025-12-15T06:06:13.196868-08:00","close_reason":"Implemented export_results_json and update_vs_baseline_pct methods in db.py, added export-results CLI command. vs_baseline_pct now calculated and persisted for all runs."}
{"id":"arcfusion-qhj","title":"Cache baseline transformer - train once, reference forever","description":"Train vanilla Transformer_MHA baseline once and save to training_runs table. Future benchmark runs should load the cached baseline result instead of retraining. Only retrain baseline when config changes (d_model, n_layers, etc.).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-13T16:55:47.117325-08:00","updated_at":"2025-12-13T17:33:18.582724-08:00","closed_at":"2025-12-13T17:33:18.582724-08:00","close_reason":"Baseline caching implemented via config hash matching. Train vanilla transformer once, lookup by config_hash in notes field."}
{"id":"arcfusion-sxn","title":"Save key findings to DB - schema changes needed","description":"## Problem\nKey findings from training (like Mamba beating Transformers by 20%) should be saved to DB for historical analysis, but current schema may not support this.\n\n## Key Findings to Track\n- Architecture comparisons (A vs B perplexity delta)\n- Breakthrough results (significant improvements)\n- Failed experiments (what didn't work)\n- Component combinations tested\n- Statistical significance (std dev, n runs)\n\n## Current Schema Gaps\n- No way to link related training runs (same experiment)\n- No experiment_id or batch_id concept\n- No findings/notes table for insights\n- Limited metadata on runs\n\n## Proposed Schema Changes\n1. Add experiment_runs table to group related runs\n2. Add findings table for insights/conclusions\n3. Add experiment_id FK to training_runs\n4. Add tags/labels for categorization\n\n## Acceptance Criteria\n- [ ] Can group runs into experiments\n- [ ] Can record findings/insights with evidence\n- [ ] Can query historical comparisons\n- [ ] Can tag/categorize experiments","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-14T07:37:15.761391-08:00","updated_at":"2025-12-14T16:43:06.984131-08:00","closed_at":"2025-12-14T16:43:06.984131-08:00","close_reason":"Implemented experiments and findings tables with full CRUD operations for scientific tracking"}
{"id":"arcfusion-v3i","title":"Benchmark integration for architecture scoring","description":"Add support for storing and querying benchmark results (perplexity, accuracy, speed) to score dreamed architectures against real performance data","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T12:15:47.542041-08:00","updated_at":"2025-12-11T15:18:29.598876-08:00","closed_at":"2025-12-11T15:18:29.598876-08:00","close_reason":"Complete: CLI commands for add/list/leaderboard/show/compare. DB layer was already done."}
{"id":"arcfusion-wbi","title":"Component DB granularity for distinct, trainable recipes","description":"Ensure component database is fine-grained enough that:\n1. Composer can create recipes that are trainable by ML Agent\n2. Recipes stay faithful to the original idea\n3. Resulting models are DISTINCT - don't all blend together\n\nMay require:\n- More specific component variants (not just 'Attention' but 'MultiHeadAttention', 'GroupedQueryAttention', etc.)\n- Clearer interface specifications\n- Assembly instruction patterns that preserve architectural intent\n- Component compatibility metadata","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-12T12:15:27.388153-08:00","updated_at":"2025-12-12T13:21:23.836477-08:00","closed_at":"2025-12-12T13:21:23.836477-08:00","close_reason":"Added MQA/SlidingWindow/Linear attention variants, temperature sampling for greedy, validity constraints for random walk, fixed categorization"}
{"id":"arcfusion-wcm","title":"LLM-powered paper analysis with Claude API","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:19.280082-08:00","updated_at":"2025-12-11T12:15:33.171477-08:00","closed_at":"2025-12-11T12:15:33.171477-08:00","close_reason":"Implemented: PaperAnalyzer extracts components with interfaces, hyperparameters, complexity, code sketches"}
{"id":"arcfusion-xol","title":"Scale up: ingest more ML papers into pipeline","description":"## Goal\n\nExpand the knowledge base by ingesting more ML architecture papers into the pipeline.\n\n## Why Now\n\nAs we build out the auto-validation pipeline, we need:\n- More components to dream with\n- More proven architectures to learn from\n- Better relationship data between components\n\n## Papers to Prioritize\n\n### Foundational (pre-2020)\n- ResNet (residual connections)\n- ELMo (contextualized embeddings)\n- GPT-1 (decoder-only transformer)\n- XLNet (permutation language modeling)\n- T5 (encoder-decoder, text-to-text)\n\n### Modern (2020-2024)\n- GPT-3/4 architecture details\n- PaLM (pathways, scaling)\n- Chinchilla (compute-optimal scaling)\n- Mistral/Mixtral (MoE, sliding window)\n- Llama 2/3 (grouped query attention, improvements)\n- Gemma (efficient small models)\n- Phi-1/2/3 (data quality focus)\n- RWKV variants\n- Mamba 2\n\n### Efficiency/Training\n- LoRA (low-rank adaptation)\n- QLoRA (quantized fine-tuning)\n- Flash Attention 2/3\n- Ring Attention\n- Mixture of Experts papers\n\n## Approach\n\n1. Use ArxivFetcher to batch fetch papers\n2. Run through PaperAnalyzer for deep extraction\n3. Deduplicate components\n4. Build relationship graph\n\n## Success Criteria\n\n- [ ] 50+ papers ingested\n- [ ] 100+ unique components\n- [ ] Rich relationship graph for dreaming","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T15:13:19.150207-08:00","updated_at":"2025-12-12T14:17:31.363926-08:00","closed_at":"2025-12-12T14:17:31.363926-08:00","close_reason":"Expanded DB: 72 components (+9), 17 engines (+6). Added LoRA, MoE, ALiBi, CrossAttention, T5, Mixtral, PaLM, BLOOM, Falcon, GPT-NeoX."}
{"id":"arcfusion-xw9","title":"Include training time in results rankings","description":"## Problem\nCurrent rankings only show perplexity, but training time matters for practical use. Mamba takes 706s vs MHA's 57s - that's important context.\n\n## Current Results (missing time context)\n```\nModel             Perplexity    vs Baseline\nMamba             226.14        -19.9%\nHybrid            242.90        -13.9%\nMHA               282.26        baseline\n```\n\n## Proposed Results (with time)\n```\nModel             Perplexity    vs Baseline    Time      Speed\nMamba             226.14        -19.9%         706s      0.08x\nHybrid            242.90        -13.9%         565s      0.10x\nMHA               282.26        baseline       57s       1.0x\nGQA               294.03        +4.2%          61s       0.93x\nMQA               294.05        +4.2%          54s       1.06x\n```\n\n## Implementation\n1. Already tracking `time_seconds` in results\n2. Add to summary table formatting\n3. Calculate speed relative to baseline\n4. Store in fair_comparison_results.json\n\n## Quality/Speed Tradeoff Metric\nCould add a combined metric:\n- `efficiency = perplexity_improvement / time_cost`\n- Shows \"perplexity gain per second spent\"\n\n## Acceptance Criteria\n- [ ] Time shown in results table\n- [ ] Speed relative to baseline calculated\n- [ ] Results JSON includes time data\n- [ ] Optional: efficiency metric","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T08:02:30.631525-08:00","updated_at":"2025-12-14T08:16:36.76025-08:00","closed_at":"2025-12-14T08:16:36.76025-08:00","close_reason":"Added Time column to results summary table. Training time now displayed alongside perplexity and baseline comparison."}
{"id":"arcfusion-xxm","title":"Batch training for dreamed architectures","description":"Train dreamed-up architecture ideas in batches:\n- Queue up multiple dream compositions\n- Run overnight or when user is AFK\n- Collect results for review later\n- Maximize GPU utilization during off-hours","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-14T16:19:15.836125-08:00","updated_at":"2025-12-14T16:32:31.174725-08:00","closed_at":"2025-12-14T16:32:31.174725-08:00","close_reason":"Added scripts/batch_train.py for overnight batch training"}
{"id":"arcfusion-y8w","title":"Run trained models through benchmarks and record results in DB","description":"After training, run models through standard benchmarks. Ensure benchmark difficulty is calibrated so models don't saturate near 100% (cannot distinguish between models). Store benchmark results in arcfusion.db for tracking and comparison.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:28.919697-08:00","updated_at":"2025-12-13T17:33:17.771115-08:00","closed_at":"2025-12-13T17:33:17.771115-08:00","close_reason":"Training results now saved to training_runs table with full config, hardware, and baseline comparison data."}
{"id":"arcfusion-ye0","title":"Combined perplexity + training time score","description":"Training time is almost as important as perplexity. Create a composite score:\n- Combine perplexity improvement with training efficiency\n- Rank all results by this combined metric\n- Help identify architectures that are both good AND fast\n- Consider: score = perplexity_improvement / log(training_time) or similar","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-14T16:19:19.884443-08:00","updated_at":"2025-12-14T16:24:31.268554-08:00","closed_at":"2025-12-14T16:24:31.268554-08:00","close_reason":"Implemented combined score: 0.7*ppl + 0.3*time efficiency"}
{"id":"arcfusion-zdr","title":"Brainstorm: Patch Transformer weaknesses with dreamed architectures","description":"## Context\nTransformer is the dominant architecture for AI but has known weaknesses. Our pipeline can dream new architectures by combining components. We validated this with Mamba achieving 20% better perplexity than MHA.\n\n## Updated Benchmark Results (Dec 2024)\n\n| Architecture | PPL | Quality | Time (s) | Notes |\n|-------------|-----|---------|----------|-------|\n| Mamba (pure SSM) | 226.1 | **1.215x** | 1919.8 | Best quality |\n| Hybrid (2:1 Jamba) | 242.9 | 1.131x | 1542.7 | Interleaved attention |\n| **MambaHeavy (3:1)** | 247.0 | 1.112x | **563.5** | Attn at end only |\n| MHA (baseline) | 274.8 | 1.000x | 287.0 | Standard Transformer |\n| GQA | 294.0 | 0.934x | 75.8 | Fastest |\n| MQA | 294.1 | 0.934x | 82.0 | KV-efficient |\n\n## Key Findings\n\n### Quality Ranking\n1. **Pure Mamba** - Best perplexity (226.1), 21.5% better than baseline\n2. **Hybrid (2:1)** - Good balance (242.9), 13.1% better\n3. **MambaHeavy (3:1)** - Attention-at-end (247.0), 11.2% better\n4. **MHA** - Baseline (274.8)\n\n### Important Discovery: Attention Position Matters\n- **MambaHeavy** (attention at END): PPL 247.0 in 563.5s\n- **Hybrid** (attention INTERLEAVED): PPL 242.9 in 1542.7s\n- **Conclusion**: Interleaved attention is better for quality\n- **But**: MambaHeavy is 3x faster than Hybrid!\n\n### Tradeoff Analysis\n- Want best quality? Use **pure Mamba** \n- Want quality + speed balance? Use **MambaHeavy** (3x faster than Hybrid, only slightly worse)\n- Want fastest training? Use **GQA/MQA** (but worse quality)\n\n## Architecture Patterns Tested\n\n### Pattern A: Interleaved (Jamba-style)\n```\n[Mamba] -\u003e [Mamba] -\u003e [MHA] -\u003e [Mamba]  # 2:1 ratio\n```\nResult: Good quality (242.9 ppl) but slow (1542.7s)\n\n### Pattern B: Attention-at-End  \n```\n[Mamba] -\u003e [Mamba] -\u003e [Mamba] -\u003e [MHA]  # 3:1 ratio\n```\nResult: Slightly worse (247.0 ppl) but 3x faster (563.5s)\n\n## Remaining Questions\n- [ ] Would attention-FIRST work better than attention-END?\n- [ ] What about 4:1 ratio (pure Mamba + one attention)?\n- [ ] Linear attention as alternative to O(n²) attention?\n\n## Completed\n- [x] Benchmark pure MHA, GQA, MQA, Mamba\n- [x] Implement and test Hybrid (2:1 Jamba-style)\n- [x] Implement and test MambaHeavy (3:1 attention-at-end)\n- [x] Add efficiency scoring to compare models","status":"in_progress","priority":1,"issue_type":"feature","created_at":"2025-12-14T07:37:12.60744-08:00","updated_at":"2025-12-15T06:39:34.583641-08:00"}
{"id":"arcfusion-zzp","title":"Cloud training integration: Groq/Modal/Lambda for auto-pipeline","description":"## Problem\n\nLocal compute is too limited to run meaningful training of auto-pipeline generated architectures. Need to offload training to cloud providers.\n\n## Potential Providers\n\n### Modal\n- Serverless GPU compute\n- Python-native, good DX\n- Pay-per-second billing\n\n### Lambda Labs\n- GPU cloud instances\n- Good for longer training runs\n\n### RunPod\n- Cheap GPU rentals\n- Spot instances available\n\n### Groq\n- Fast inference (maybe not for training?)\n\n## Implementation Ideas\n\n1. **Remote execution wrapper**: Serialize model code + config, send to cloud, run training, return metrics\n\n2. **Integration with ValidationPipeline**: `--device cloud:modal` flag\n\n3. **Cost tracking**: Store compute costs in benchmark_results","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T15:51:44.874322-08:00","updated_at":"2025-12-12T13:50:21.582784-08:00","closed_at":"2025-12-12T13:50:21.582784-08:00","close_reason":"Implemented Modal cloud training integration with CloudTrainer, CloudConfig, CloudResult. Added --cloud CLI flag and 13 new tests."}
