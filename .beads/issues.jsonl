{"id":"arcfusion-003","title":"Interface-aware dream composition","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:21.711003-08:00","updated_at":"2025-12-11T12:15:36.11899-08:00","closed_at":"2025-12-11T12:15:36.11899-08:00","close_reason":"Implemented: Shape normalization, category ordering, 4 strategies (greedy, random, crossover, mutate)"}
{"id":"arcfusion-040","title":"Auto-validation pipeline: build, train, and benchmark dreamed architectures","description":"## Vision\n\nCreate an automated pipeline that takes dreamed architectures and actually validates them:\n\n```\nDream → CodeGen → Build Model → Train (small) → Benchmark → Score → Feedback\n```\n\n## Why This Matters\n\nCurrently we estimate scores with formulas. To truly validate the dream engine, we need to:\n1. Build real models from dreamed component lists\n2. Train them (on small datasets to minimize compute)\n3. Run actual benchmarks\n4. Feed scores back to improve future dreams\n\n## Pipeline Components\n\n### 1. Model Builder\n- Take CodeGenerator output and make it runnable\n- Handle edge cases (missing dims, incompatible interfaces)\n- Parameterize model size (tiny/small/medium for testing)\n\n### 2. Training Harness\n- Small dataset options (WikiText-2, TinyStories, MNIST for sanity)\n- Quick training runs (few epochs, small batch)\n- Configurable compute budget (max time, max steps)\n\n### 3. Benchmark Suite\n- Lightweight benchmarks first (perplexity, simple classification)\n- Standardized evaluation protocol\n- Results stored in benchmark_results table\n\n### 4. Feedback Loop\n- Update component usefulness_score based on real performance\n- Update configuration scores\n- Track which component combinations actually work\n\n## Compute Considerations\n\n- Start TINY: 1M param models, 1000 training steps\n- Use CPU-friendly sizes initially\n- Could integrate with cloud compute later (Lambda, Modal, etc.)\n\n## Dependencies\n\n- Solid CodeGenerator (we have this)\n- Benchmark integration (arcfusion-v3i) for storing results\n\n## Success Criteria\n\n- [ ] Can take a dream output and produce a trainable model\n- [ ] Can train on small dataset in \u003c5 minutes\n- [ ] Can run benchmark and store results\n- [ ] Feedback improves future dream scores\n\n## Notes\n\nThis is the \"close the loop\" feature that makes ArcFusion genuinely useful for architecture search.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T14:58:41.413332-08:00","updated_at":"2025-12-11T15:47:04.338619-08:00","closed_at":"2025-12-11T15:47:04.338619-08:00","close_reason":"Implemented: ModelBuilder, TrainingHarness, BenchmarkRunner, ValidationPipeline. CLI validate command added. Requires PyTorch (optional).","dependencies":[{"issue_id":"arcfusion-040","depends_on_id":"arcfusion-v3i","type":"blocks","created_at":"2025-12-11T14:58:47.289242-08:00","created_by":"daemon"}]}
{"id":"arcfusion-1kl","title":"Save model recipes for reproducibility","description":"## Problem\nWe can train models but can't easily recreate them later. The model code is embedded as strings in MODELS dict but not linked to training runs.\n\n## Current State\n- Model code stored as strings in cloud_train_fair.py MODELS dict\n- Training runs reference model_name but not the actual code\n- No way to recover exact model definition from a training run\n\n## Requirements\n1. Save model code/recipe with each training run\n2. Store hyperparameters used (d_model, n_layers, etc.)\n3. Store random seed for reproducibility\n4. Optionally store model weights/checkpoints\n\n## Schema Changes Needed\n- Add model_code TEXT column to training_runs?\n- Or create separate recipes table with versioning?\n- Link training_runs to recipes via recipe_id\n\n## Acceptance Criteria\n- [ ] Can recreate any previously trained model\n- [ ] Recipe includes full model code\n- [ ] Hyperparameters are queryable\n- [ ] Seeds are tracked for reproducibility","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T07:37:14.459225-08:00","updated_at":"2025-12-14T07:37:47.496247-08:00"}
{"id":"arcfusion-2q6","title":"Cache WikiText-2 data on Modal to avoid timeouts","description":"## Problem\nMamba training failed with \"ReadTimeout\" loading WikiText-2 from HuggingFace. Network issues on Modal cause wasted GPU time.\n\n## Options\n\n### Option 1: Modal Volume (Persistent)\n- Store data in Modal Volume once\n- Load from volume on each run\n- Cost: ~$0.20/GB/month = ~$0.01/month for WikiText-2\n- Pro: Never re-download\n- Con: Volume setup complexity\n\n### Option 2: Pre-bake into Image\n- Download data during image build\n- Data cached in image layers\n- Cost: Included in image storage\n- Pro: Always available, no runtime download\n- Con: Rebuilds image when data changes\n\n### Option 3: Retry with backoff\n- Add retry logic to load_dataset()\n- Exponential backoff on timeout\n- Pro: Simple, no infrastructure changes\n- Con: Still wastes time on retries\n\n## Recommendation\n**Option 2 (Pre-bake into Image)** because:\n- WikiText-2 is small (~13MB)\n- Data won't change\n- Zero runtime download time\n- Most reliable\n\n## Implementation\n```python\nimage = (\n    modal.Image.debian_slim(python_version=\"3.11\")\n    .pip_install(\"torch\u003e=2.0\", \"datasets\", \"tiktoken\")\n    .run_commands(\n        \"python -c \\\"from datasets import load_dataset; load_dataset('wikitext', 'wikitext-2-raw-v1')\\\"\"\n    )\n)\n```\n\n## Cost Analysis\n- Current: ~60s download per run × N runs = wasted GPU time\n- With cache: 0s download\n- Image storage: Negligible (data is \u003c20MB)\n- **ROI**: Pays for itself after 1 prevented timeout\n\n## Acceptance Criteria\n- [ ] Data pre-loaded in Modal image\n- [ ] Zero download time on training runs\n- [ ] No more timeout failures","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-14T08:02:29.118136-08:00","updated_at":"2025-12-14T08:12:21.559211-08:00","closed_at":"2025-12-14T08:12:21.559211-08:00","close_reason":"WikiText-2 now pre-cached in Modal image during build. Data load time reduced from timeout (60s+) to ~13s. Added run_commands step to download dataset during image build."}
{"id":"arcfusion-30o","title":"Auto-validate generated code before saving","description":"GeneratedCode.save() writes code without calling validate_syntax(). Add automatic validation and sanitization of component names in docstrings to prevent invalid Python from being saved.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-11T13:02:23.288818-08:00","updated_at":"2025-12-11T13:11:37.162829-08:00","closed_at":"2025-12-11T13:11:37.162829-08:00","close_reason":"save() now validates syntax by default, raises ValueError on invalid code"}
{"id":"arcfusion-38g","title":"Add training_runs table to DB schema for detailed benchmark tracking","description":"Add a training_runs table to track: model architecture, training config (steps, lr, batch_size), hardware (GPU type, mixed precision), results (loss, perplexity, time), and baseline comparison data. Links to existing benchmark_results for post-training evaluation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:25:29.635508-08:00","updated_at":"2025-12-13T16:28:40.45964-08:00","closed_at":"2025-12-13T16:28:40.45964-08:00","close_reason":"Added training_runs table to DB schema with full CRUD methods. Tracks model config, hardware, results, and baseline comparison. 228 tests pass."}
{"id":"arcfusion-4lc","title":"Research: Minimum training scale to differentiate architecture quality","description":"## Research Question\n\nHow much training is needed before it becomes apparent whether an architecture is good or bad?\n\n## Why This Matters\n\nIf we can detect architecture quality with minimal training:\n- Faster iteration on dream experiments\n- Lower compute costs\n- More architectures explored per dollar\n\nIf we need substantial training:\n- Need cloud compute integration\n- Batch validation runs\n- Different validation strategy\n\n## Experiment Design\n\n1. **Baseline**: Train known-good architecture (Transformer) at various scales\n   - 100 steps, 1K steps, 10K steps, 100K steps\n   - Track loss curves and final perplexity\n\n2. **Bad architecture**: Train known-bad architecture (random component soup)\n   - Same scale progression\n   - Compare curves to baseline\n\n3. **Analysis**: At what point do curves diverge?\n   - Early divergence = can validate cheaply\n   - Late divergence = need more compute\n\n## Metrics to Track\n\n- Loss at each checkpoint\n- Perplexity trajectory\n- Parameter efficiency (loss per param)\n- Training stability (gradient variance)\n\n## Literature Review\n\n- Scaling laws papers (Chinchilla, etc.)\n- Neural architecture search papers\n- Early stopping criteria research\n\n## Dependencies\n\n- Need PyTorch installed\n- Need cloud compute for larger runs (arcfusion-zzp)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T15:52:04.874825-08:00","updated_at":"2025-12-12T13:57:06.25733-08:00","closed_at":"2025-12-12T13:57:06.25733-08:00","close_reason":"Research complete: Need 2000+ steps to differentiate architecture quality. Simple architectures train faster initially on random data. Updated cloud defaults accordingly.","dependencies":[{"issue_id":"arcfusion-4lc","depends_on_id":"arcfusion-zzp","type":"blocks","created_at":"2025-12-11T15:52:10.640381-08:00","created_by":"daemon"}]}
{"id":"arcfusion-5ux","title":"Validate relationships in analyzer before adding","description":"In analyzer.py lines 334-348, component relationships are added without verifying both components exist. If LLM returns relationship for skipped component (low confidence), it fails silently or creates orphaned refs.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-11T13:02:16.95983-08:00","updated_at":"2025-12-11T13:11:37.107019-08:00","closed_at":"2025-12-11T13:11:37.107019-08:00","close_reason":"Added validation with warnings for skipped relationships"}
{"id":"arcfusion-5wj","title":"Web UI for architecture exploration","description":"Build a web interface to visualize components, relationships, and dream new architectures interactively","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-11T12:15:52.710451-08:00","updated_at":"2025-12-12T14:26:48.162589-08:00","closed_at":"2025-12-12T14:26:48.162589-08:00","close_reason":"Implemented FastAPI web UI with REST API, interactive frontend, vis.js graph, and dream interface"}
{"id":"arcfusion-753","title":"Ensure Modal GPU cleanup to avoid billing","description":"","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-14T07:13:39.313164-08:00","updated_at":"2025-12-14T07:14:33.860315-08:00","closed_at":"2025-12-14T07:14:33.860315-08:00","close_reason":"Modal scales to zero by default - we pay only for actual compute. Our use of 'with app.run():' ensures clean shutdown. No scaledown_window/min_containers used. To monitor: 'modal app list' shows running apps."}
{"id":"arcfusion-7xk","title":"Training too slow - evaluate faster GPU options","description":"Current training on A10G takes ~97s per model for 2000 steps. Options to explore:\n\n1. **A100 GPU** (~3x faster, ~3x cost) - best for serious experiments\n2. **H100 GPU** (~5x faster, ~5x cost) - overkill for small models?\n3. **Reduce model size** - smaller d_model/n_layers for faster iteration\n4. **Reduce steps** - 1000 steps might be enough to differentiate architectures\n5. **Batch multiple models** - train in parallel on same GPU\n\nQuestions to answer:\n- What's the minimum training needed to reliably differentiate architectures?\n- Is the bottleneck GPU compute or data loading?\n- Would spot instances save money?","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T17:42:01.101448-08:00","updated_at":"2025-12-14T07:08:20.817428-08:00","closed_at":"2025-12-14T07:08:20.817428-08:00","close_reason":"Switched to A100 GPU. Training now 3-4x faster (55-65s vs 200s on T4)"}
{"id":"arcfusion-8rs","title":"Track arcfusion.db in git","description":"The database file isn't being tracked by git. Add it to version control so component/engine data persists across clones.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T16:19:14.914455-08:00","updated_at":"2025-12-14T16:19:14.914455-08:00"}
{"id":"arcfusion-9c9","title":"DB table: recipes with adjustments tracking","description":"New DB table to store dreamed recipes:\n- recipe_id (unique)\n- component_ids (ordered list)\n- assembly_instructions (JSON: connections, residuals, shapes)\n- source_strategy (greedy, crossover, mutate, etc.)\n- created_at timestamp\n\nPlus adjustments table:\n- adjustment_id\n- recipe_id (FK)\n- original_value\n- adjusted_value  \n- reason (why modification was needed)\n- adjusted_at timestamp\n\nThis enables: recipe recreation, composer learning from failures, training run reproducibility.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:15:07.58677-08:00","updated_at":"2025-12-12T12:24:43.228152-08:00","closed_at":"2025-12-12T12:24:43.228152-08:00","close_reason":"DB tables implemented: recipes (recipe_id, name, component_ids, assembly, strategy, estimated_score, parent_engine_ids, notes) and recipe_adjustments (adjustment_id, recipe_id, adjustment_type, original_value, adjusted_value, reason, component_id). Full CRUD methods and stats tracking."}
{"id":"arcfusion-a1s","title":"ML Agent: faithful recipe execution with modification tracking","description":"ML Agent receives Recipe from Composer and:\n- Makes best effort to train the model\n- Stays faithful to the recipe provided\n- Records ANY modifications needed to enable training\n- Modifications inform Composer for future dreams\n- Enables recreation of training runs\n\nKey principle: ML Agent has leeway to make necessary adjustments, but ALL adjustments must be recorded.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:15:00.085093-08:00","updated_at":"2025-12-12T12:58:49.512701-08:00","closed_at":"2025-12-12T12:58:49.512701-08:00","close_reason":"ML Agent implemented with execute_recipe(), adjustment tracking, ExecutionResult dataclass. Records all modifications (component_skip, build_fix, train_fix). 16 new tests, 215 total passing.","dependencies":[{"issue_id":"arcfusion-a1s","depends_on_id":"arcfusion-p9w","type":"blocks","created_at":"2025-12-12T12:15:33.445742-08:00","created_by":"daemon"},{"issue_id":"arcfusion-a1s","depends_on_id":"arcfusion-9c9","type":"blocks","created_at":"2025-12-12T12:15:33.493527-08:00","created_by":"daemon"}]}
{"id":"arcfusion-a8o","title":"Multi-agent autonomous development system","description":"Brainstorm: 6-8 AI agents working together:\n- 1 Project Maintainer agent: approves/rejects PRs on GitHub\n- Other agents: claim beads issues, write code, submit PRs to the repo\n- Autonomous development workflow with human oversight via PR reviews","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-14T16:19:13.545493-08:00","updated_at":"2025-12-14T16:19:13.545493-08:00"}
{"id":"arcfusion-a9f","title":"Local training validation before cloud deploy","description":"Test training runs locally before deploying to cloud:\n- Catch errors early (syntax, shape mismatches, missing imports)\n- Run quick ~50 step validation\n- Minimize wasted cloud GPU time from training errors","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T16:19:16.918777-08:00","updated_at":"2025-12-14T16:19:16.918777-08:00"}
{"id":"arcfusion-abz","title":"Speed up Modal training - larger GPUs or less data","description":"Current training on Modal T4 GPUs times out at 30 min. Options: 1) Use A10G/A100 GPUs 2) Reduce dataset size 3) Use step-based training vs epochs 4) Reduce model size. Goal: complete training runs in \u003c10 min for fast iteration.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:27.035282-08:00","updated_at":"2025-12-13T16:28:37.654998-08:00","closed_at":"2025-12-13T16:28:37.654998-08:00","close_reason":"Added mixed precision (FP16 autocast + GradScaler) and upgraded to A10G GPU in cloud_train_fair.py. Expected 2-3x speedup."}
{"id":"arcfusion-avc","title":"Validation: Can we discover Transformer without the attention paper?","description":"## Research Question\n\nIf the \"Attention Is All You Need\" paper had never been published, could ArcFusion's dream engine independently discover the Transformer architecture by composing components from other papers?\n\n## Why This Matters\n\nThis is a key validation of the entire ArcFusion approach:\n- If YES → The system can genuinely discover novel architectures\n- If NO → We're just recombining known patterns, not innovating\n\n## Experiment Design\n\n### Setup\n1. Remove Transformer and all attention-related components from the database\n2. Seed only with pre-2017 components (RNNs, CNNs, LSTMs, embeddings, etc.)\n3. Add components from papers that influenced attention (memory networks, seq2seq, etc.)\n\n### Test\n1. Run dream engine with various strategies (greedy, random, crossover, mutate)\n2. Generate many candidate architectures\n3. Analyze: Do any resemble Transformer's key innovations?\n   - Multi-head attention pattern\n   - Encoder-decoder with cross-attention\n   - Positional encodings + self-attention (no recurrence)\n\n### Success Criteria\n- **Strong success**: Dream engine produces attention-like mechanism\n- **Partial success**: Produces components that could lead to attention with minor tweaks\n- **Failure**: Only produces RNN/CNN variants\n\n## Dependencies\n- Robust component extraction from pre-2017 papers\n- Good relationship scoring between components\n- Multiple dream strategies working well\n\n## Notes\n- This may require ingesting more historical papers (2014-2016 era)\n- May need to tune dream engine parameters\n- Could be a good benchmark for measuring system improvement over time","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T14:19:39.83596-08:00","updated_at":"2025-12-11T14:43:31.692448-08:00","closed_at":"2025-12-11T14:43:31.692448-08:00","close_reason":"Experiment complete. PARTIAL SUCCESS: Dream engine found architecture with attention+parallelism+position (5/5 score). Key finding: Can get 80% of the way but misses self-attention, multi-head, and dot-product innovations that required human insight."}
{"id":"arcfusion-ayd","title":"Track component configurations (sub-architectures)","description":"## Problem\n\nCurrently we track full engines (e.g., Transformer with 15 components) but not successful sub-configurations. For example, if 7 specific components from Transformer in a certain order consistently produce good results, we have no way to capture and reuse that pattern.\n\n## Proposed Solution\n\nTrack \"configurations\" - ordered subsets of components that work well together:\n\n### New DB Table: `component_configurations`\n```sql\nCREATE TABLE component_configurations (\n    config_id TEXT PRIMARY KEY,\n    name TEXT,                          -- e.g., \"Transformer Core Block\"\n    description TEXT,\n    component_ids TEXT,                 -- JSON array of component IDs in order\n    source_engine_id TEXT,              -- Engine this was derived from (optional)\n    configuration_score REAL,           -- How well this config performs\n    usage_count INTEGER DEFAULT 0,      -- How often used in dreams\n    validated BOOLEAN DEFAULT 0,        -- Has been tested/validated\n    created_at TIMESTAMP\n);\n```\n\n### Key Features\n1. **Configuration extraction**: Analyze engines to find common successful patterns\n2. **Configuration scoring**: Track which configs produce good dream results\n3. **Dream integration**: Use proven configs as building blocks for new architectures\n4. **Configuration discovery**: Find configs that appear across multiple engines\n\n### Use Cases\n- \"Attention + LayerNorm + FFN\" block appears in many architectures → track as config\n- User validates a dreamed architecture → extract and save its sub-configs\n- Dream engine prefers using proven configs when available\n\n## Acceptance Criteria\n- [ ] Add `component_configurations` table to schema\n- [ ] Add CRUD operations for configurations\n- [ ] Add method to extract configs from existing engines\n- [ ] Integrate with composer to prefer known-good configs\n- [ ] CLI commands: `arcfusion config list/show/extract`","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T14:17:56.139247-08:00","updated_at":"2025-12-11T14:27:49.145668-08:00","closed_at":"2025-12-11T14:27:49.145668-08:00","close_reason":"Feature complete: DB schema, CRUD ops, extraction, CLI commands, and dream integration all implemented. 167 tests passing."}
{"id":"arcfusion-b9y","title":"Use cached baseline average - train transformer N times, store mean result","description":"The vanilla Transformer_MHA baseline is critical for fair comparisons. Current approach trains it once and caches. Better approach:\n\n1. **Train baseline N times** (e.g., 3-5 runs) with different seeds\n2. **Store all runs** in training_runs table  \n3. **Compute mean + stddev** for perplexity\n4. **Use mean as reference** for vs_baseline_pct calculations\n5. **Never retrain** - just pull cached average from DB\n\nBenefits:\n- More reliable baseline (reduces variance from random init)\n- Faster benchmarking (skip baseline training entirely)\n- Statistical significance (can compute confidence intervals)\n\nImplementation:\n- Add 'seed' field to TrainingRun and CONFIG\n- Add get_baseline_stats(config) -\u003e (mean_ppl, std_ppl, n_runs)\n- Run baseline 3-5x on first setup, then never again","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-13T17:42:04.211643-08:00","updated_at":"2025-12-13T19:07:43.273021-08:00","closed_at":"2025-12-13T19:07:43.273021-08:00","close_reason":"Implemented baseline averaging: added seed to TrainingRun, get_baseline_stats(), get_baseline_seeds_needed() in db.py. Updated cloud_train_fair.py to train N baselines with different seeds and use mean perplexity for comparisons. Testing blocked by Modal service RemoteError - retry later."}
{"id":"arcfusion-ctf","title":"PyTorch code generation from dreamed architectures","description":"Generate runnable PyTorch code from dreamed component combinations, using code_sketch fields and interface compatibility","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T12:15:49.586369-08:00","updated_at":"2025-12-11T12:37:51.398835-08:00","closed_at":"2025-12-11T12:37:51.398835-08:00","close_reason":"Implemented CodeGenerator with generate CLI command. Generates valid PyTorch nn.Module code from dreamed architectures using greedy, random, crossover, and mutate strategies."}
{"id":"arcfusion-d7u","title":"Core database schema with components, engines, relationships","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:17.526253-08:00","updated_at":"2025-12-11T12:15:31.395323-08:00","closed_at":"2025-12-11T12:15:31.395323-08:00","close_reason":"Implemented: 8 tables (components, engines, relationships, compatibility, papers, benchmarks, dreams, validations)"}
{"id":"arcfusion-e2f","title":"Validation: Implement Transformer using only DB components via auto-pipeline","description":"## Goal\n\nTest the auto-validation pipeline by implementing the Transformer architecture using ONLY components extracted into our database - no external code.\n\n## Why This Test\n\n- We KNOW Transformer works (it's a winning recipe)\n- If our pipeline can reconstruct it from DB components and train successfully, the pipeline is validated\n- Fair comparison: only use what we've extracted, not code from the internet\n\n## Test Protocol\n\n1. **Component Selection**\n   - Query DB for Transformer-relevant components\n   - Use: MultiHeadAttention, FeedForward, LayerNorm, Embedding, etc.\n   - Only components with code sketches from our extraction\n\n2. **Model Assembly**\n   - Use CodeGenerator to assemble from DB components\n   - Standard Transformer config: 6 layers, 512 dim, 8 heads\n   - Keep it small for fast iteration\n\n3. **Training**\n   - Small dataset (WikiText-2 or similar)\n   - Short training run (sanity check, not SOTA)\n   - Track loss curves\n\n4. **Benchmark**\n   - Measure perplexity\n   - Compare to known Transformer baselines\n   - Store in benchmark_results\n\n## Success Criteria\n\n- [ ] Model assembles from DB components without manual code\n- [ ] Model trains without errors\n- [ ] Achieves reasonable perplexity (not random)\n- [ ] Results stored in DB\n- [ ] Pipeline proven end-to-end\n\n## Why \"Fair Comparison\" Matters\n\nIf we copy Transformer code from HuggingFace, we're not testing our system. The whole point is: can our extracted component representations actually work?\n\n## Dependencies\n\n- Auto-validation pipeline (arcfusion-040)\n- Sufficient Transformer components in DB (we have these from seeds)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-11T15:13:20.534039-08:00","updated_at":"2025-12-11T17:47:48.47835-08:00","closed_at":"2025-12-11T17:47:48.47835-08:00","close_reason":"Validation pipeline works end-to-end: Transformer builds from DB components (5 components, 1M params), trains successfully (50 steps, loss ~6.9), and benchmarks run correctly. Fixed vocab_size propagation in codegen.py and validator.py.","dependencies":[{"issue_id":"arcfusion-e2f","depends_on_id":"arcfusion-040","type":"blocks","created_at":"2025-12-11T15:13:26.60807-08:00","created_by":"daemon"}]}
{"id":"arcfusion-ecq","title":"Brainstorm: Ingest more arXiv papers or focus on training?","description":"## Question\nShould we prioritize ingesting more arXiv papers into the component database, or focus on training/benchmarking with what we have?\n\n## Arguments for More Papers\n- More components = more dreaming options\n- May find novel techniques we haven't considered\n- Builds comprehensive component database\n- One-time effort that pays dividends\n\n## Arguments for Focus on Training\n- Already have proven components (MHA, GQA, MQA, Mamba)\n- Training validates which components actually work\n- Mamba result shows our current components can beat baselines\n- Can always add papers later based on what we learn\n\n## Current Component Status\n- Attention: MHA, GQA, MQA (tested)\n- SSM: Mamba (tested, -20% perplexity!)\n- Position: Learned, could add RoPE/ALiBi\n- Normalization: LayerNorm\n- Activation: GELU\n\n## Potential Papers to Ingest\n- RetNet (retention mechanism)\n- RWKV (RNN-like transformers)  \n- Linear Attention variants\n- Flash Attention (efficiency)\n- Mixture of Experts papers\n\n## Recommendation\nFocus on training first - validate hybrid architectures, then selectively ingest papers for components we want to test.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-14T07:43:14.477875-08:00","updated_at":"2025-12-14T07:43:14.477875-08:00"}
{"id":"arcfusion-f73","title":"Add test coverage for untested modules","description":"composer.py, dedup.py, decomposer.py, and seeds.py have no dedicated tests. Add tests for:\n- All 4 dream strategies (greedy, random, mutate, crossover)\n- Duplicate detection and merging\n- Paper decomposition\n- Database seeding","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T13:02:07.568054-08:00","updated_at":"2025-12-11T13:07:14.417626-08:00","closed_at":"2025-12-11T13:07:14.417626-08:00","close_reason":"Added 102 new tests (157 total) covering composer, dedup, decomposer, and seeds modules"}
{"id":"arcfusion-frl","title":"Fuzzy deduplication with variant detection","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:20.748145-08:00","updated_at":"2025-12-11T12:15:34.510907-08:00","closed_at":"2025-12-11T12:15:34.510907-08:00","close_reason":"Implemented: ComponentDeduplicator with normalized names, semantic signatures, architecture variant detection"}
{"id":"arcfusion-hjg","title":"Implement local testing process before cloud deployment","description":"## Problem\nDeploying untested code to Modal wastes GPU time and money when there are bugs. We should validate locally first.\n\n## Proposed Process\n1. **Syntax check**: Run model code through exec() locally\n2. **Shape test**: Forward pass with small batch (2, 16)\n3. **Parameter count**: Verify comparable to baseline (~30M params)\n4. **Gradient test**: One backward pass to check autograd works\n5. **Only then**: Deploy to Modal for full training\n\n## Implementation\nCreate a `test_model_locally()` function that:\n```python\ndef test_model_locally(model_code: str, model_name: str) -\u003e bool:\n    # 1. Parse and exec\n    ns = {}\n    exec(model_code, ns)\n    model_class = ns.get(model_name)\n    \n    # 2. Instantiate\n    model = model_class(d_model=256, vocab_size=50257, n_layers=4, n_heads=8)\n    \n    # 3. Forward pass\n    x = torch.randint(0, 50257, (2, 16))\n    out = model(x)\n    assert out.shape == (2, 16, 50257)\n    \n    # 4. Backward pass\n    loss = out.sum()\n    loss.backward()\n    \n    return True\n```\n\n## Benefits\n- Catch bugs before spending GPU credits\n- Faster iteration (no Modal cold start)\n- Validate shapes and gradients work\n- Parameter count sanity check\n\n## Acceptance Criteria\n- [ ] Local test function implemented\n- [ ] Integrated into cloud_train_fair.py workflow\n- [ ] All new models pass local test before cloud","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T08:02:28.935724-08:00","updated_at":"2025-12-14T08:02:28.935724-08:00"}
{"id":"arcfusion-hns","title":"Focus on text modality only - no vision/audio/multimodal","description":"ArcFusion should focus exclusively on text/language model architectures. No vision, audio, or multimodal components. This keeps the scope manageable and allows fair comparisons between attention mechanisms on language modeling tasks.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-13T16:46:57.824019-08:00","updated_at":"2025-12-13T16:53:55.36472-08:00","closed_at":"2025-12-13T16:53:55.36472-08:00","close_reason":"Established text-only focus. Training script now uses WikiText-2 with GPT-2 tokenizer. All benchmarks are language modeling (perplexity)."}
{"id":"arcfusion-iyg","title":"Always include vanilla transformer as baseline in benchmarks","description":"Every benchmark run should include a vanilla transformer (standard MHA) as the baseline. This provides a consistent reference point to measure improvements or regressions from architectural changes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:30.956414-08:00","updated_at":"2025-12-13T16:28:38.899894-08:00","closed_at":"2025-12-13T16:28:38.899894-08:00","close_reason":"Baseline transformer (Transformer_MHA) is now always trained first in cloud_train_fair.py. All other models show vs_baseline comparison."}
{"id":"arcfusion-l3a","title":"Expand paper knowledge base","description":"Analyze more seminal papers: GPT-3, PaLM, Gemini, Claude architecture (if published), Mixtral, Phi, etc.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-11T12:15:50.758608-08:00","updated_at":"2025-12-12T14:17:32.259717-08:00","closed_at":"2025-12-12T14:17:32.259717-08:00","close_reason":"Covered by arcfusion-xol - papers/architectures expanded together."}
{"id":"arcfusion-p9w","title":"Recipe dataclass: ordered components + assembly instructions","description":"Composer outputs a Recipe dataclass containing:\n- Ordered list of component IDs\n- Assembly instructions (how components connect, residuals, etc.)\n- Metadata (strategy used, score, timestamp)\n\nThis is the handoff format between Composer and ML Agent.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-12T12:14:52.735707-08:00","updated_at":"2025-12-12T12:24:41.769431-08:00","closed_at":"2025-12-12T12:24:41.769431-08:00","close_reason":"Recipe dataclass implemented with: name, component_ids, assembly instructions (connections, residuals, shapes, categories, notes), strategy, estimated_score, parent_engine_ids. Composer.create_recipe() generates Recipes from any dream strategy."}
{"id":"arcfusion-pqn","title":"Ensure all benchmark results are saved","description":"## Problem\nTraining runs produce valuable benchmark data but not all results are being persisted properly.\n\n## Current State\n- Results saved to experiments/fair_comparison_results.json\n- Some results saved to DB via add_training_run()\n- No guarantee all runs are captured\n\n## Requirements\n1. Every training run should be saved to DB automatically\n2. Results should include all metrics (loss, perplexity, time, params)\n3. Comparison results should be queryable (vs baseline %)\n4. Should be able to regenerate any results JSON from DB\n\n## Acceptance Criteria\n- [ ] All training runs persisted to DB\n- [ ] Can query historical comparisons\n- [ ] Results JSON can be regenerated from DB","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-14T07:37:13.522653-08:00","updated_at":"2025-12-14T07:37:45.831531-08:00"}
{"id":"arcfusion-qhj","title":"Cache baseline transformer - train once, reference forever","description":"Train vanilla Transformer_MHA baseline once and save to training_runs table. Future benchmark runs should load the cached baseline result instead of retraining. Only retrain baseline when config changes (d_model, n_layers, etc.).","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-13T16:55:47.117325-08:00","updated_at":"2025-12-13T17:33:18.582724-08:00","closed_at":"2025-12-13T17:33:18.582724-08:00","close_reason":"Baseline caching implemented via config hash matching. Train vanilla transformer once, lookup by config_hash in notes field."}
{"id":"arcfusion-sxn","title":"Save key findings to DB - schema changes needed","description":"## Problem\nKey findings from training (like Mamba beating Transformers by 20%) should be saved to DB for historical analysis, but current schema may not support this.\n\n## Key Findings to Track\n- Architecture comparisons (A vs B perplexity delta)\n- Breakthrough results (significant improvements)\n- Failed experiments (what didn't work)\n- Component combinations tested\n- Statistical significance (std dev, n runs)\n\n## Current Schema Gaps\n- No way to link related training runs (same experiment)\n- No experiment_id or batch_id concept\n- No findings/notes table for insights\n- Limited metadata on runs\n\n## Proposed Schema Changes\n1. Add experiment_runs table to group related runs\n2. Add findings table for insights/conclusions\n3. Add experiment_id FK to training_runs\n4. Add tags/labels for categorization\n\n## Acceptance Criteria\n- [ ] Can group runs into experiments\n- [ ] Can record findings/insights with evidence\n- [ ] Can query historical comparisons\n- [ ] Can tag/categorize experiments","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-14T07:37:15.761391-08:00","updated_at":"2025-12-14T07:37:48.921097-08:00"}
{"id":"arcfusion-v3i","title":"Benchmark integration for architecture scoring","description":"Add support for storing and querying benchmark results (perplexity, accuracy, speed) to score dreamed architectures against real performance data","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T12:15:47.542041-08:00","updated_at":"2025-12-11T15:18:29.598876-08:00","closed_at":"2025-12-11T15:18:29.598876-08:00","close_reason":"Complete: CLI commands for add/list/leaderboard/show/compare. DB layer was already done."}
{"id":"arcfusion-wbi","title":"Component DB granularity for distinct, trainable recipes","description":"Ensure component database is fine-grained enough that:\n1. Composer can create recipes that are trainable by ML Agent\n2. Recipes stay faithful to the original idea\n3. Resulting models are DISTINCT - don't all blend together\n\nMay require:\n- More specific component variants (not just 'Attention' but 'MultiHeadAttention', 'GroupedQueryAttention', etc.)\n- Clearer interface specifications\n- Assembly instruction patterns that preserve architectural intent\n- Component compatibility metadata","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-12T12:15:27.388153-08:00","updated_at":"2025-12-12T13:21:23.836477-08:00","closed_at":"2025-12-12T13:21:23.836477-08:00","close_reason":"Added MQA/SlidingWindow/Linear attention variants, temperature sampling for greedy, validity constraints for random walk, fixed categorization"}
{"id":"arcfusion-wcm","title":"LLM-powered paper analysis with Claude API","description":"","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-11T12:15:19.280082-08:00","updated_at":"2025-12-11T12:15:33.171477-08:00","closed_at":"2025-12-11T12:15:33.171477-08:00","close_reason":"Implemented: PaperAnalyzer extracts components with interfaces, hyperparameters, complexity, code sketches"}
{"id":"arcfusion-xol","title":"Scale up: ingest more ML papers into pipeline","description":"## Goal\n\nExpand the knowledge base by ingesting more ML architecture papers into the pipeline.\n\n## Why Now\n\nAs we build out the auto-validation pipeline, we need:\n- More components to dream with\n- More proven architectures to learn from\n- Better relationship data between components\n\n## Papers to Prioritize\n\n### Foundational (pre-2020)\n- ResNet (residual connections)\n- ELMo (contextualized embeddings)\n- GPT-1 (decoder-only transformer)\n- XLNet (permutation language modeling)\n- T5 (encoder-decoder, text-to-text)\n\n### Modern (2020-2024)\n- GPT-3/4 architecture details\n- PaLM (pathways, scaling)\n- Chinchilla (compute-optimal scaling)\n- Mistral/Mixtral (MoE, sliding window)\n- Llama 2/3 (grouped query attention, improvements)\n- Gemma (efficient small models)\n- Phi-1/2/3 (data quality focus)\n- RWKV variants\n- Mamba 2\n\n### Efficiency/Training\n- LoRA (low-rank adaptation)\n- QLoRA (quantized fine-tuning)\n- Flash Attention 2/3\n- Ring Attention\n- Mixture of Experts papers\n\n## Approach\n\n1. Use ArxivFetcher to batch fetch papers\n2. Run through PaperAnalyzer for deep extraction\n3. Deduplicate components\n4. Build relationship graph\n\n## Success Criteria\n\n- [ ] 50+ papers ingested\n- [ ] 100+ unique components\n- [ ] Rich relationship graph for dreaming","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-11T15:13:19.150207-08:00","updated_at":"2025-12-12T14:17:31.363926-08:00","closed_at":"2025-12-12T14:17:31.363926-08:00","close_reason":"Expanded DB: 72 components (+9), 17 engines (+6). Added LoRA, MoE, ALiBi, CrossAttention, T5, Mixtral, PaLM, BLOOM, Falcon, GPT-NeoX."}
{"id":"arcfusion-xw9","title":"Include training time in results rankings","description":"## Problem\nCurrent rankings only show perplexity, but training time matters for practical use. Mamba takes 706s vs MHA's 57s - that's important context.\n\n## Current Results (missing time context)\n```\nModel             Perplexity    vs Baseline\nMamba             226.14        -19.9%\nHybrid            242.90        -13.9%\nMHA               282.26        baseline\n```\n\n## Proposed Results (with time)\n```\nModel             Perplexity    vs Baseline    Time      Speed\nMamba             226.14        -19.9%         706s      0.08x\nHybrid            242.90        -13.9%         565s      0.10x\nMHA               282.26        baseline       57s       1.0x\nGQA               294.03        +4.2%          61s       0.93x\nMQA               294.05        +4.2%          54s       1.06x\n```\n\n## Implementation\n1. Already tracking `time_seconds` in results\n2. Add to summary table formatting\n3. Calculate speed relative to baseline\n4. Store in fair_comparison_results.json\n\n## Quality/Speed Tradeoff Metric\nCould add a combined metric:\n- `efficiency = perplexity_improvement / time_cost`\n- Shows \"perplexity gain per second spent\"\n\n## Acceptance Criteria\n- [ ] Time shown in results table\n- [ ] Speed relative to baseline calculated\n- [ ] Results JSON includes time data\n- [ ] Optional: efficiency metric","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-14T08:02:30.631525-08:00","updated_at":"2025-12-14T08:16:36.76025-08:00","closed_at":"2025-12-14T08:16:36.76025-08:00","close_reason":"Added Time column to results summary table. Training time now displayed alongside perplexity and baseline comparison."}
{"id":"arcfusion-xxm","title":"Batch training for dreamed architectures","description":"Train dreamed-up architecture ideas in batches:\n- Queue up multiple dream compositions\n- Run overnight or when user is AFK\n- Collect results for review later\n- Maximize GPU utilization during off-hours","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-14T16:19:15.836125-08:00","updated_at":"2025-12-14T16:19:15.836125-08:00"}
{"id":"arcfusion-y8w","title":"Run trained models through benchmarks and record results in DB","description":"After training, run models through standard benchmarks. Ensure benchmark difficulty is calibrated so models don't saturate near 100% (cannot distinguish between models). Store benchmark results in arcfusion.db for tracking and comparison.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-13T16:07:28.919697-08:00","updated_at":"2025-12-13T17:33:17.771115-08:00","closed_at":"2025-12-13T17:33:17.771115-08:00","close_reason":"Training results now saved to training_runs table with full config, hardware, and baseline comparison data."}
{"id":"arcfusion-ye0","title":"Combined perplexity + training time score","description":"Training time is almost as important as perplexity. Create a composite score:\n- Combine perplexity improvement with training efficiency\n- Rank all results by this combined metric\n- Help identify architectures that are both good AND fast\n- Consider: score = perplexity_improvement / log(training_time) or similar","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-14T16:19:19.884443-08:00","updated_at":"2025-12-14T16:19:19.884443-08:00"}
{"id":"arcfusion-zdr","title":"Brainstorm: Patch Transformer weaknesses with dreamed architectures","description":"## Context\nTransformer is the dominant architecture for AI but has known weaknesses. Our pipeline can dream new architectures by combining components. We validated this with Mamba achieving 20% better perplexity than MHA.\n\n## Known Transformer Weaknesses\n\n### 1. O(n²) Attention Complexity\n- Quadratic scaling with sequence length\n- 128k context = 16 billion attention operations per layer\n- **Patch**: SSM/Mamba (O(n)), Linear Attention, sparse attention patterns\n\n### 2. Limited Context Window  \n- Hard to extend beyond training length\n- Performance degrades on longer sequences\n- **Patch**: ALiBi (relative positions extrapolate), RoPE, SSM (infinite context)\n\n### 3. Position Encoding Brittleness\n- Learned absolute positions don't generalize\n- Fixed sin/cos limited to training length\n- **Patch**: RoPE, ALiBi, no-position SSM (implicit via recurrence)\n\n### 4. KV Cache Memory\n- Inference memory grows O(n) with context\n- 128k context × 32 layers × 128 heads = huge memory\n- **Patch**: GQA (fewer KV heads), MQA (single KV head), sliding window, SSM (fixed state)\n\n### 5. No Recurrence\n- Can't naturally handle infinite streams\n- Must re-process full context each step\n- **Patch**: SSM/Mamba (recurrent), RWKV, RetNet\n\n### 6. Attention Dilution\n- Long sequences dilute attention weights\n- Important tokens get less attention share\n- **Patch**: Sparse attention, local+global patterns, SSM (selective memory)\n\n## Evidence from Our Experiments\n\n| Architecture | Perplexity | vs MHA | Addresses |\n|-------------|-----------|--------|-----------|\n| Mamba (SSM) | 226.14 | -19.9% | Complexity, context, recurrence |\n| MHA (baseline) | 282.26 | - | - |\n| GQA | 294.03 | +4.2% | KV cache |\n| MQA | 294.05 | +4.2% | KV cache |\n\n**Key finding**: Mamba's 20% improvement shows SSM can beat attention for language modeling while addressing multiple weaknesses.\n\n## Potential Dreamed Architectures\n\n### Hybrid Transformer-Mamba\n- Alternate attention and SSM layers\n- Attention for global reasoning, SSM for local/sequential\n- Best of both worlds?\n\n### Multi-Scale Architecture  \n- Local: SSM or sliding window attention\n- Global: sparse attention or full attention every N layers\n- Different components at different scales\n\n### Adaptive Architecture\n- Route tokens to attention vs SSM based on content\n- Use attention for complex reasoning, SSM for routine processing\n- MoE-style selection\n\n## Brainstorm Questions\n1. What component combinations address which weaknesses?\n2. Can we dream a \"best of both worlds\" architecture?\n3. What evaluation benchmarks reveal each weakness?\n4. How do we measure improvement on specific weaknesses?\n5. Should we test hybrid architectures next?\n\n## Next Steps\n- [ ] Implement hybrid Transformer-Mamba architecture\n- [ ] Add Linear Attention variant\n- [ ] Test on longer sequences to measure context scaling\n- [ ] Benchmark inference speed and memory","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-14T07:37:12.60744-08:00","updated_at":"2025-12-14T07:41:31.406507-08:00"}
{"id":"arcfusion-zzp","title":"Cloud training integration: Groq/Modal/Lambda for auto-pipeline","description":"## Problem\n\nLocal compute is too limited to run meaningful training of auto-pipeline generated architectures. Need to offload training to cloud providers.\n\n## Potential Providers\n\n### Modal\n- Serverless GPU compute\n- Python-native, good DX\n- Pay-per-second billing\n\n### Lambda Labs\n- GPU cloud instances\n- Good for longer training runs\n\n### RunPod\n- Cheap GPU rentals\n- Spot instances available\n\n### Groq\n- Fast inference (maybe not for training?)\n\n## Implementation Ideas\n\n1. **Remote execution wrapper**: Serialize model code + config, send to cloud, run training, return metrics\n\n2. **Integration with ValidationPipeline**: `--device cloud:modal` flag\n\n3. **Cost tracking**: Store compute costs in benchmark_results","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-11T15:51:44.874322-08:00","updated_at":"2025-12-12T13:50:21.582784-08:00","closed_at":"2025-12-12T13:50:21.582784-08:00","close_reason":"Implemented Modal cloud training integration with CloudTrainer, CloudConfig, CloudResult. Added --cloud CLI flag and 13 new tests."}
